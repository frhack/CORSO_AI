<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>dx — live compiler demo (Pyodide)</title>
<style>
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
    background: #0d1117; color: #c9d1d9;
    min-height: 100vh;
}
.controls { display: flex; gap: 8px; align-items: center; }
button {
    background: #238636; color: #fff; border: none; border-radius: 6px;
    padding: 7px 20px; font-size: 13px; font-weight: 600; cursor: pointer;
    transition: background 0.15s;
}
button:hover { background: #2ea043; }
button:disabled { background: #21262d; color: #8b949e; cursor: not-allowed; }
select {
    background: #21262d; color: #c9d1d9; border: 1px solid #30363d;
    border-radius: 6px; padding: 6px 10px; font-size: 12px; cursor: pointer;
}
main {
    display: grid; grid-template-columns: 520px 1fr;
    height: 100vh;
}
@media (max-width: 900px) {
    main { grid-template-columns: 1fr; }
    .code-panel { max-height: 40vh; }
}
.code-panel {
    border-right: 1px solid #21262d; overflow: auto; padding: 0;
    background: #0d1117;
}
.panel-header {
    font-size: 12px; color: #8b949e; text-transform: uppercase;
    letter-spacing: 1px; padding: 12px 16px 8px;
    border-bottom: 1px solid #21262d; background: #161b22;
    position: sticky; top: 0; z-index: 1;
}
.editor-wrap {
    position: relative; width: 100%; height: calc(100% - 40px);
    overflow: hidden;
}
.editor-wrap pre, .editor-wrap textarea {
    font-family: 'SF Mono', 'JetBrains Mono', 'Fira Code', 'Cascadia Code', 'Menlo', 'Consolas', monospace;
    font-size: 13px; line-height: 1.5; padding: 12px 16px;
    tab-size: 4; margin: 0; border: none; outline: none;
    white-space: pre; overflow: auto;
    position: absolute; top: 0; left: 0; width: 100%; height: 100%;
}
.editor-highlight {
    background: #0d1117; color: #c9d1d9; pointer-events: none; z-index: 0;
}
.editor-textarea {
    background: transparent; color: transparent; caret-color: #c9d1d9;
    z-index: 1; resize: none; -webkit-text-fill-color: transparent;
}
.kw { color: #ff7b72; }
.ty { color: #79c0ff; }
.fn { color: #d2a8ff; }
.num { color: #79c0ff; }
.str { color: #a5d6ff; }
.cm { color: #8b949e; font-style: italic; }
.pr { color: #ffa657; font-weight: 600; }
.op { color: #ff7b72; }
.viz-panel {
    padding: 16px; display: flex; flex-direction: column; gap: 12px;
    overflow: auto;
}
.stats-bar {
    display: flex; gap: 32px; padding: 8px 0;
}
.stat .label { font-size: 11px; color: #8b949e; text-transform: uppercase; letter-spacing: 1px; }
.stat .value { font-size: 22px; font-weight: 700; color: #58a6ff; font-variant-numeric: tabular-nums; }
.canvas-row { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; }
.card {
    background: #161b22; border: 1px solid #21262d; border-radius: 8px;
    padding: 12px; display: flex; flex-direction: column; gap: 8px;
}
.card-title {
    font-size: 11px; color: #8b949e; text-transform: uppercase;
    letter-spacing: 1px;
}
canvas { border-radius: 4px; width: 75%; aspect-ratio: 1; display: block; background: #0d1117; }
.card-loss canvas { aspect-ratio: 4/3; }
.predictions {
    display: grid; grid-template-columns: 1fr 1fr; gap: 6px 24px;
    font-family: 'SF Mono', 'JetBrains Mono', monospace; font-size: 13px;
}
.pred-row { display: flex; align-items: center; gap: 8px; }
.pred-input { color: #8b949e; min-width: 70px; }
.pred-value { font-weight: 700; min-width: 55px; transition: color 0.3s; }
.pred-exp { color: #8b949e; font-size: 11px; }
.console-out {
    font-family: 'SF Mono', 'JetBrains Mono', monospace;
    font-size: 11px; line-height: 1.5; color: #8b949e;
    max-height: 100px; overflow-y: auto; white-space: pre;
    padding: 8px; background: #0d1117; border-radius: 4px;
    border: 1px solid #21262d;
}
.js-toggle {
    font-size: 11px; color: #58a6ff; cursor: pointer; user-select: none;
    padding: 4px 0;
}
.js-toggle:hover { text-decoration: underline; }
.js-output {
    font-family: 'SF Mono', 'JetBrains Mono', monospace;
    font-size: 11px; line-height: 1.4; color: #7ee787;
    max-height: 200px; overflow-y: auto; white-space: pre;
    padding: 8px; background: #0d1117; border-radius: 4px;
    border: 1px solid #21262d; display: none;
}
.badge {
    display: inline-block; padding: 2px 8px; border-radius: 10px;
    font-size: 10px; font-weight: 600; letter-spacing: 0.5px;
}
.badge-live { background: #1f6feb33; color: #58a6ff; }
</style>
</head>
<body>

<main>
    <section class="code-panel">
        <div class="editor-wrap">
            <pre class="editor-highlight" id="editor-highlight"></pre>
            <textarea class="editor-textarea" id="source-editor" spellcheck="false"></textarea>
        </div>
    </section>

    <section class="viz-panel">
        <div class="controls">
            <button id="btn-train" disabled>Loading compiler...</button>
            <button id="btn-stop" disabled>Stop</button>
            <select id="speed">
                <option value="5">5 ep/frame</option>
                <option value="20" selected>20 ep/frame</option>
                <option value="100">100 ep/frame</option>
            </select>
        </div>

        <div class="stats-bar">
            <div class="stat">
                <div class="label">Epoch</div>
                <div class="value" id="stat-epoch">0</div>
            </div>
            <div class="stat">
                <div class="label">Loss</div>
                <div class="value" id="stat-loss">&mdash;</div>
            </div>
            <div class="stat">
                <div class="label">Params</div>
                <div class="value" id="stat-params">&mdash;</div>
            </div>
            <div class="stat">
                <div class="label">Status</div>
                <div class="value" id="stat-status" style="color:#8b949e">Loading...</div>
            </div>
        </div>

        <div class="canvas-row">
            <div class="card">
                <div class="card-title">Decision Boundary</div>
                <canvas id="cv-boundary"></canvas>
            </div>
            <div class="card card-loss">
                <div class="card-title">Loss Curve</div>
                <canvas id="cv-loss"></canvas>
            </div>
        </div>

        <div class="card">
            <div class="card-title">3D Output Surface <span style="font-size:9px;color:#8b949e;text-transform:none;letter-spacing:0">(drag to rotate)</span></div>
            <canvas id="cv-3d" style="width:100%;aspect-ratio:2/1;cursor:grab"></canvas>
        </div>

        <div class="card">
            <div class="card-title">Predictions</div>
            <div class="predictions" id="predictions">
                <div class="pred-row"><span class="pred-input">0 xor 0</span><span class="pred-value" id="p00">&mdash;</span><span class="pred-exp">(exp: 0)</span></div>
                <div class="pred-row"><span class="pred-input">0 xor 1</span><span class="pred-value" id="p01">&mdash;</span><span class="pred-exp">(exp: 1)</span></div>
                <div class="pred-row"><span class="pred-input">1 xor 0</span><span class="pred-value" id="p10">&mdash;</span><span class="pred-exp">(exp: 1)</span></div>
                <div class="pred-row"><span class="pred-input">1 xor 1</span><span class="pred-value" id="p11">&mdash;</span><span class="pred-exp">(exp: 0)</span></div>
            </div>
        </div>

        <div class="card">
            <div class="card-title">Console</div>
            <div class="js-toggle" id="js-toggle">&#9654; Show generated JS</div>
            <div class="js-output" id="js-output"></div>
            <div class="console-out" id="console-out"></div>
        </div>
    </section>
</main>

<script>
// ================================================================
// DX SOURCE CODE (for display + compilation)
// ================================================================
const DX_SOURCE = `type Net
    w1 : Tensor[8 2]    -- 8 hidden × 2 input
    b1 : Vec[8]          -- 1 bias per hidden
    w2 : Tensor[1 8]    -- 1 output × 8 hidden
    b2 : Vec[1]          -- 1 bias output

fun Net'apply(x: Vec[2]) -> Vec[1]
    sigmoid(w1 o x + b1)
    sigmoid(w2 o it + b2)

net : Net(nn'init_normal(1.0))
lr : 0.5

X : [ [0 0]  [0 1]
      [1 0]  [1 1] ]

Y : [ [0] [1]
      [1] [0] ]

for epoch in 0..20000
    total_loss : 0.0
    for (x y) in X'zip(Y)
        pred : net(x)
        diff : pred - y
        loss : diff . diff
        grads : d loss / d net'params
        net'params : net'params - lr * grads
        total_loss : total_loss + loss
    if epoch % 4000 = 0
        print("epoch {epoch}  loss {total_loss}")

for (x y) in X'zip(Y)
    pred : net(x)
    print("{pred}")`;

// ================================================================
// SYNTAX HIGHLIGHTING
// ================================================================
function highlightDx(code) {
    let h = code.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');
    return h.split('\n').map(line => {
        const cmIdx = line.indexOf('--');
        let cp = cmIdx >= 0 ? line.slice(0, cmIdx) : line;
        let cm = cmIdx >= 0 ? '<span class="cm">' + line.slice(cmIdx) + '</span>' : '';
        cp = cp.replace(/("(?:[^"\\]|\\.)*?")/g, '<span class="str">$1</span>');
        cp = cp.replace(/\b(type|fun|for|if|in|print|arena|let|return)\b/g, '<span class="kw">$1</span>');
        cp = cp.replace(/\b(Net|Vec|Tensor|Float|Int|String)\b/g, '<span class="ty">$1</span>');
        cp = cp.replace(/\b(it|me)\b/g, '<span class="pr">$1</span>');
        cp = cp.replace(/\b(nn|mse|sigmoid|relu|softmax|gelu)\b/g, '<span class="fn">$1</span>');
        cp = cp.replace(/\bd\s+(\w+)\s*\/\s*d\s+(\w+)/g, '<span class="op">d</span> $1 / <span class="op">d</span> $2');
        cp = cp.replace(/ \. /g, ' \u00b7 ');
        cp = cp.replace(/\b(\d+\.?\d*)\b/g, '<span class="num">$1</span>');
        return cp + cm;
    }).join('\n');
}

function syncHighlight() {
    const ta = document.getElementById('source-editor');
    const pre = document.getElementById('editor-highlight');
    pre.innerHTML = highlightDx(ta.value) + '\n';
}

function syncScroll() {
    const ta = document.getElementById('source-editor');
    const pre = document.getElementById('editor-highlight');
    pre.scrollTop = ta.scrollTop;
    pre.scrollLeft = ta.scrollLeft;
}

// ================================================================
// EMBEDDED DX COMPILER MODULES
// ================================================================
const DX_MODULES = {
    "__init__.py": "",
    "lexer.py": "\"\"\"Lexer for dx 0.01.\n\nTokenizes dx source code into a stream of tokens with INDENT/DEDENT\ntracking, similar to Python's tokenizer.\n\"\"\"\n\nfrom __future__ import annotations\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom typing import Iterator\nimport re\nimport sys\n\n\nclass TT(Enum):\n    \"\"\"Token types.\"\"\"\n    # Literals\n    INT = auto()\n    FLOAT = auto()\n    STRING = auto()\n    IDENT = auto()\n\n    # Keywords\n    TYPE = auto()\n    FUN = auto()\n    FOR = auto()\n    IN = auto()\n    IF = auto()\n    ELSE = auto()\n    ARENA = auto()\n    AS = auto()\n    AND = auto()\n    OR = auto()\n    NOT = auto()\n    MATCH = auto()\n    IMPORT = auto()\n\n    # Pronouns\n    ME = auto()\n    IT = auto()\n\n    # Operators\n    COLON = auto()       # :\n    EQ = auto()          # =\n    NEQ = auto()         # <>\n    APOST = auto()       # '\n    MATMUL = auto()      # o (identifier 'o')\n    DOT = auto()          # .\n    DOTDOT = auto()      # ..\n    PLUS = auto()        # +\n    MINUS = auto()       # -\n    STAR = auto()        # *\n    SLASH = auto()       # /\n    PERCENT = auto()     # %\n    CARET = auto()       # ^\n    GT = auto()          # >\n    LT = auto()          # <\n    GE = auto()          # >=\n    LE = auto()          # <=\n    BANG = auto()        # !\n    BANGBANG = auto()    # !!\n    BANGBANGBANG = auto() # !!!\n    QUESTION = auto()    # ?\n    HASH = auto()        # #\n    MINUS_COLON = auto()    # -=\n    PIPE = auto()        # |\n    AMP = auto()         # &\n    TILDE = auto()       # ~\n\n    # Arrows\n    ARROW = auto()       # ->\n    FAT_ARROW = auto()   # =>\n\n    # Derivative\n    DERIV = auto()       # d (when used as derivative operator)\n\n    # Grouping\n    LPAREN = auto()      # (\n    RPAREN = auto()      # )\n    LBRACKET = auto()    # [\n    RBRACKET = auto()    # ]\n    LBRACE = auto()      # {\n    RBRACE = auto()      # }\n\n    # Structure\n    NEWLINE = auto()\n    INDENT = auto()\n    DEDENT = auto()\n    COMMA = auto()       # ,\n    EOF = auto()\n\n\nKEYWORDS = {\n    'type': TT.TYPE,\n    'fun': TT.FUN,\n    'for': TT.FOR,\n    'in': TT.IN,\n    'if': TT.IF,\n    'else': TT.ELSE,\n    'arena': TT.ARENA,\n    'as': TT.AS,\n    'and': TT.AND,\n    'or': TT.OR,\n    'not': TT.NOT,\n    'match': TT.MATCH,\n    'import': TT.IMPORT,\n    'me': TT.ME,\n    'it': TT.IT,\n}\n\n\n@dataclass\nclass Token:\n    type: TT\n    value: str\n    line: int\n    col: int\n\n    def __repr__(self):\n        if self.type in (TT.INDENT, TT.DEDENT, TT.NEWLINE, TT.EOF):\n            return f\"Token({self.type.name}, L{self.line})\"\n        return f\"Token({self.type.name}, {self.value!r}, L{self.line}:{self.col})\"\n\n\ndef tokenize(source: str) -> list[Token]:\n    \"\"\"Tokenize dx source code into a list of tokens.\"\"\"\n    tokens: list[Token] = []\n    lines = source.split('\\n')\n    indent_stack = [0]\n    line_num = 0\n    # Bracket nesting depth \u2014 suppress INDENT/DEDENT/NEWLINE when > 0\n    # (same rule as Python: implicit line continuation inside (), [], {})\n    bracket_depth = 0\n\n    # State: are we at the beginning of a line (need to handle indentation)?\n    i = 0\n    total = len(source)\n\n    def peek(offset=0):\n        pos = i + offset\n        return source[pos] if pos < total else '\\0'\n\n    def add(tt, val, ln, cl):\n        tokens.append(Token(tt, val, ln, cl))\n\n    while i < total:\n        ch = source[i]\n\n        # --- Newline ---\n        if ch == '\\n':\n            line_num += 1\n            # Inside brackets: implicit line continuation (no NEWLINE/INDENT/DEDENT)\n            if bracket_depth > 0:\n                i += 1\n                continue\n\n            # Emit NEWLINE (collapse multiple newlines)\n            if tokens and tokens[-1].type not in (TT.NEWLINE, TT.INDENT, TT.DEDENT):\n                add(TT.NEWLINE, '\\\\n', line_num, 0)\n            i += 1\n\n            # Handle indentation of the next non-blank, non-comment line\n            # Skip blank lines and comment-only lines\n            while i < total:\n                # Count spaces at start of this line\n                indent = 0\n                while i < total and source[i] == ' ':\n                    indent += 1\n                    i += 1\n\n                # If we hit end of file or newline, it's a blank line \u2014 skip\n                if i >= total:\n                    break\n                if source[i] == '\\n':\n                    line_num += 1\n                    if tokens and tokens[-1].type not in (TT.NEWLINE, TT.INDENT, TT.DEDENT):\n                        add(TT.NEWLINE, '\\\\n', line_num, 0)\n                    i += 1\n                    continue\n\n                # Check for comment-only line\n                if source[i] == '-' and i + 1 < total and source[i + 1] == '-':\n                    # Skip to end of line\n                    while i < total and source[i] != '\\n':\n                        i += 1\n                    continue\n\n                # Non-blank, non-comment line \u2014 process indentation\n                break\n\n            if i >= total:\n                break\n\n            current_indent = indent_stack[-1]\n            if indent > current_indent:\n                indent_stack.append(indent)\n                add(TT.INDENT, '', line_num, 0)\n            else:\n                while indent < indent_stack[-1]:\n                    indent_stack.pop()\n                    add(TT.DEDENT, '', line_num, 0)\n                if indent != indent_stack[-1]:\n                    raise SyntaxError(\n                        f\"Inconsistent indentation at line {line_num}: \"\n                        f\"expected {indent_stack[-1]}, got {indent}\"\n                    )\n            continue\n\n        # --- Whitespace (not newline) ---\n        if ch == ' ' or ch == '\\t':\n            i += 1\n            continue\n\n        # --- Comment: -- to end of line ---\n        if ch == '-' and peek(1) == '-' and peek(2) != '>':\n            # But check it's not -> (arrow)\n            while i < total and source[i] != '\\n':\n                i += 1\n            continue\n\n        # Track line/col for token start\n        # Compute current line and col\n        col = 0\n        j = i - 1\n        while j >= 0 and source[j] != '\\n':\n            col += 1\n            j -= 1\n\n        # --- String literal ---\n        if ch == '\"':\n            start = i\n            i += 1\n            val = []\n            while i < total and source[i] != '\"':\n                if source[i] == '\\\\':\n                    i += 1\n                    if i < total:\n                        esc = source[i]\n                        if esc == 'n':\n                            val.append('\\n')\n                        elif esc == 't':\n                            val.append('\\t')\n                        elif esc == '\"':\n                            val.append('\"')\n                        elif esc == '\\\\':\n                            val.append('\\\\')\n                        else:\n                            val.append(esc)\n                else:\n                    val.append(source[i])\n                i += 1\n            if i < total:\n                i += 1  # skip closing quote\n            add(TT.STRING, ''.join(val), line_num + 1, col)\n            continue\n\n        # --- Number ---\n        if ch.isdigit() or (ch == '-' and peek(1).isdigit() and\n                            (not tokens or tokens[-1].type in\n                             (TT.LPAREN, TT.LBRACKET, TT.COMMA, TT.COLON,\n                              TT.NEWLINE, TT.INDENT, TT.FAT_ARROW,\n                              TT.MATMUL, TT.DOT, TT.PLUS, TT.MINUS,\n                              TT.STAR, TT.SLASH, TT.CARET))):\n            start = i\n            if ch == '-':\n                i += 1\n            while i < total and source[i].isdigit():\n                i += 1\n            is_float = False\n            if i < total and source[i] == '.' and peek(1) != '.':\n                is_float = True\n                i += 1\n                while i < total and source[i].isdigit():\n                    i += 1\n            # Scientific notation: e/E followed by optional +/- and digits\n            if i < total and source[i] in ('e', 'E'):\n                e_pos = i + 1\n                if e_pos < total and source[e_pos] in ('+', '-'):\n                    e_pos += 1\n                if e_pos < total and source[e_pos].isdigit():\n                    is_float = True\n                    i = e_pos\n                    while i < total and source[i].isdigit():\n                        i += 1\n            val = source[start:i]\n            if is_float:\n                add(TT.FLOAT, val, line_num + 1, col)\n            else:\n                add(TT.INT, val, line_num + 1, col)\n            continue\n\n        # --- Identifier / keyword ---\n        if ch.isalpha() or ch == '_':\n            start = i\n            while i < total and (source[i].isalnum() or source[i] == '_'):\n                i += 1\n            word = source[start:i]\n\n            # 'd' as derivative operator: d followed by space then identifier\n            if word == 'd' and i < total and source[i] == ' ':\n                # Look ahead to see if this is `d param(expr)` pattern\n                # d is derivative when followed by an identifier (not 'def' etc.)\n                save = i\n                i += 1  # skip space\n                while i < total and source[i] == ' ':\n                    i += 1\n                if i < total and (source[i].isalpha() or source[i] == '_'):\n                    add(TT.DERIV, 'd', line_num + 1, col)\n                    i = save  # let the next iteration handle the param\n                    i += 1  # skip the space\n                    continue\n                else:\n                    i = save\n\n            # 'o' as matmul operator\n            if word == 'o':\n                add(TT.MATMUL, 'o', line_num + 1, col)\n                continue\n\n            # Check for keyword\n            if word in KEYWORDS:\n                add(KEYWORDS[word], word, line_num + 1, col)\n            else:\n                add(TT.IDENT, word, line_num + 1, col)\n            continue\n\n        # --- Operators and punctuation ---\n        # Multi-char first\n        two = source[i:i+2] if i + 1 < total else ''\n        three = source[i:i+3] if i + 2 < total else ''\n\n        if three == '!!!':\n            add(TT.BANGBANGBANG, '!!!', line_num + 1, col)\n            i += 3\n            continue\n        if two == '!!':\n            add(TT.BANGBANG, '!!', line_num + 1, col)\n            i += 2\n            continue\n        if two == '-:':\n            add(TT.MINUS_COLON, '-:', line_num + 1, col)\n            i += 2\n            continue\n        if two == '->':\n            add(TT.ARROW, '->', line_num + 1, col)\n            i += 2\n            continue\n        if two == '=>':\n            add(TT.FAT_ARROW, '=>', line_num + 1, col)\n            i += 2\n            continue\n        if two == '<>':\n            add(TT.NEQ, '<>', line_num + 1, col)\n            i += 2\n            continue\n        if two == '>=':\n            add(TT.GE, '>=', line_num + 1, col)\n            i += 2\n            continue\n        if two == '<=':\n            add(TT.LE, '<=', line_num + 1, col)\n            i += 2\n            continue\n        if two == '..':\n            add(TT.DOTDOT, '..', line_num + 1, col)\n            i += 2\n            continue\n\n        # Single char\n        single = {\n            ':': TT.COLON,\n            '=': TT.EQ,\n            '+': TT.PLUS,\n            '-': TT.MINUS,\n            '*': TT.STAR,\n            '/': TT.SLASH,\n            '%': TT.PERCENT,\n            '^': TT.CARET,\n            '>': TT.GT,\n            '<': TT.LT,\n            '!': TT.BANG,\n            '?': TT.QUESTION,\n            '#': TT.HASH,\n            '|': TT.PIPE,\n            '&': TT.AMP,\n            '(': TT.LPAREN,\n            ')': TT.RPAREN,\n            '[': TT.LBRACKET,\n            ']': TT.RBRACKET,\n            '{': TT.LBRACE,\n            '}': TT.RBRACE,\n            ',': TT.COMMA,\n            \"'\": TT.APOST,\n            '.': TT.DOT,\n            '~': TT.TILDE,\n        }\n\n        if ch in single:\n            # Track bracket nesting for implicit line continuation\n            if ch in ('(', '[', '{'):\n                bracket_depth += 1\n            elif ch in (')', ']', '}'):\n                bracket_depth = max(0, bracket_depth - 1)\n            add(single[ch], ch, line_num + 1, col)\n            i += 1\n            continue\n\n        raise SyntaxError(f\"Unexpected character {ch!r} at line {line_num + 1}, col {col}\")\n\n    # Emit final NEWLINE if needed\n    if tokens and tokens[-1].type not in (TT.NEWLINE, TT.DEDENT):\n        add(TT.NEWLINE, '\\\\n', line_num + 1, 0)\n\n    # Close remaining indents\n    while len(indent_stack) > 1:\n        indent_stack.pop()\n        add(TT.DEDENT, '', line_num + 1, 0)\n\n    add(TT.EOF, '', line_num + 1, 0)\n    return tokens\n\n\nif __name__ == '__main__':\n    if len(sys.argv) < 2:\n        print(\"Usage: python -m dx.lexer <file.dx>\")\n        sys.exit(1)\n    with open(sys.argv[1]) as f:\n        source = f.read()\n    for tok in tokenize(source):\n        print(tok)\n",
    "ast_nodes.py": "\"\"\"AST node definitions for dx 0.01.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n\n# --- Base ---\n\n@dataclass\nclass Node:\n    line: int = 0\n    col: int = 0\n\n\n# --- Types ---\n\n@dataclass\nclass TypeAnnotation(Node):\n    \"\"\"A type reference like Vec[784] or Tensor[128 784].\"\"\"\n    name: str = \"\"\n    dims: list[int] = field(default_factory=list)\n\n\n# --- Expressions ---\n\n@dataclass\nclass Num(Node):\n    value: float = 0.0\n    is_int: bool = True\n\n\n@dataclass\nclass Str(Node):\n    value: str = \"\"\n\n\n@dataclass\nclass Ident(Node):\n    name: str = \"\"\n\n\n@dataclass\nclass Pronoun(Node):\n    \"\"\"me or it.\"\"\"\n    name: str = \"\"\n\n\n@dataclass\nclass VecLiteral(Node):\n    elements: list = field(default_factory=list)\n\n\n@dataclass\nclass SetLiteral(Node):\n    elements: list = field(default_factory=list)  # list of Ident nodes\n\n\n@dataclass\nclass BinOp(Node):\n    op: str = \"\"\n    left: Node = None\n    right: Node = None\n\n\n@dataclass\nclass UnaryOp(Node):\n    op: str = \"\"\n    operand: Node = None\n\n\n@dataclass\nclass MemberAccess(Node):\n    \"\"\"obj'field.\"\"\"\n    obj: Node = None\n    field_name: str = \"\"\n\n\n@dataclass\nclass MentionAccess(Node):\n    \"\"\"obj''field \u2014 metadata access (e.g. person''name).\"\"\"\n    obj: Node = None\n    field_name: str = \"\"\n\n\n@dataclass\nclass Call(Node):\n    func: Node = None\n    args: list = field(default_factory=list)\n    kwargs: dict = field(default_factory=dict)\n\n\n@dataclass\nclass MethodCall(Node):\n    \"\"\"obj'method(args) \u2014 sugar for Type_method(obj, args).\"\"\"\n    obj: Node = None\n    method: str = \"\"\n    args: list = field(default_factory=list)\n    kwargs: dict = field(default_factory=dict)\n\n\n@dataclass\nclass Index(Node):\n    obj: Node = None\n    index: Node = None\n\n\n@dataclass\nclass Range(Node):\n    start: Node = None\n    end: Node = None\n\n\n@dataclass\nclass Lambda(Node):\n    params: list[str] = field(default_factory=list)\n    body: Node = None\n\n\n@dataclass\nclass AsExpr(Node):\n    \"\"\"x as Type.\"\"\"\n    expr: Node = None\n    target_type: str = \"\"\n\n\n@dataclass\nclass DerivExpr(Node):\n    \"\"\"d expr / d param \u2014 Leibniz notation for derivative.\"\"\"\n    param: Node = None  # MemberAccess or Ident\n    expr: Node = None\n\n\n@dataclass\nclass NormExpr(Node):\n    \"\"\"|expr| \u2014 L2 norm or absolute value.\"\"\"\n    expr: Node = None\n\n\n# --- Statements ---\n\n@dataclass\nclass Assignment(Node):\n    target: Node = None  # Ident, tuple destructure, or MemberAccess\n    value: Node = None\n\n\n@dataclass\nclass CompoundAssign(Node):\n    \"\"\"target -= value\"\"\"\n    target: Node = None   # MemberAccess (model'params)\n    value: Node = None    # BinOp(0.001 * grads)\n\n\n@dataclass\nclass TupleDestructure(Node):\n    \"\"\"(a b c) on the left of :.\"\"\"\n    names: list[str] = field(default_factory=list)\n\n\n@dataclass\nclass TypeDef(Node):\n    name: str = \"\"\n    fields: list[tuple[str, 'TypeAnnotation']] = field(default_factory=list)\n\n\n@dataclass\nclass FunDef(Node):\n    type_name: str = \"\"  # \"\" for free functions\n    method_name: str = \"\"\n    params: list[tuple[str, Optional['TypeAnnotation']]] = field(default_factory=list)\n    return_type: Optional['TypeAnnotation'] = None\n    body: list = field(default_factory=list)\n\n\n@dataclass\nclass ForLoop(Node):\n    var: Node = None  # Ident or TupleDestructure\n    iterable: Node = None\n    body: list = field(default_factory=list)\n\n\n@dataclass\nclass IfExpr(Node):\n    condition: Node = None\n    body: list = field(default_factory=list)\n    else_body: list = field(default_factory=list)\n\n\n@dataclass\nclass ArenaBlock(Node):\n    name: str = \"\"\n    body: list = field(default_factory=list)\n\n\n@dataclass\nclass PrintStmt(Node):\n    \"\"\"print(...) \u2014 special-cased for string interpolation.\"\"\"\n    format_str: str = \"\"\n    args: list = field(default_factory=list)\n\n\n@dataclass\nclass ExprStmt(Node):\n    \"\"\"An expression used as a statement (its value becomes `it`).\"\"\"\n    expr: Node = None\n\n\n@dataclass\nclass Program(Node):\n    statements: list = field(default_factory=list)\n\n    def pretty(self, max_stmts=50) -> str:\n        lines = [f\"Program(line={self.line}, col={self.col})\"]\n        for i, s in enumerate(self.statements[:max_stmts]):\n            lines.append(f\"  [{i:03}] {type(s).__name__} @ {getattr(s,'line', '?')}:{getattr(s,'col','?')}\")\n        if len(self.statements) > max_stmts:\n            lines.append(f\"  ... ({len(self.statements)-max_stmts} more)\")\n        return \"\\n\".join(lines)\n",
    "parser.py": "\"\"\"Parser for dx 0.01.\n\nParses a token stream into an AST. Handles:\n- Type definitions, function definitions\n- For loops, if/else, arena blocks\n- Operators with correct precedence\n- Member access ('), method calls\n- Pronouns (me, it), lambdas\n- Tuple destructuring\n\"\"\"\n\nfrom __future__ import annotations\nimport sys\nfrom .lexer import Token, TT, tokenize\nfrom . import ast_nodes as ast\n\n\nclass ParseError(Exception):\n    def __init__(self, msg, token=None):\n        self.token = token\n        loc = f\" at line {token.line}\" if token else \"\"\n        super().__init__(f\"{msg}{loc}\")\n\n\nclass Parser:\n    def __init__(self, tokens: list[Token]):\n        self.tokens = tokens\n        self.pos = 0\n\n    # --- Utilities ---\n\n    def peek(self, offset=0) -> Token:\n        p = self.pos + offset\n        if p < len(self.tokens):\n            return self.tokens[p]\n        return self.tokens[-1]  # EOF\n\n    def at(self, *types) -> bool:\n        return self.peek().type in types\n\n    def eat(self, tt: TT) -> Token:\n        tok = self.peek()\n        if tok.type != tt:\n            raise ParseError(f\"Expected {tt.name}, got {tok.type.name} ({tok.value!r})\", tok)\n        self.pos += 1\n        return tok\n\n    def maybe(self, tt: TT) -> Token | None:\n        if self.at(tt):\n            return self.eat(tt)\n        return None\n\n    def skip_newlines(self):\n        while self.at(TT.NEWLINE):\n            self.pos += 1\n\n    # --- Program ---\n\n    def skip_whitespace(self):\n        \"\"\"Skip newlines, indents, and dedents (e.g. after multi-line brackets).\"\"\"\n        while self.at(TT.NEWLINE) or self.at(TT.INDENT) or self.at(TT.DEDENT):\n            self.pos += 1\n\n    def parse(self) -> ast.Program:\n        self.skip_whitespace()\n        stmts = []\n        while not self.at(TT.EOF):\n            stmt = self.parse_statement()\n            if stmt is not None:\n                stmts.append(stmt)\n            self.skip_whitespace()\n        return ast.Program(statements=stmts)\n\n    # --- Statements ---\n\n    def parse_statement(self) -> ast.Node | None:\n        tok = self.peek()\n\n        if tok.type == TT.TYPE:\n            return self.parse_typedef()\n        if tok.type == TT.FUN:\n            return self.parse_fundef()\n        if tok.type == TT.FOR:\n            return self.parse_for()\n        if tok.type == TT.IF:\n            return self.parse_if()\n        if tok.type == TT.ARENA:\n            return self.parse_arena()\n        if tok.type == TT.NEWLINE:\n            self.pos += 1\n            return None\n\n        # Assignment or expression statement\n        return self.parse_assignment_or_expr()\n\n    def parse_assignment_or_expr(self) -> ast.Node:\n        \"\"\"Parse assignment (target : value) or expression statement.\"\"\"\n        tok = self.peek()\n\n        # Tuple destructuring: (a b) : expr\n        if tok.type == TT.LPAREN:\n            # Look ahead to see if this is (names) : ...\n            save = self.pos\n            if self._try_tuple_destructure():\n                # It was a tuple destructure assignment\n                return self._parsed_tuple_assign\n            self.pos = save\n\n        # Try parsing an expression; if followed by :, it's an assignment\n        expr = self.parse_expr()\n\n        if self.at(TT.COLON):\n            self.eat(TT.COLON)\n            value = self.parse_expr()\n            return ast.Assignment(target=expr, value=value, line=tok.line, col=tok.col)\n\n        if self.at(TT.MINUS_COLON):\n            self.eat(TT.MINUS_COLON)\n            value = self.parse_expr()\n            return ast.CompoundAssign(target=expr, value=value, line=tok.line, col=tok.col)\n\n        # Expression statement (value becomes `it`)\n        return ast.ExprStmt(expr=expr, line=tok.line, col=tok.col)\n\n    def _try_tuple_destructure(self) -> bool:\n        \"\"\"Try to parse (a b c) : expr. Returns True on success.\"\"\"\n        tok = self.peek()\n        self.eat(TT.LPAREN)\n        names = []\n        while not self.at(TT.RPAREN):\n            if self.at(TT.IDENT):\n                names.append(self.eat(TT.IDENT).value)\n            elif self.at(TT.IT):\n                names.append(self.eat(TT.IT).value)\n            elif self.at(TT.ME):\n                names.append(self.eat(TT.ME).value)\n            else:\n                return False\n        self.eat(TT.RPAREN)\n\n        if not self.at(TT.COLON):\n            return False\n\n        self.eat(TT.COLON)\n        value = self.parse_expr()\n        target = ast.TupleDestructure(names=names, line=tok.line, col=tok.col)\n        self._parsed_tuple_assign = ast.Assignment(\n            target=target, value=value, line=tok.line, col=tok.col\n        )\n        return True\n\n    # --- Type Definition ---\n\n    def parse_typedef(self) -> ast.TypeDef:\n        tok = self.eat(TT.TYPE)\n        name = self.eat(TT.IDENT).value\n        self.skip_newlines()\n        self.eat(TT.INDENT)\n\n        fields = []\n        while not self.at(TT.DEDENT) and not self.at(TT.EOF):\n            self.skip_newlines()\n            if self.at(TT.DEDENT):\n                break\n            fname = self.eat(TT.IDENT).value\n            self.eat(TT.COLON)\n            ftype = self.parse_type_annotation()\n            fields.append((fname, ftype))\n            self.skip_newlines()\n\n        self.maybe(TT.DEDENT)\n        return ast.TypeDef(name=name, fields=fields, line=tok.line, col=tok.col)\n\n    def parse_type_annotation(self) -> ast.TypeAnnotation:\n        \"\"\"Parse a type like Vec[784] or Tensor[128 784] or Float.\"\"\"\n        name_tok = self.eat(TT.IDENT)\n        dims = []\n        if self.at(TT.LBRACKET):\n            self.eat(TT.LBRACKET)\n            while not self.at(TT.RBRACKET):\n                dim = self.eat(TT.INT)\n                dims.append(int(dim.value))\n            self.eat(TT.RBRACKET)\n        return ast.TypeAnnotation(name=name_tok.value, dims=dims,\n                                  line=name_tok.line, col=name_tok.col)\n\n    # --- Function Definition ---\n\n    def parse_fundef(self) -> ast.FunDef:\n        tok = self.eat(TT.FUN)\n        # Parse name: could be Type'method or just func_name\n        name = self.eat(TT.IDENT).value\n        type_name = \"\"\n        method_name = name\n\n        if self.at(TT.APOST):\n            self.eat(TT.APOST)\n            type_name = name\n            method_name = self.eat(TT.IDENT).value\n\n        # Parameters\n        self.eat(TT.LPAREN)\n        params = []\n        while not self.at(TT.RPAREN):\n            pname = self.eat(TT.IDENT).value\n            ptype = None\n            if self.at(TT.COLON):\n                self.eat(TT.COLON)\n                ptype = self.parse_type_annotation()\n            params.append((pname, ptype))\n        self.eat(TT.RPAREN)\n\n        # Return type\n        ret_type = None\n        if self.at(TT.ARROW):\n            self.eat(TT.ARROW)\n            ret_type = self.parse_type_annotation()\n\n        # Body\n        self.skip_newlines()\n        body = self.parse_block()\n\n        return ast.FunDef(\n            type_name=type_name, method_name=method_name,\n            params=params, return_type=ret_type, body=body,\n            line=tok.line, col=tok.col\n        )\n\n    # --- Block ---\n\n    def parse_block(self) -> list[ast.Node]:\n        \"\"\"Parse an indented block of statements.\"\"\"\n        self.eat(TT.INDENT)\n        stmts = []\n        while not self.at(TT.DEDENT) and not self.at(TT.EOF):\n            self.skip_newlines()\n            if self.at(TT.DEDENT) or self.at(TT.EOF):\n                break\n            stmt = self.parse_statement()\n            if stmt is not None:\n                stmts.append(stmt)\n            self.skip_newlines()\n        self.maybe(TT.DEDENT)\n        return stmts\n\n    # --- For ---\n\n    def parse_for(self) -> ast.ForLoop:\n        tok = self.eat(TT.FOR)\n\n        # Loop variable: ident or (a b)\n        if self.at(TT.LPAREN):\n            self.eat(TT.LPAREN)\n            names = []\n            while not self.at(TT.RPAREN):\n                names.append(self.eat(TT.IDENT).value)\n                self.maybe(TT.COMMA)\n            self.eat(TT.RPAREN)\n            var = ast.TupleDestructure(names=names, line=tok.line)\n        else:\n            var = ast.Ident(name=self.eat(TT.IDENT).value, line=tok.line)\n\n        self.eat(TT.IN)\n        iterable = self.parse_expr()\n        self.skip_newlines()\n        body = self.parse_block()\n\n        return ast.ForLoop(var=var, iterable=iterable, body=body,\n                           line=tok.line, col=tok.col)\n\n    # --- If ---\n\n    def parse_if(self) -> ast.IfExpr:\n        tok = self.eat(TT.IF)\n        cond = self.parse_expr()\n        self.skip_newlines()\n        body = self.parse_block()\n        else_body = []\n        if self.at(TT.ELSE):\n            self.eat(TT.ELSE)\n            self.skip_newlines()\n            else_body = self.parse_block()\n        return ast.IfExpr(condition=cond, body=body, else_body=else_body,\n                          line=tok.line, col=tok.col)\n\n    # --- Arena ---\n\n    def parse_arena(self) -> ast.ArenaBlock:\n        tok = self.eat(TT.ARENA)\n        name = self.eat(TT.IDENT).value\n        self.skip_newlines()\n        body = self.parse_block()\n        return ast.ArenaBlock(name=name, body=body, line=tok.line, col=tok.col)\n\n    # --- Expressions (Pratt parser) ---\n\n    def parse_expr(self) -> ast.Node:\n        \"\"\"Top-level expression parser.\"\"\"\n        return self.parse_lambda()\n\n    def parse_lambda(self) -> ast.Node:\n        \"\"\"Parse lambda: (params) => body, or fall through to lower precedence.\"\"\"\n        # Check for lambda: (params) => ...\n        if self.at(TT.LPAREN):\n            save = self.pos\n            if self._try_lambda():\n                return self._parsed_lambda\n            self.pos = save\n\n        return self.parse_or()\n\n    def _try_lambda(self) -> bool:\n        \"\"\"Try parsing (params) => expr. Returns True on success.\"\"\"\n        tok = self.peek()\n        self.eat(TT.LPAREN)\n        params = []\n        while not self.at(TT.RPAREN):\n            if self.at(TT.IDENT):\n                params.append(self.eat(TT.IDENT).value)\n            else:\n                return False\n        self.eat(TT.RPAREN)\n\n        if not self.at(TT.FAT_ARROW):\n            return False\n\n        self.eat(TT.FAT_ARROW)\n        body = self.parse_expr()\n        self._parsed_lambda = ast.Lambda(params=params, body=body,\n                                         line=tok.line, col=tok.col)\n        return True\n\n    def parse_or(self) -> ast.Node:\n        left = self.parse_and()\n        while self.at(TT.OR):\n            op = self.eat(TT.OR)\n            right = self.parse_and()\n            left = ast.BinOp(op='or', left=left, right=right, line=op.line)\n        return left\n\n    def parse_and(self) -> ast.Node:\n        left = self.parse_not()\n        while self.at(TT.AND):\n            op = self.eat(TT.AND)\n            right = self.parse_not()\n            left = ast.BinOp(op='and', left=left, right=right, line=op.line)\n        return left\n\n    def parse_not(self) -> ast.Node:\n        if self.at(TT.NOT):\n            op = self.eat(TT.NOT)\n            operand = self.parse_not()\n            return ast.UnaryOp(op='not', operand=operand, line=op.line)\n        return self.parse_comparison()\n\n    def parse_comparison(self) -> ast.Node:\n        left = self.parse_cosine_sim()\n        while self.at(TT.EQ, TT.NEQ, TT.LT, TT.GT, TT.LE, TT.GE):\n            op = self.peek()\n            self.pos += 1\n            right = self.parse_cosine_sim()\n            left = ast.BinOp(op=op.value, left=left, right=right, line=op.line)\n        return left\n\n    def parse_cosine_sim(self) -> ast.Node:\n        left = self.parse_addition()\n        while self.at(TT.TILDE):\n            op = self.eat(TT.TILDE)\n            right = self.parse_addition()\n            left = ast.BinOp(op='~', left=left, right=right, line=op.line)\n        return left\n\n    def parse_addition(self) -> ast.Node:\n        left = self.parse_dot_product()\n        while self.at(TT.PLUS, TT.MINUS):\n            op = self.peek()\n            self.pos += 1\n            right = self.parse_dot_product()\n            left = ast.BinOp(op=op.value, left=left, right=right, line=op.line)\n        return left\n\n    def parse_dot_product(self) -> ast.Node:\n        left = self.parse_matmul()\n        while self.at(TT.DOT) and not self._is_dotdot():\n            op = self.eat(TT.DOT)\n            right = self.parse_matmul()\n            left = ast.BinOp(op='.', left=left, right=right, line=op.line)\n        return left\n\n    def _is_dotdot(self) -> bool:\n        return self.peek().type == TT.DOTDOT\n\n    def parse_matmul(self) -> ast.Node:\n        left = self.parse_multiplication()\n        while self.at(TT.MATMUL):\n            op = self.eat(TT.MATMUL)\n            right = self.parse_multiplication()\n            left = ast.BinOp(op='o', left=left, right=right, line=op.line)\n        return left\n\n    def parse_multiplication(self) -> ast.Node:\n        left = self.parse_unary()\n        while self.at(TT.STAR, TT.SLASH, TT.PERCENT):\n            op = self.peek()\n            self.pos += 1\n            right = self.parse_unary()\n            left = ast.BinOp(op=op.value, left=left, right=right, line=op.line)\n        return left\n\n    def parse_unary(self) -> ast.Node:\n        if self.at(TT.MINUS):\n            op = self.eat(TT.MINUS)\n            operand = self.parse_power()\n            return ast.UnaryOp(op='-', operand=operand, line=op.line)\n        return self.parse_power()\n\n    def parse_power(self) -> ast.Node:\n        base = self.parse_postfix()\n        if self.at(TT.CARET):\n            self.eat(TT.CARET)\n            # Right-associative\n            exp = self.parse_unary()\n            return ast.BinOp(op='^', left=base, right=exp, line=base.line)\n        return base\n\n    def parse_postfix(self) -> ast.Node:\n        \"\"\"Parse postfix operations: member access, call, index, as.\"\"\"\n        node = self.parse_primary()\n\n        while True:\n            if self.at(TT.APOST):\n                node = self._parse_member_or_method(node)\n            elif self.at(TT.LPAREN):\n                # Function/method call \u2014 only if it's directly adjacent\n                # (no newline between)\n                node = self._parse_call(node)\n            elif self.at(TT.LBRACKET):\n                self.eat(TT.LBRACKET)\n                idx = self.parse_expr()\n                self.eat(TT.RBRACKET)\n                node = ast.Index(obj=node, index=idx, line=node.line)\n            elif self.at(TT.AS):\n                self.eat(TT.AS)\n                target = self.eat(TT.IDENT).value\n                node = ast.AsExpr(expr=node, target_type=target, line=node.line)\n            elif self.at(TT.DOTDOT):\n                # Range operator: expr..expr\n                self.eat(TT.DOTDOT)\n                end = self.parse_addition()\n                node = ast.Range(start=node, end=end, line=node.line)\n            else:\n                break\n\n        return node\n\n    def _parse_member_or_method(self, obj: ast.Node) -> ast.Node:\n        \"\"\"Parse obj'field, obj'method(args), or obj''field (mention).\"\"\"\n        self.eat(TT.APOST)\n\n        # Double apostrophe '' \u2192 mention access (e.g. person''name)\n        if self.at(TT.APOST):\n            self.eat(TT.APOST)\n            name = self.eat(TT.IDENT).value\n            return ast.MentionAccess(obj=obj, field_name=name, line=obj.line)\n\n        name = self.eat(TT.IDENT).value\n\n        # Check if followed by ( \u2014 method call\n        if self.at(TT.LPAREN):\n            args, kwargs = self._parse_arglist()\n            return ast.MethodCall(obj=obj, method=name, args=args,\n                                  kwargs=kwargs, line=obj.line)\n\n        # Check if followed by another ' \u2014 chained access: obj'a'b\n        node = ast.MemberAccess(obj=obj, field_name=name, line=obj.line)\n        return node\n\n    def _parse_call(self, func: ast.Node) -> ast.Node:\n        \"\"\"Parse f(args).\"\"\"\n        args, kwargs = self._parse_arglist()\n        return ast.Call(func=func, args=args, kwargs=kwargs, line=func.line)\n\n    def _parse_arglist(self) -> tuple[list, dict]:\n        \"\"\"Parse (arg1, arg2, key: val).\"\"\"\n        self.eat(TT.LPAREN)\n        args = []\n        kwargs = {}\n        first = True\n\n        while not self.at(TT.RPAREN):\n            if not first:\n                self.eat(TT.COMMA)\n            first = False\n            # Check for keyword arg: ident : expr\n            if self.at(TT.IDENT) and self.peek(1).type == TT.COLON:\n                key = self.eat(TT.IDENT).value\n                self.eat(TT.COLON)\n                val = self.parse_expr()\n                kwargs[key] = val\n            else:\n                args.append(self.parse_expr())\n\n        self.eat(TT.RPAREN)\n        return args, kwargs\n\n    # --- Primary ---\n\n    def parse_primary(self) -> ast.Node:\n        tok = self.peek()\n\n        if tok.type == TT.INT:\n            self.pos += 1\n            return ast.Num(value=int(tok.value), is_int=True,\n                           line=tok.line, col=tok.col)\n\n        if tok.type == TT.FLOAT:\n            self.pos += 1\n            return ast.Num(value=float(tok.value), is_int=False,\n                           line=tok.line, col=tok.col)\n\n        if tok.type == TT.STRING:\n            self.pos += 1\n            return ast.Str(value=tok.value, line=tok.line, col=tok.col)\n\n        if tok.type == TT.ME:\n            self.pos += 1\n            return ast.Pronoun(name='me', line=tok.line, col=tok.col)\n\n        if tok.type == TT.IT:\n            self.pos += 1\n            return ast.Pronoun(name='it', line=tok.line, col=tok.col)\n\n        if tok.type == TT.DERIV:\n            return self.parse_deriv()\n\n        if tok.type == TT.IDENT:\n            self.pos += 1\n            return ast.Ident(name=tok.value, line=tok.line, col=tok.col)\n\n        if tok.type == TT.LPAREN:\n            # Parenthesized expression\n            self.eat(TT.LPAREN)\n            expr = self.parse_expr()\n            self.eat(TT.RPAREN)\n            return expr\n\n        if tok.type == TT.LBRACKET:\n            return self.parse_vec_literal()\n\n        if tok.type == TT.LBRACE:\n            return self.parse_set_literal()\n\n        if tok.type == TT.PIPE:\n            return self.parse_norm()\n\n        raise ParseError(f\"Unexpected token {tok.type.name} ({tok.value!r})\", tok)\n\n    def parse_deriv(self) -> ast.DerivExpr:\n        \"\"\"Parse Leibniz notation: d <expr> / d <param>\"\"\"\n        tok = self.eat(TT.DERIV)\n        expr = self.parse_primary()\n        self.eat(TT.SLASH)\n        self.eat(TT.DERIV)\n        param = self.parse_primary()\n        while self.at(TT.APOST):\n            self.eat(TT.APOST)\n            name = self.eat(TT.IDENT).value\n            param = ast.MemberAccess(obj=param, field_name=name, line=param.line)\n        return ast.DerivExpr(param=param, expr=expr, line=tok.line, col=tok.col)\n\n    def parse_vec_literal(self) -> ast.VecLiteral:\n        tok = self.eat(TT.LBRACKET)\n        elements = []\n        while not self.at(TT.RBRACKET):\n            # Skip whitespace tokens inside brackets (multi-line literals)\n            while self.at(TT.NEWLINE) or self.at(TT.INDENT) or self.at(TT.DEDENT):\n                self.pos += 1\n            if self.at(TT.RBRACKET):\n                break\n            if self.at(TT.LBRACKET):\n                # Nested: [[0 0] [0 1] ...] \u2014 list of vectors\n                elements.append(self.parse_vec_literal())\n            else:\n                elements.append(self.parse_expr())\n            self.maybe(TT.COMMA)\n        self.eat(TT.RBRACKET)\n        return ast.VecLiteral(elements=elements, line=tok.line, col=tok.col)\n\n    def parse_set_literal(self) -> ast.SetLiteral:\n        tok = self.eat(TT.LBRACE)\n        elements = []\n        while not self.at(TT.RBRACE):\n            elements.append(self.parse_expr())\n            self.maybe(TT.COMMA)\n        self.eat(TT.RBRACE)\n        return ast.SetLiteral(elements=elements, line=tok.line, col=tok.col)\n\n    def parse_norm(self) -> ast.NormExpr:\n        \"\"\"Parse |expr|.\"\"\"\n        tok = self.eat(TT.PIPE)\n        expr = self.parse_expr()\n        self.eat(TT.PIPE)\n        return ast.NormExpr(expr=expr, line=tok.line, col=tok.col)\n\n\ndef parse(source: str) -> ast.Program:\n    \"\"\"Parse dx source code into an AST.\"\"\"\n    tokens = tokenize(source)\n    parser = Parser(tokens)\n    return parser.parse()\n\n\ndef print_ast(node, indent=0):\n    \"\"\"Pretty-print an AST node.\"\"\"\n    prefix = \"  \" * indent\n    if isinstance(node, ast.Program):\n        print(f\"{prefix}Program\")\n        for s in node.statements:\n            print_ast(s, indent + 1)\n    elif isinstance(node, ast.TypeDef):\n        print(f\"{prefix}TypeDef {node.name}\")\n        for fname, ftype in node.fields:\n            dims = f\"[{' '.join(str(d) for d in ftype.dims)}]\" if ftype.dims else \"\"\n            print(f\"{prefix}  {fname}: {ftype.name}{dims}\")\n    elif isinstance(node, ast.FunDef):\n        name = f\"{node.type_name}'{node.method_name}\" if node.type_name else node.method_name\n        params = \", \".join(\n            f\"{p}: {t.name}{'[' + ' '.join(str(d) for d in t.dims) + ']' if t and t.dims else t.name if t else '?'}\"\n            for p, t in node.params\n        )\n        ret = \"\"\n        if node.return_type:\n            dims = f\"[{' '.join(str(d) for d in node.return_type.dims)}]\" if node.return_type.dims else \"\"\n            ret = f\" -> {node.return_type.name}{dims}\"\n        print(f\"{prefix}FunDef {name}({params}){ret}\")\n        for s in node.body:\n            print_ast(s, indent + 1)\n    elif isinstance(node, ast.Assignment):\n        print(f\"{prefix}Assign\")\n        print_ast(node.target, indent + 1)\n        print_ast(node.value, indent + 1)\n    elif isinstance(node, ast.CompoundAssign):\n        print(f\"{prefix}CompoundAssign(-=)\")\n        print_ast(node.target, indent + 1)\n        print_ast(node.value, indent + 1)\n    elif isinstance(node, ast.TupleDestructure):\n        print(f\"{prefix}TupleDestructure({', '.join(node.names)})\")\n    elif isinstance(node, ast.ForLoop):\n        print(f\"{prefix}For\")\n        print_ast(node.var, indent + 1)\n        print(f\"{prefix}  in:\")\n        print_ast(node.iterable, indent + 2)\n        print(f\"{prefix}  body:\")\n        for s in node.body:\n            print_ast(s, indent + 2)\n    elif isinstance(node, ast.IfExpr):\n        print(f\"{prefix}If\")\n        print_ast(node.condition, indent + 1)\n        for s in node.body:\n            print_ast(s, indent + 1)\n    elif isinstance(node, ast.ArenaBlock):\n        print(f\"{prefix}Arena {node.name}\")\n        for s in node.body:\n            print_ast(s, indent + 1)\n    elif isinstance(node, ast.BinOp):\n        print(f\"{prefix}BinOp({node.op})\")\n        print_ast(node.left, indent + 1)\n        print_ast(node.right, indent + 1)\n    elif isinstance(node, ast.UnaryOp):\n        print(f\"{prefix}UnaryOp({node.op})\")\n        print_ast(node.operand, indent + 1)\n    elif isinstance(node, ast.MemberAccess):\n        print(f\"{prefix}MemberAccess('{node.field_name})\")\n        print_ast(node.obj, indent + 1)\n    elif isinstance(node, ast.MethodCall):\n        kw = f\", kwargs={list(node.kwargs.keys())}\" if node.kwargs else \"\"\n        print(f\"{prefix}MethodCall('{node.method}, {len(node.args)} args{kw})\")\n        print_ast(node.obj, indent + 1)\n        for a in node.args:\n            print_ast(a, indent + 1)\n    elif isinstance(node, ast.Call):\n        kw = f\", kwargs={list(node.kwargs.keys())}\" if node.kwargs else \"\"\n        print(f\"{prefix}Call({len(node.args)} args{kw})\")\n        print_ast(node.func, indent + 1)\n        for a in node.args:\n            print_ast(a, indent + 1)\n    elif isinstance(node, ast.Index):\n        print(f\"{prefix}Index\")\n        print_ast(node.obj, indent + 1)\n        print_ast(node.index, indent + 1)\n    elif isinstance(node, ast.Range):\n        print(f\"{prefix}Range\")\n        print_ast(node.start, indent + 1)\n        print_ast(node.end, indent + 1)\n    elif isinstance(node, ast.Lambda):\n        print(f\"{prefix}Lambda({', '.join(node.params)})\")\n        print_ast(node.body, indent + 1)\n    elif isinstance(node, ast.AsExpr):\n        print(f\"{prefix}As({node.target_type})\")\n        print_ast(node.expr, indent + 1)\n    elif isinstance(node, ast.DerivExpr):\n        print(f\"{prefix}Deriv\")\n        print_ast(node.param, indent + 1)\n        print_ast(node.expr, indent + 1)\n    elif isinstance(node, ast.Num):\n        print(f\"{prefix}Num({node.value})\")\n    elif isinstance(node, ast.Str):\n        print(f\"{prefix}Str({node.value!r})\")\n    elif isinstance(node, ast.Ident):\n        print(f\"{prefix}Ident({node.name})\")\n    elif isinstance(node, ast.Pronoun):\n        print(f\"{prefix}Pronoun({node.name})\")\n    elif isinstance(node, ast.ExprStmt):\n        print(f\"{prefix}ExprStmt\")\n        print_ast(node.expr, indent + 1)\n    elif isinstance(node, ast.VecLiteral):\n        print(f\"{prefix}VecLiteral({len(node.elements)} elems)\")\n        for e in node.elements:\n            print_ast(e, indent + 1)\n    elif isinstance(node, ast.SetLiteral):\n        print(f\"{prefix}SetLiteral({len(node.elements)} elems)\")\n        for e in node.elements:\n            print_ast(e, indent + 1)\n    elif isinstance(node, ast.MentionAccess):\n        print(f\"{prefix}MentionAccess(''{node.field_name})\")\n        print_ast(node.obj, indent + 1)\n    elif isinstance(node, ast.NormExpr):\n        print(f\"{prefix}Norm\")\n        print_ast(node.expr, indent + 1)\n    else:\n        print(f\"{prefix}??? {type(node).__name__}\")\n\n\nif __name__ == '__main__':\n    if len(sys.argv) < 2:\n        print(\"Usage: python -m dx.parser <file.dx>\")\n        sys.exit(1)\n    with open(sys.argv[1]) as f:\n        source = f.read()\n    tree = parse(source)\n    print_ast(tree)\n",
    "typechecker.py": "\"\"\"Type checker for dx 0.01.\n\nVerifies tensor shapes, resolves member access through nested types,\nchecks matmul compatibility, and tracks me/it pronouns.\n\"\"\"\n\nfrom __future__ import annotations\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom . import ast_nodes as ast\n\n\n@dataclass\nclass DxType:\n    \"\"\"Type representation: name + optional dimension list.\"\"\"\n    name: str  # \"Float\", \"Int\", \"Vec\", \"Tensor\", \"Net\", etc.  \"?\" = unknown\n    dims: list[int] = field(default_factory=list)\n\n    def __repr__(self):\n        if self.dims:\n            return f\"{self.name}[{' '.join(str(d) for d in self.dims)}]\"\n        return self.name\n\n    def __eq__(self, other):\n        return isinstance(other, DxType) and self.name == other.name and self.dims == other.dims\n\n    @property\n    def is_unknown(self):\n        return self.name == \"?\"\n\n\nFLOAT = DxType(\"Float\")\nINT = DxType(\"Int\")\nSTRING = DxType(\"String\")\nVOID = DxType(\"Void\")\nUNKNOWN = DxType(\"?\")\n\n\ndef vec_type(n: int) -> DxType:\n    return DxType(\"Vec\", [n])\n\n\ndef tensor_type(m: int, n: int) -> DxType:\n    return DxType(\"Tensor\", [m, n])\n\n\nclass TypeCheckError(Exception):\n    pass\n\n\n# Built-in functions that preserve the shape of their first argument\nSHAPE_PRESERVING = {'relu', 'softmax', 'gelu', 'sigmoid', 'tanh'}\n\n\ndef fold_dims(op: str, a: int, b: int) -> int:\n    \"\"\"Constant-fold a dimension expression: a op b.\"\"\"\n    if op == '*':\n        return a * b\n    if op == '/' and b != 0:\n        return a // b\n    if op == '+':\n        return a + b\n    if op == '-':\n        return a - b\n    raise TypeCheckError(f\"Cannot fold dimension expression: {a} {op} {b}\")\n\n\nclass TypeChecker:\n    \"\"\"Type checker for dx 0.01.\"\"\"\n\n    def __init__(self):\n        self.types: dict[str, list[tuple[str, DxType]]] = {}  # type name -> fields\n        self.functions: dict[str, ast.FunDef] = {}  # \"Type'method\" -> FunDef\n        self.env: dict[str, DxType] = {}  # variable -> type\n        self.it_type: Optional[DxType] = None\n        self.current_method_type: Optional[str] = None  # for resolving `me`\n        self.errors: list[str] = []\n\n    def error(self, msg: str, node: ast.Node = None):\n        loc = f\" at line {node.line}\" if node and node.line else \"\"\n        self.errors.append(f\"{msg}{loc}\")\n\n    def check(self, program: ast.Program) -> bool:\n        \"\"\"Type-check a program. Returns True if no errors.\"\"\"\n        # Phase 1: Register all types and functions\n        for stmt in program.statements:\n            if isinstance(stmt, ast.TypeDef):\n                self._register_type(stmt)\n            elif isinstance(stmt, ast.FunDef):\n                self._register_function(stmt)\n\n        # Phase 2: Check all statements\n        for stmt in program.statements:\n            self.check_stmt(stmt)\n\n        return len(self.errors) == 0\n\n    def _register_type(self, node: ast.TypeDef):\n        fields = []\n        for fname, ftype in node.fields:\n            fields.append((fname, self._resolve_type_annotation(ftype)))\n        self.types[node.name] = fields\n\n    def _register_function(self, node: ast.FunDef):\n        key = f\"{node.type_name}'{node.method_name}\" if node.type_name else node.method_name\n        self.functions[key] = node\n\n    def _resolve_type_annotation(self, ann: ast.TypeAnnotation) -> DxType:\n        return DxType(ann.name, list(ann.dims))\n\n    # --- Statement checking ---\n\n    def check_stmt(self, node: ast.Node):\n        if isinstance(node, ast.TypeDef):\n            pass  # Already registered in phase 1\n\n        elif isinstance(node, ast.FunDef):\n            self._check_fundef(node)\n\n        elif isinstance(node, ast.Assignment):\n            self._check_assignment(node)\n\n        elif isinstance(node, ast.ExprStmt):\n            self.it_type = self.infer_type(node.expr)\n\n        elif isinstance(node, ast.ForLoop):\n            self._check_for(node)\n\n        elif isinstance(node, ast.ArenaBlock):\n            for s in node.body:\n                self.check_stmt(s)\n\n        elif isinstance(node, ast.IfExpr):\n            self.infer_type(node.condition)\n            for s in node.body:\n                self.check_stmt(s)\n            for s in node.else_body:\n                self.check_stmt(s)\n\n    def _check_fundef(self, node: ast.FunDef):\n        old_env = dict(self.env)\n        old_it = self.it_type\n        old_method_type = self.current_method_type\n\n        self.current_method_type = node.type_name or None\n        self.it_type = None\n\n        for pname, ptype in node.params:\n            if ptype:\n                self.env[pname] = self._resolve_type_annotation(ptype)\n            else:\n                self.env[pname] = UNKNOWN\n\n        for stmt in node.body:\n            self.check_stmt(stmt)\n\n        self.env = old_env\n        self.it_type = old_it\n        self.current_method_type = old_method_type\n\n    def _check_assignment(self, node: ast.Assignment):\n        val_type = self.infer_type(node.value)\n        if isinstance(node.target, ast.Ident):\n            self.env[node.target.name] = val_type\n        elif isinstance(node.target, ast.TupleDestructure):\n            for name in node.target.names:\n                self.env[name] = UNKNOWN\n\n    def _check_for(self, node: ast.ForLoop):\n        self.infer_type(node.iterable)\n        if isinstance(node.var, ast.Ident):\n            self.env[node.var.name] = INT if isinstance(node.iterable, ast.Range) else UNKNOWN\n        elif isinstance(node.var, ast.TupleDestructure):\n            for name in node.var.names:\n                self.env[name] = UNKNOWN\n        for s in node.body:\n            self.check_stmt(s)\n\n    # --- Type inference ---\n\n    def infer_type(self, expr: ast.Node) -> DxType:\n        \"\"\"Infer expression type. Returns UNKNOWN when undecidable.\"\"\"\n        if isinstance(expr, ast.Num):\n            return INT if expr.is_int else FLOAT\n\n        if isinstance(expr, ast.Str):\n            return STRING\n\n        if isinstance(expr, ast.Ident):\n            return self.env.get(expr.name, UNKNOWN)\n\n        if isinstance(expr, ast.Pronoun):\n            if expr.name == 'me' and self.current_method_type:\n                return DxType(self.current_method_type)\n            if expr.name == 'it' and self.it_type:\n                return self.it_type\n            return UNKNOWN\n\n        if isinstance(expr, ast.MemberAccess):\n            return self._infer_member_access(expr)\n\n        if isinstance(expr, ast.BinOp):\n            return self._infer_binop(expr)\n\n        if isinstance(expr, ast.UnaryOp):\n            return self.infer_type(expr.operand)\n\n        if isinstance(expr, ast.MethodCall):\n            return self._infer_method_call(expr)\n\n        if isinstance(expr, ast.Call):\n            return self._infer_call(expr)\n\n        if isinstance(expr, ast.AsExpr):\n            return DxType(expr.target_type)\n\n        if isinstance(expr, ast.DerivExpr):\n            return self.infer_type(expr.param)\n\n        if isinstance(expr, ast.NormExpr):\n            return FLOAT\n\n        if isinstance(expr, ast.VecLiteral):\n            return vec_type(len(expr.elements))\n\n        return UNKNOWN\n\n    def _infer_member_access(self, expr: ast.MemberAccess) -> DxType:\n        obj_type = self.infer_type(expr.obj)\n        if obj_type.is_unknown:\n            return UNKNOWN\n\n        # Transpose: Tensor[M N]'T -> Tensor[N M]\n        if expr.field_name == 'T':\n            if obj_type.name == 'Tensor' and len(obj_type.dims) == 2:\n                return tensor_type(obj_type.dims[1], obj_type.dims[0])\n            return obj_type  # Vec'T = Vec, etc.\n\n        # Look up field in type definition\n        for fname, ftype in self.types.get(obj_type.name, []):\n            if fname == expr.field_name:\n                return ftype\n\n        return UNKNOWN\n\n    def _infer_binop(self, expr: ast.BinOp) -> DxType:\n        left = self.infer_type(expr.left)\n        right = self.infer_type(expr.right)\n\n        if expr.op == 'o':\n            return self._check_matmul(left, right, expr)\n\n        if expr.op in ('+', '-'):\n            return self._check_add_sub(left, right, expr)\n\n        if expr.op in ('*', '/', '%', '^'):\n            if left.is_unknown or right.is_unknown:\n                return UNKNOWN\n            if left == INT and right == INT and expr.op not in ('/',):\n                return INT\n            if left.name in ('Float', 'Int') and right.name in ('Float', 'Int'):\n                return FLOAT\n            return UNKNOWN\n\n        if expr.op in ('=', '<>', '<', '>', '<=', '>=', 'and', 'or'):\n            return INT\n\n        if expr.op == '.':\n            return FLOAT  # dot product -> scalar\n\n        return UNKNOWN\n\n    def _check_matmul(self, left: DxType, right: DxType, node: ast.Node) -> DxType:\n        if left.is_unknown or right.is_unknown:\n            return UNKNOWN\n\n        # Tensor[M N] o Tensor[N P] -> Tensor[M P]\n        if left.name == 'Tensor' and right.name == 'Tensor':\n            if len(left.dims) == 2 and len(right.dims) == 2:\n                if left.dims[1] != right.dims[0]:\n                    self.error(\n                        f\"Matmul shape mismatch: {left} o {right} \"\n                        f\"(inner dims {left.dims[1]} != {right.dims[0]})\", node)\n                    return UNKNOWN\n                return tensor_type(left.dims[0], right.dims[1])\n\n        # Tensor[M N] o Vec[N] -> Vec[M]\n        if left.name == 'Tensor' and right.name == 'Vec':\n            if len(left.dims) == 2 and len(right.dims) == 1:\n                if left.dims[1] != right.dims[0]:\n                    self.error(\n                        f\"Matmul shape mismatch: {left} o {right} \"\n                        f\"(inner dims {left.dims[1]} != {right.dims[0]})\", node)\n                    return UNKNOWN\n                return vec_type(left.dims[0])\n\n        # Vec[M] o Tensor[M N] -> Vec[N]\n        if left.name == 'Vec' and right.name == 'Tensor':\n            if len(left.dims) == 1 and len(right.dims) == 2:\n                if left.dims[0] != right.dims[0]:\n                    self.error(\n                        f\"Matmul shape mismatch: {left} o {right} \"\n                        f\"(inner dims {left.dims[0]} != {right.dims[0]})\", node)\n                    return UNKNOWN\n                return vec_type(right.dims[1])\n\n        return UNKNOWN\n\n    def _check_add_sub(self, left: DxType, right: DxType, node: ast.Node) -> DxType:\n        if left.is_unknown or right.is_unknown:\n            if not left.is_unknown:\n                return left\n            if not right.is_unknown:\n                return right\n            return UNKNOWN\n\n        if left == right:\n            return left\n\n        # Tensor[M N] + Vec[N] -> Tensor[M N]  (broadcast)\n        if left.name == 'Tensor' and right.name == 'Vec':\n            if len(left.dims) == 2 and len(right.dims) == 1:\n                if left.dims[1] != right.dims[0]:\n                    self.error(f\"Broadcast shape mismatch: {left} + {right}\", node)\n                return left\n\n        if left.name == 'Vec' and right.name == 'Tensor':\n            if len(left.dims) == 1 and len(right.dims) == 2:\n                if left.dims[0] != right.dims[1]:\n                    self.error(f\"Broadcast shape mismatch: {left} + {right}\", node)\n                return right\n\n        # Scalar arithmetic\n        if left.name in ('Float', 'Int') and right.name in ('Float', 'Int'):\n            return FLOAT\n\n        return UNKNOWN\n\n    def _infer_method_call(self, expr: ast.MethodCall) -> DxType:\n        \"\"\"Infer type of obj'method(args).\"\"\"\n        obj_type = self.infer_type(expr.obj)\n\n        # Infer arg types for side effects (assignments to `it`, etc.)\n        for arg in expr.args:\n            self.infer_type(arg)\n\n        if obj_type.is_unknown:\n            return UNKNOWN\n\n        # Registered function: Type'method\n        key = f\"{obj_type.name}'{expr.method}\"\n        if key in self.functions:\n            fundef = self.functions[key]\n            if fundef.return_type:\n                return self._resolve_type_annotation(fundef.return_type)\n            return UNKNOWN\n\n        # Field whose type has an 'apply' method (nested type resolution)\n        for fname, ftype in self.types.get(obj_type.name, []):\n            if fname == expr.method:\n                apply_key = f\"{ftype.name}'apply\"\n                if apply_key in self.functions:\n                    fundef = self.functions[apply_key]\n                    if fundef.return_type:\n                        return self._resolve_type_annotation(fundef.return_type)\n                return UNKNOWN\n\n        return UNKNOWN\n\n    def _infer_call(self, expr: ast.Call) -> DxType:\n        \"\"\"Infer type of func(args).\"\"\"\n        # Infer arg types for side effects\n        for arg in expr.args:\n            self.infer_type(arg)\n\n        if isinstance(expr.func, ast.Ident):\n            name = expr.func.name\n\n            # Type constructor: GPT(...) -> GPT\n            if name in self.types:\n                return DxType(name)\n\n            # Variable of user type: model(x) -> Type'apply(x)\n            if name in self.env:\n                var_type = self.env[name]\n                if not var_type.is_unknown and var_type.name in self.types:\n                    apply_key = f\"{var_type.name}'apply\"\n                    if apply_key in self.functions:\n                        fundef = self.functions[apply_key]\n                        if fundef.return_type:\n                            return self._resolve_type_annotation(fundef.return_type)\n\n            # Shape-preserving built-ins: relu, softmax, gelu\n            if name in SHAPE_PRESERVING and expr.args:\n                return self.infer_type(expr.args[0])\n\n            if name == 'argmax':\n                return INT\n\n            if name == 'print':\n                return VOID\n\n        return UNKNOWN\n\n    # --- Public API (used by codegen) ---\n\n    def get_type_fields(self, type_name: str) -> list[tuple[str, DxType]]:\n        return self.types.get(type_name, [])\n\n    def get_method(self, type_name: str, method_name: str) -> Optional[ast.FunDef]:\n        key = f\"{type_name}'{method_name}\"\n        return self.functions.get(key)\n",
    "ir.py": "\"\"\"IR (Intermediate Representation) for the dx compiler.\n\nThe IR sits between the AST and C emission. Each node represents\na single operation with named inputs/outputs and explicit dimensions.\nThis enables semantic naming, constant extraction, and clean C generation.\n\nThree categories of nodes:\n  - Types: IRType, IRStructDef, IRGradStructDef\n  - Operations: IRMatmul, IRVecAdd, IRActivation, etc.\n  - Control: IRFunction, IRFor, IRIf, IRPrint, etc.\n\"\"\"\n\nfrom __future__ import annotations\nfrom dataclasses import dataclass, field\n\n\n# ---------------------------------------------------------------------------\n# Types\n# ---------------------------------------------------------------------------\n\n@dataclass\nclass IRType:\n    \"\"\"Type of an IR value.\"\"\"\n    kind: str          # 'float', 'int', 'vec', 'tensor', 'struct'\n    dims: list[int] = field(default_factory=list)  # [] for scalar, [N] for vec, [M,N] for tensor\n    struct_name: str | None = None  # for kind='struct'\n\n    @property\n    def size(self) -> int:\n        if self.kind in ('float', 'int'):\n            return 1\n        result = 1\n        for d in self.dims:\n            result *= d\n        return result\n\n    def __repr__(self):\n        if self.kind == 'struct':\n            return f'IRType(struct:{self.struct_name})'\n        if self.dims:\n            return f'IRType({self.kind}{self.dims})'\n        return f'IRType({self.kind})'\n\n\n@dataclass\nclass IRStructDef:\n    \"\"\"Definition of a struct type (from TypeDef dx).\"\"\"\n    name: str\n    fields: list[tuple[str, IRType]]  # (field_name, type)\n\n\n@dataclass\nclass IRGradStructDef:\n    \"\"\"Gradient struct for a type.\"\"\"\n    name: str          # e.g. \"NetGrad\"\n    base_struct: str   # e.g. \"Net\"\n    fields: list[tuple[str, IRType]]  # (d_field_name, type)\n\n\n# ---------------------------------------------------------------------------\n# Operations \u2014 the core of the IR\n# ---------------------------------------------------------------------------\n\n@dataclass\nclass IRNode:\n    \"\"\"Base node. Every node produces a named output.\"\"\"\n    out: str           # output name (semantic, not t1/t2)\n    out_type: IRType   # type of the output\n    source_line: int = 0  # original dx line for debug\n\n\n# --- Tensor operations ---\n\n@dataclass\nclass IRMatmul(IRNode):\n    \"\"\"out = A @ B (with transpose variants).\"\"\"\n    a: str = ''\n    b: str = ''\n    M: int = 0\n    K: int = 0\n    N: int = 0\n    trans_a: bool = False\n    trans_b: bool = False\n\n\n@dataclass\nclass IRMatvec(IRNode):\n    \"\"\"out[M] = W[M,N] @ x[N] (matrix-vector multiply).\"\"\"\n    w: str = ''\n    x: str = ''\n    M: int = 0\n    N: int = 0\n\n\n@dataclass\nclass IRVecAdd(IRNode):\n    \"\"\"out = a + b (element-wise).\"\"\"\n    a: str = ''\n    b: str = ''\n    n: int = 0\n\n\n@dataclass\nclass IRBroadcastAdd(IRNode):\n    \"\"\"out[M,N] = A[M,N] + b[N] (broadcast over rows).\"\"\"\n    a: str = ''\n    b: str = ''\n    M: int = 0\n    N: int = 0\n\n\n@dataclass\nclass IRActivation(IRNode):\n    \"\"\"out = activation(x). kind: sigmoid|relu|gelu|softmax.\"\"\"\n    x: str = ''\n    n: int = 0\n    kind: str = ''\n\n\n@dataclass\nclass IRLayerNorm(IRNode):\n    \"\"\"out = layer_norm(x, gamma, beta). Saves mean and rstd.\"\"\"\n    x: str = ''\n    gamma: str = ''\n    beta: str = ''\n    mean_buf: str = ''  # auxiliary buffer for mean\n    rstd_buf: str = ''  # auxiliary buffer for rstd\n    S: int = 0\n    D: int = 0\n\n\n@dataclass\nclass IRCausalMHA(IRNode):\n    \"\"\"out = causal_multi_head_attention(Q, K, V).\"\"\"\n    q: str = ''\n    k: str = ''\n    v: str = ''\n    attn_buf: str = ''  # buffer for attention weights\n    S: int = 0\n    D: int = 0\n    H: int = 0\n\n\n@dataclass\nclass IREmbed(IRNode):\n    \"\"\"out[S,D] = table[tokens[i], :].\"\"\"\n    table: str = ''\n    tokens: str = ''\n    S: int = 0\n    D: int = 0\n\n\n# --- Scalar operations ---\n\n@dataclass\nclass IRScalarOp(IRNode):\n    \"\"\"out = left op right. op: +|-|*|/.\"\"\"\n    left: str = ''\n    right: str = ''\n    op: str = ''\n\n\n@dataclass\nclass IRScalarConst(IRNode):\n    \"\"\"out = constant float or int.\"\"\"\n    value: float | int = 0\n\n\n# --- Memory operations ---\n\n@dataclass\nclass IRAlloc(IRNode):\n    \"\"\"Allocate buffer. heap=True for >10K floats.\"\"\"\n    n: int = 0\n    heap: bool = False\n\n\n@dataclass\nclass IRZero(IRNode):\n    \"\"\"Zero buffer: memset(target, 0, n * sizeof(float)).\"\"\"\n    target: str = ''\n    n: int = 0\n\n\n@dataclass\nclass IRCopy(IRNode):\n    \"\"\"Copy: memcpy(out, src, n * sizeof(float)).\"\"\"\n    src: str = ''\n    n: int = 0\n\n\n# --- Loss ---\n\n@dataclass\nclass IRLoss(IRNode):\n    \"\"\"out = loss_fn(pred, target). kind: mse|cross_entropy|cross_entropy_seq.\"\"\"\n    pred: str = ''\n    target: str = ''\n    kind: str = ''\n    dims: list[int] = field(default_factory=list)  # [N] for mse/ce, [S,V] for ce_seq\n\n\n# --- Sub-component ---\n\n@dataclass\nclass IRSubApply(IRNode):\n    \"\"\"out = sub_component.apply(input). Calls forward of a sub-type.\"\"\"\n    component: str = ''     # e.g. \"me->attn\"\n    component_type: str = ''  # e.g. \"Attention\"\n    field_name: str = ''    # e.g. \"attn\"\n    input: str = ''\n    input_n: int = 0\n\n\n# ---------------------------------------------------------------------------\n# Control flow\n# ---------------------------------------------------------------------------\n\n@dataclass\nclass IRFunction:\n    \"\"\"A function (forward or backward).\"\"\"\n    name: str\n    params: list[tuple[str, IRType]]\n    body: list  # list of IRNode, IRFor, IRIf, etc.\n    return_var: str = ''\n    return_type: IRType = None\n    is_method: bool = False\n    type_name: str = ''  # for methods: the struct type\n    method_name: str = ''  # for methods: 'apply', etc.\n\n\n@dataclass\nclass IRFor:\n    \"\"\"for var in start..end { body }.\"\"\"\n    var: str = ''\n    start: int | str = 0\n    end: int | str = 0\n    body: list = field(default_factory=list)  # mix of IRNode and IRFor/IRIf\n\n\n@dataclass\nclass IRIf:\n    \"\"\"if (cond) { then } else { else }.\"\"\"\n    cond: str = ''\n    then_body: list = field(default_factory=list)\n    else_body: list = field(default_factory=list)\n\n\n@dataclass\nclass IRPrint:\n    \"\"\"printf(format, args...).\"\"\"\n    format_str: str = ''\n    args: list[tuple[str, str]] = field(default_factory=list)  # (var_name, format_specifier)\n\n\n@dataclass\nclass IROptimizerStep:\n    \"\"\"Optimization step: SGD or Adam.\"\"\"\n    kind: str = ''  # 'sgd' | 'adam'\n    params: str = ''\n    grads: str = ''\n    n_params: int = 0\n    lr: float = 0.01\n    config: dict = field(default_factory=dict)  # beta1, beta2, epsilon for Adam\n\n\n@dataclass\nclass IRGradCompute:\n    \"\"\"Compute gradients: grads : d model'params(loss).\"\"\"\n    grad_var: str = ''\n    model_var: str = ''\n    model_type: str = ''\n    loss_var: str = ''\n    loss_kind: str = ''\n    loss_info: dict = field(default_factory=dict)\n    pred_var: str = ''\n    pred_info: dict = field(default_factory=dict)\n    n_params: int = 0\n\n\n@dataclass\nclass IRParamUpdate:\n    \"\"\"Update parameters: model'params -= lr * grads.\"\"\"\n    model_var: str = ''\n    model_type: str = ''\n    grad_var: str = ''\n    n_params: int = 0\n    lr: object = 0.001          # float constant or str variable name\n\n\n@dataclass\nclass IRBackwardStep:\n    \"\"\"Combined backward pass + optimizer step from opt'step(d net'params(loss)).\"\"\"\n    model_var: str = ''\n    model_type: str = ''\n    loss_var: str = ''\n    loss_kind: str = ''\n    loss_info: dict = field(default_factory=dict)\n    pred_var: str = ''\n    pred_info: dict = field(default_factory=dict)\n    grad_var: str = ''\n    n_params: int = 0\n    opt_kind: str = 'sgd'\n    opt_lr: float = 0.01\n    opt_config: dict = field(default_factory=dict)\n\n\n@dataclass\nclass IRModelInit:\n    \"\"\"Initialize a model: TypeName_init(&model, stddev).\"\"\"\n    var: str = ''\n    type_name: str = ''\n    stddev: float = 0.02\n    n_params: int = 0\n    heap: bool = False\n\n\n@dataclass\nclass IRModelCall:\n    \"\"\"out = model(input) \u2014 calls Type'apply forward.\"\"\"\n    out: str = ''\n    out_type: IRType = None\n    model_var: str = ''\n    model_type: str = ''\n    input_var: str = ''\n    cache_var: str = ''\n    out_n: int = 0\n\n\n@dataclass\nclass IROptDecl:\n    \"\"\"Optimizer declaration: opt : nn'sgd(net'params, lr: X).\"\"\"\n    kind: str = ''  # 'sgd' | 'adam'\n    lr: float = 0.01\n\n\n@dataclass\nclass IRVecLiteral:\n    \"\"\"Declare a vector literal: float name[] = {v1, v2, ...}.\"\"\"\n    var: str = ''\n    values: list[float] = field(default_factory=list)\n    var_type: IRType = None\n\n\n@dataclass\nclass IRStringArray:\n    \"\"\"Declare a string array: const char* name[] = {\"a\", \"b\", ...}.\"\"\"\n    var: str = ''\n    values: list[str] = field(default_factory=list)\n\n\n@dataclass\nclass IRSetDecl:\n    \"\"\"Set declaration: const float* name_data[] + const char* name_names[].\"\"\"\n    var: str = ''\n    elements: list[str] = field(default_factory=list)  # variable names\n    elem_dim: int = 0  # dimension of each Vec element\n\n\n@dataclass\nclass IRTextLoad:\n    \"\"\"Load text data: text = text_load(\"path\").\"\"\"\n    var: str = ''\n    path: str = ''\n\n\n@dataclass\nclass IRTextRandomBatch:\n    \"\"\"Get random batch from text: (x, y) = text.random_batch(seq_len, batch).\"\"\"\n    x_var: str = ''\n    y_var: str = ''\n    text_var: str = ''\n    seq_len: int = 128\n    batch_size: int = 1\n\n\n@dataclass\nclass IRTextEncode:\n    \"\"\"Encode string to tokens: var = text.encode(\"string\").\"\"\"\n    var: str = ''\n    len_var: str = ''\n    text_var: str = ''\n    prompt_str: str = ''\n    max_len: int = 256\n\n\n@dataclass\nclass IRGenerate:\n    \"\"\"Autoregressive generation: generated = nn'generate(model, prompt, ...).\"\"\"\n    out_var: str = ''\n    model_var: str = ''\n    model_type: str = ''\n    prompt_var: str = ''\n    max_len: int = 500\n    temperature: float = 0.8\n    seq_len: int = 128\n    vocab_size: int = 128\n    out_n: int = 0\n    cache_var: str = ''\n\n\n@dataclass\nclass IRTextDecode:\n    \"\"\"Decode and print: print(text'decode(tokens)).\"\"\"\n    text_var: str = ''\n    tokens_var: str = ''\n\n\n@dataclass\nclass IRDataLoad:\n    \"\"\"Load dataset: (x, y) = nn'data'mnist().\"\"\"\n    x_name: str = ''\n    y_name: str = ''\n    provider: str = ''  # 'mnist'\n    split: str = 'train'\n\n\n@dataclass\nclass IRCountOp:\n    \"\"\"Count matching items in dataset: n = dataset'zip(labels)'count(lambda).\"\"\"\n    out_var: str = ''\n    x_ds: str = ''\n    y_ds: str = ''\n    lambda_body: str = ''  # compiled C expression for the condition\n    model_call: dict | None = None  # info for model forward pass in lambda\n\n\n@dataclass\nclass IRArenaBlock:\n    \"\"\"Arena block \u2014 contains forward/backward that can be optimized together.\"\"\"\n    body: list = field(default_factory=list)\n\n\n@dataclass\nclass IRDerivExpr:\n    \"\"\"Forward-mode AD: deriv = d param(expr).\"\"\"\n    out_var: str = ''\n    fn_name: str = ''\n    arg_expr: str = ''\n\n\n@dataclass\nclass IRAssign:\n    \"\"\"Generic scalar assignment: var = expr.\"\"\"\n    var: str = ''\n    expr: str = ''  # C expression string\n    var_type: IRType = None\n    is_decl: bool = False  # first assignment (needs declaration)\n\n\n# ---------------------------------------------------------------------------\n# Module \u2014 the complete IR for a program\n# ---------------------------------------------------------------------------\n\n@dataclass\nclass IRModule:\n    \"\"\"Complete IR representation of a dx program.\"\"\"\n    struct_defs: list[IRStructDef] = field(default_factory=list)\n    grad_struct_defs: list[IRGradStructDef] = field(default_factory=list)\n    functions: list[IRFunction] = field(default_factory=list)       # forward functions\n    init_fns: list[str] = field(default_factory=list)               # type names needing init\n    scalar_fns: list[IRFunction] = field(default_factory=list)      # pure scalar functions\n    dual_fns: list[IRFunction] = field(default_factory=list)        # forward-mode AD versions\n    main_body: list = field(default_factory=list)                   # top-level statements\n    constants: dict[str, int | float] = field(default_factory=dict) # named constants\n    type_order: list[str] = field(default_factory=list)             # topological order of types\n",
    "ir_lower.py": "\"\"\"AST \u2192 IR lowering for the dx compiler.\n\nWalks the AST and produces an IRModule with typed, named IR nodes.\nMirrors the pattern matching in codegen_c.py but outputs structured IR\ninstead of C strings directly.\n\"\"\"\n\nfrom __future__ import annotations\nfrom . import ast_nodes as ast\nfrom .ir import *\nfrom .typechecker import TypeChecker, DxType, UNKNOWN, INT, FLOAT, STRING, VOID\n\n\ndef _dt(ann: ast.TypeAnnotation) -> DxType:\n    \"\"\"Convert TypeAnnotation to DxType.\"\"\"\n    return DxType(ann.name, list(ann.dims))\n\n\ndef _size(dt: DxType) -> int:\n    \"\"\"Number of floats in a DxType.\"\"\"\n    if dt.name == 'Vec' and dt.dims:\n        return dt.dims[0]\n    if dt.name == 'Tensor' and len(dt.dims) == 2:\n        return dt.dims[0] * dt.dims[1]\n    return 1\n\n\ndef _is_vec_or_tensor(dt: DxType) -> bool:\n    return dt.name in ('Vec', 'Tensor') and len(dt.dims) > 0\n\n\ndef _to_ir_type(dt: DxType, type_defs: dict) -> IRType:\n    \"\"\"Convert DxType to IRType.\"\"\"\n    if dt.name == 'Vec' and dt.dims:\n        return IRType('vec', list(dt.dims))\n    if dt.name == 'Tensor' and len(dt.dims) == 2:\n        return IRType('tensor', list(dt.dims))\n    if dt.name == 'Float':\n        return IRType('float')\n    if dt.name == 'Int':\n        return IRType('int')\n    if dt.name in type_defs:\n        return IRType('struct', struct_name=dt.name)\n    return IRType('float')\n\n\ndef _param_field(cvar: str) -> str | None:\n    \"\"\"If cvar is 'me->field', return 'field'. Else None.\"\"\"\n    if cvar.startswith('me->'):\n        return cvar[4:]\n    return None\n\n\nclass IRLower:\n    \"\"\"Lowers a dx AST to IR.\"\"\"\n\n    def __init__(self, program: ast.Program):\n        self.program = program\n        self.type_defs: dict[str, list[tuple[str, ast.TypeAnnotation]]] = {}\n        self.fun_defs: dict[str, ast.FunDef] = {}\n\n        for s in program.statements:\n            if isinstance(s, ast.TypeDef):\n                self.type_defs[s.name] = s.fields\n            elif isinstance(s, ast.FunDef):\n                key = f\"{s.type_name}'{s.method_name}\" if s.type_name else s.method_name\n                self.fun_defs[key] = s\n\n        self.tn = 0  # temp counter for fresh names\n        self.env: dict[str, DxType] = {}\n        self.it_var: tuple[str, DxType] | None = None\n        self.me_type: str | None = None\n        self.tape: list[dict] = []\n        self.recording = False\n        self.tape_aliases: dict[str, str] = {}\n\n        # Main-level state\n        self.main_declared: set[str] = set()\n        self.chain: dict[str, dict] = {}\n        self.model_vars: dict[str, str] = {}\n        self.free_fns: set[str] = set()\n        self.text_vars: dict[str, bool] = {}\n        self.datasets: dict[str, dict] = {}\n        self.inline_data: dict[str, dict] = {}  # name \u2192 {rows, cols, var}\n        self.sets: dict[str, dict] = {}  # set_name \u2192 {elements, dim}\n        self.set_element_vars: dict[str, str] = {}  # loop var \u2192 set name (during for choose)\n        self.heap_models: dict[str, bool] = {}\n        self.opt_method: str = 'sgd'\n        self.opt_lr: float = 1.0\n        self.grad_vars: dict[str, dict] = {}  # grad_var \u2192 {model_var, model_type, n_params}\n        self.scalar_vals: dict[str, float] = {}  # name \u2192 numeric value (for lr lookup)\n\n        self.loss_registry = {\n            'mse': {'forward': 'MseForward', 'backward': 'MseBackward', 'args': 'pred_target'},\n            'sse': {'forward': 'SseForward', 'backward': 'SseBackward', 'args': 'pred_target'},\n            'cross_entropy': {'forward': 'CrossEntropyForward', 'backward': 'CrossEntropyBackward', 'args': 'pred_label'},\n            'cross_entropy_seq': {'forward': 'CrossEntropySeqForward', 'backward': 'CrossEntropySeqBackward', 'args': 'logits_targets'},\n        }\n        self.data_providers = {\n            'mnist': {\n                'train': {'images': 'data/train-images-idx3-ubyte', 'labels': 'data/train-labels-idx1-ubyte'},\n                'test': {'images': 'data/t10k-images-idx3-ubyte', 'labels': 'data/t10k-labels-idx1-ubyte'},\n                'loader_images': 'MnistLoadImages', 'loader_labels': 'MnistLoadLabels',\n            },\n        }\n\n    # ------- helpers -------\n\n    def _fresh(self) -> str:\n        self.tn += 1\n        return f't{self.tn}'\n\n    def _semantic_fresh(self, hint: str) -> str:\n        \"\"\"Generate a semantic name, avoiding duplicates.\"\"\"\n        if hint not in self._used_names:\n            self._used_names.add(hint)\n            return hint\n        for i in range(2, 100):\n            candidate = f'{hint}{i}'\n            if candidate not in self._used_names:\n                self._used_names.add(candidate)\n                return candidate\n        return self._fresh()\n\n    def _total_params(self, type_name: str) -> int:\n        total = 0\n        for _, ftype in self.type_defs.get(type_name, []):\n            dt = _dt(ftype)\n            if dt.name in self.type_defs:\n                total += self._total_params(dt.name)\n            else:\n                total += _size(dt)\n        return total\n\n    def _field_type(self, type_name: str, field_name: str) -> DxType:\n        for fn, ft in self.type_defs.get(type_name, []):\n            if fn == field_name:\n                dt = _dt(ft)\n                if dt.name in self.type_defs:\n                    return DxType(dt.name)\n                return dt\n        return UNKNOWN\n\n    def _topo_sort_types(self) -> list[str]:\n        visited = set()\n        order = []\n        def visit(name):\n            if name in visited:\n                return\n            visited.add(name)\n            for _, ft in self.type_defs.get(name, []):\n                dt = _dt(ft)\n                if dt.name in self.type_defs:\n                    visit(dt.name)\n            order.append(name)\n        for name in self.type_defs:\n            visit(name)\n        return order\n\n    def _infer_input_dim(self) -> int:\n        for key, fundef in self.fun_defs.items():\n            if fundef.type_name and fundef.method_name == 'apply':\n                if fundef.params:\n                    ptype = fundef.params[0][1]\n                    if ptype and ptype.name == 'Vec':\n                        return ptype.dims[0]\n        return 0\n\n    def _infer_output_dim(self) -> int:\n        for key, fundef in self.fun_defs.items():\n            if fundef.type_name and fundef.method_name == 'apply':\n                if fundef.return_type and fundef.return_type.name == 'Vec':\n                    return fundef.return_type.dims[0]\n        return 0\n\n    # ------- struct lowering -------\n\n    def _lower_struct_def(self, name: str, fields: list) -> IRStructDef:\n        ir_fields = []\n        for fn, ft in fields:\n            dt = _dt(ft)\n            ir_fields.append((fn, _to_ir_type(dt, self.type_defs)))\n        return IRStructDef(name=name, fields=ir_fields)\n\n    def _lower_grad_struct_def(self, name: str, fields: list) -> IRGradStructDef:\n        ir_fields = []\n        for fn, ft in fields:\n            dt = _dt(ft)\n            if dt.name in self.type_defs:\n                ir_fields.append((f'd_{fn}', IRType('struct', struct_name=f'{dt.name}Grad')))\n            else:\n                ir_fields.append((f'd_{fn}', _to_ir_type(dt, self.type_defs)))\n        return IRGradStructDef(name=f'{name}Grad', base_struct=name, fields=ir_fields)\n\n    # ------- tape building (FunDef body \u2192 tape entries) -------\n\n    def _build_tape(self, fundef: ast.FunDef) -> list[dict]:\n        self.tn = 0\n        self.tape = []\n        self.recording = True\n        self.it_var = None\n        self.me_type = fundef.type_name\n        self.tape_aliases = {}\n        self._used_names = set()\n\n        self.env = {}\n        for pname, ptype in fundef.params:\n            if ptype:\n                self.env[pname] = _dt(ptype)\n\n        for stmt in fundef.body:\n            if isinstance(stmt, ast.ExprStmt):\n                var, dt = self._texpr(stmt.expr)\n                self.it_var = (var, dt)\n            elif isinstance(stmt, ast.Assignment) and isinstance(stmt.target, ast.Ident):\n                var, dt = self._texpr(stmt.value)\n                self.env[stmt.target.name] = dt\n                self.tape_aliases[stmt.target.name] = var\n\n        self.recording = False\n        return list(self.tape)\n\n    def _texpr(self, node) -> tuple[str, DxType]:\n        if isinstance(node, ast.Num):\n            return (f'{node.value}f' if not node.is_int else str(int(node.value)), FLOAT if not node.is_int else INT)\n\n        if isinstance(node, ast.Ident):\n            in_env = node.name in self.env or node.name in self.tape_aliases\n            is_field = False\n            if self.me_type:\n                for fn, _ in self.type_defs.get(self.me_type, []):\n                    if fn == node.name:\n                        is_field = True\n                        break\n            if is_field and in_env:\n                raise ValueError(\n                    f\"Ambiguous name '{node.name}': both local variable and \"\n                    f\"field of {self.me_type}. Use me'{node.name} to refer to the field.\")\n            if is_field and not in_env:\n                fdt = self._field_type(self.me_type, node.name)\n                return (f'me->{node.name}', fdt)\n            dt = self.env.get(node.name, UNKNOWN)\n            var = self.tape_aliases.get(node.name, node.name)\n            return (var, dt)\n\n        if isinstance(node, ast.Pronoun):\n            if node.name == 'it' and self.it_var:\n                return self.it_var\n            if node.name == 'me':\n                return ('me', DxType(self.me_type) if self.me_type else UNKNOWN)\n            return ('??', UNKNOWN)\n\n        if isinstance(node, ast.MemberAccess):\n            obj_var, obj_dt = self._texpr(node.obj)\n            if obj_var == 'me' and self.me_type:\n                fdt = self._field_type(self.me_type, node.field_name)\n                return (f'me->{node.field_name}', fdt)\n            return (f'{obj_var}.{node.field_name}', UNKNOWN)\n\n        if isinstance(node, ast.UnaryOp) and node.op == '-':\n            xv, xt = self._texpr(node.operand)\n            if _is_vec_or_tensor(xt):\n                n = _size(xt)\n                out = self._semantic_fresh('neg')\n                self.tape.append({'op': 'negate', 'out': out, 'n': n, 'x': xv, 'N': n})\n                return (out, xt)\n            return (f'(-{xv})', xt)\n\n        if isinstance(node, ast.BinOp):\n            if node.op in ('o', '.', '~'):\n                rhs_transposed = (isinstance(node.right, ast.MemberAccess) and node.right.field_name == 'T')\n                if rhs_transposed:\n                    rv, rt = self._texpr(node.right.obj)\n                else:\n                    rv, rt = self._texpr(node.right)\n                lv, lt = self._texpr(node.left)\n\n                if lt.name == 'Tensor' and len(lt.dims) == 2 and rt.name == 'Tensor' and len(rt.dims) == 2:\n                    M, K_l = lt.dims\n                    if rhs_transposed:\n                        P, K_r = rt.dims[0], rt.dims[1]\n                    else:\n                        K_r, P = rt.dims[0], rt.dims[1]\n                    # Semantic naming based on weight field\n                    pf = _param_field(rv)\n                    if pf and pf.startswith('w') and len(pf) > 1 and pf[1:].isalpha():\n                        hint = f'{pf[1:]}Proj'\n                    elif pf:\n                        hint = f'{pf}Proj'\n                    else:\n                        hint = 'mmOut'\n                    out = self._semantic_fresh(hint)\n                    out_dt = DxType('Tensor', [M, P])\n                    op = 'matmul_2d_transB' if rhs_transposed else 'matmul_2d'\n                    self.tape.append({'op': op, 'out': out, 'n': M * P,\n                                      'A': lv, 'B': rv, 'M': M, 'K': K_l, 'P': P})\n                    return (out, out_dt)\n\n                if lt.name == 'Tensor' and len(lt.dims) == 2:\n                    M, N = lt.dims\n                    pf = _param_field(lv)\n                    hint = f'{pf}Proj' if pf else 'WProj'\n                    out = self._semantic_fresh(hint)\n                    out_dt = DxType('Vec', [M])\n                    self.tape.append({'op': 'matmul', 'out': out, 'n': M,\n                                      'W': lv, 'x': rv, 'M': M, 'N': N})\n                    return (out, out_dt)\n\n                if node.op == '~' and lt.name == 'Vec' and rt.name == 'Vec':\n                    n = _size(lt)\n                    out = self._semantic_fresh('cosSim')\n                    self.tape.append({'op': 'cosine_similarity', 'out': out, 'n': 1,\n                                      'a': lv, 'b': rv, 'N': n})\n                    return (out, FLOAT)\n\n                if lt.name == 'Vec' and rt.name == 'Vec':\n                    n = _size(lt)\n                    out = self._semantic_fresh('dotOut')\n                    self.tape.append({'op': 'dot_product', 'out': out, 'n': 1,\n                                      'a': lv, 'b': rv, 'N': n})\n                    return (out, FLOAT)\n\n            lv, lt = self._texpr(node.left)\n            rv, rt = self._texpr(node.right)\n\n            if node.op == '+':\n                if lt.name == 'Tensor' and len(lt.dims) == 2 and rt.name == 'Vec':\n                    M, N = lt.dims\n                    out = self._semantic_fresh('plusBias')\n                    self.tape.append({'op': 'broadcast_add', 'out': out, 'n': M * N,\n                                      'A': lv, 'b': rv, 'M': M, 'N': N})\n                    return (out, lt)\n                if _is_vec_or_tensor(lt) and _is_vec_or_tensor(rt):\n                    n = _size(lt)\n                    # heuristic: if one input is 'x' (function param), it's a residual\n                    hint = 'residual' if (lv == 'x' or rv == 'x') else 'sum'\n                    out = self._semantic_fresh(hint)\n                    self.tape.append({'op': 'vec_add', 'out': out, 'n': n,\n                                      'a': lv, 'b': rv, 'N': n})\n                    return (out, lt)\n                return (f'({lv} + {rv})', lt if lt != UNKNOWN else rt)\n\n            if node.op == '-':\n                if _is_vec_or_tensor(lt) and _is_vec_or_tensor(rt):\n                    n = _size(lt)\n                    out = self._semantic_fresh('diff')\n                    self.tape.append({'op': 'vec_sub', 'out': out, 'n': n,\n                                      'a': lv, 'b': rv, 'N': n})\n                    return (out, lt)\n                return (f'({lv} - {rv})', lt if lt != UNKNOWN else rt)\n            if node.op == '*':\n                return (f'({lv} * {rv})', FLOAT)\n            if node.op == '/':\n                return (f'({lv} / {rv})', FLOAT)\n\n        if isinstance(node, ast.Call):\n            fname = node.func.name if isinstance(node.func, ast.Ident) else ''\n            if fname in ('sigmoid', 'relu', 'softmax', 'gelu', 'log'):\n                av, at = self._texpr(node.args[0])\n                n = _size(at)\n                out = self._semantic_fresh(f'{fname}Out')\n                self.tape.append({'op': fname, 'out': out, 'n': n, 'x': av, 'N': n})\n                return (out, at)\n\n        if isinstance(node, ast.Index):\n            xv, xt = self._texpr(node.obj)\n            iv, it = self._texpr(node.index)\n            N = _size(xt)\n            out = self._semantic_fresh('indexed')\n            self.tape.append({'op': 'index', 'out': out, 'n': 1, 'x': xv, 'idx': iv, 'N': N})\n            return (out, FLOAT)\n\n        if isinstance(node, ast.MethodCall):\n            if isinstance(node.obj, ast.Ident) and node.obj.name == 'nn':\n                return self._texpr_nn_builtin(node)\n            if isinstance(node.obj, ast.MemberAccess):\n                obj_var, obj_dt = self._texpr(node.obj.obj)\n                if obj_var == 'me' and self.me_type:\n                    field_name = node.obj.field_name\n                    field_dt = self._field_type(self.me_type, field_name)\n                    if field_dt.name in self.type_defs:\n                        return self._texpr_sub_component_call(field_name, field_dt.name, node.args)\n            if isinstance(node.obj, ast.Pronoun) and node.obj.name == 'me':\n                field_name = node.method\n                if self.me_type:\n                    field_dt = self._field_type(self.me_type, field_name)\n                    if field_dt.name in self.type_defs:\n                        return self._texpr_sub_component_call(field_name, field_dt.name, node.args)\n\n        return ('??', UNKNOWN)\n\n    def _texpr_nn_builtin(self, node: ast.MethodCall) -> tuple[str, DxType]:\n        method = node.method\n\n        if method == 'embed':\n            table_var, table_dt = self._texpr(node.args[0])\n            tok_var, tok_dt = self._texpr(node.args[1])\n            if table_dt.name == 'Tensor' and len(table_dt.dims) == 2:\n                V, D = table_dt.dims\n                S = tok_dt.dims[0] if tok_dt.dims else 128\n            else:\n                V, D, S = 128, 128, 128\n            out = self._semantic_fresh('embedded')\n            out_dt = DxType('Tensor', [S, D])\n            self.tape.append({'op': 'embed', 'out': out, 'n': S * D,\n                              'table': table_var, 'tokens': tok_var, 'S': S, 'D': D})\n            return (out, out_dt)\n\n        if method == 'layer_norm':\n            x_var, x_dt = self._texpr(node.args[0])\n            gamma_var, gamma_dt = self._texpr(node.args[1])\n            beta_var, beta_dt = self._texpr(node.args[2])\n            if x_dt.name == 'Tensor' and len(x_dt.dims) == 2:\n                S, D = x_dt.dims\n            else:\n                S, D = 128, 128\n            out = self._semantic_fresh('lnOut')\n            out_dt = DxType('Tensor', [S, D])\n            self.tape.append({'op': 'layer_norm', 'out': out, 'n': S * D,\n                              'x': x_var, 'gamma': gamma_var, 'beta': beta_var,\n                              'S': S, 'D': D})\n            return (out, out_dt)\n\n        if method == 'causal_mha':\n            Q_var, Q_dt = self._texpr(node.args[0])\n            K_var, K_dt = self._texpr(node.args[1])\n            V_var, V_dt = self._texpr(node.args[2])\n            H = 4\n            if 'heads' in node.kwargs and isinstance(node.kwargs['heads'], ast.Num):\n                H = int(node.kwargs['heads'].value)\n            if Q_dt.name == 'Tensor' and len(Q_dt.dims) == 2:\n                S, D = Q_dt.dims\n            else:\n                S, D = 128, 128\n            out = self._semantic_fresh('mhaOut')\n            out_dt = DxType('Tensor', [S, D])\n            self.tape.append({'op': 'causal_mha', 'out': out, 'n': S * D,\n                              'Q': Q_var, 'K': K_var, 'V': V_var,\n                              'S': S, 'D': D, 'H': H})\n            return (out, out_dt)\n\n        return ('??', UNKNOWN)\n\n    def _texpr_sub_component_call(self, field_name: str, sub_type: str, args: list) -> tuple[str, DxType]:\n        apply_key = f\"{sub_type}'apply\"\n        fundef = self.fun_defs.get(apply_key)\n        if fundef and fundef.return_type:\n            ret_dt = _dt(fundef.return_type)\n            arg_var, arg_dt = self._texpr(args[0]) if args else ('??', UNKNOWN)\n            out = self._semantic_fresh(f'{field_name}Out')\n            self.tape.append({\n                'op': 'sub_apply', 'out': out, 'n': _size(ret_dt),\n                'sub_field': field_name, 'sub_type': sub_type,\n                'input': arg_var, 'input_n': _size(arg_dt),\n            })\n            return (out, ret_dt)\n        return ('??', UNKNOWN)\n\n    # ------- function lowering -------\n\n    def _lower_method(self, fundef: ast.FunDef) -> tuple[IRFunction, list[dict]]:\n        \"\"\"Lower a method FunDef to IRFunction + tape (for emission).\"\"\"\n        tape = self._build_tape(fundef)\n        tname = fundef.type_name\n        mname = fundef.method_name\n\n        params = []\n        for pname, ptype in fundef.params:\n            dt = _dt(ptype) if ptype else UNKNOWN\n            params.append((pname, _to_ir_type(dt, self.type_defs)))\n\n        ret_dt = _dt(fundef.return_type) if fundef.return_type else UNKNOWN\n        ret_var = tape[-1]['out'] if tape else ''\n\n        fn = IRFunction(\n            name=f'{tname}_{mname}',\n            params=params,\n            body=tape,  # keep tape dicts \u2014 emit_c understands them\n            return_var=ret_var,\n            return_type=_to_ir_type(ret_dt, self.type_defs),\n            is_method=True,\n            type_name=tname,\n            method_name=mname,\n        )\n        return fn, tape\n\n    def _lower_scalar_fn(self, fundef: ast.FunDef) -> IRFunction:\n        \"\"\"Lower a free (scalar) function to IRFunction.\"\"\"\n        params = []\n        for pname, ptype in fundef.params:\n            params.append((pname, IRType('float')))\n\n        fn = IRFunction(\n            name=fundef.method_name,\n            params=params,\n            body=fundef.body,  # keep AST body \u2014 emit_c compiles it\n            return_var='',\n            return_type=IRType('float'),\n            is_method=False,\n        )\n        return fn\n\n    # ------- main statement lowering -------\n\n    def _lower_main_stmt(self, stmt, body: list):\n        \"\"\"Lower a top-level statement to IR nodes, appending to body.\"\"\"\n        if isinstance(stmt, ast.TypeDef) or isinstance(stmt, ast.FunDef):\n            return  # handled separately\n\n        if isinstance(stmt, ast.Assignment):\n            self._lower_main_assign(stmt, body)\n        elif isinstance(stmt, ast.CompoundAssign):\n            self._lower_compound_assign(stmt, body)\n        elif isinstance(stmt, ast.ExprStmt):\n            self._lower_main_exprstmt(stmt, body)\n        elif isinstance(stmt, ast.ForLoop):\n            self._lower_main_for(stmt, body)\n        elif isinstance(stmt, ast.IfExpr):\n            self._lower_main_if(stmt, body)\n        elif isinstance(stmt, ast.ArenaBlock):\n            arena = IRArenaBlock(body=[])\n            for s in stmt.body:\n                self._lower_main_stmt(s, arena.body)\n            body.append(arena)\n        elif isinstance(stmt, ast.PrintStmt):\n            body.append(self._lower_print(stmt))\n\n    def _lower_main_assign(self, stmt: ast.Assignment, body: list):\n        if isinstance(stmt.target, ast.TupleDestructure):\n            self._lower_main_tuple_assign(stmt, body)\n            return\n\n        # model'params : model'params - lr * grads (explicit param update)\n        if isinstance(stmt.target, ast.MemberAccess) and isinstance(stmt.target.obj, ast.Ident):\n            model_var = stmt.target.obj.name\n            if model_var in self.model_vars and stmt.target.field_name == 'params':\n                update = self._try_lower_param_update(model_var, stmt.value, body)\n                if update:\n                    return\n\n        if not isinstance(stmt.target, ast.Ident):\n            return\n\n        name = stmt.target.name\n        val = stmt.value\n\n        # SetLiteral: {mario, luigi, ...}\n        if isinstance(val, ast.SetLiteral):\n            elem_names = [e.name for e in val.elements if isinstance(e, ast.Ident)]\n            # Infer dim from first element type\n            first_dt = self.env.get(elem_names[0], UNKNOWN)\n            dim = first_dt.dims[0] if first_dt.dims else 1\n            body.append(IRSetDecl(var=name, elements=elem_names, elem_dim=dim))\n            self.main_declared.add(name)\n            self.env[name] = DxType('Set', [len(elem_names), dim])\n            self.sets[name] = {'elements': elem_names, 'dim': dim}\n            return\n\n        # VecLiteral (flat or nested)\n        if isinstance(val, ast.VecLiteral):\n            # String array: [\"mario\" \"luigi\" ...] \u2192 const char* name[]\n            if val.elements and isinstance(val.elements[0], ast.Str):\n                strings = [e.value for e in val.elements if isinstance(e, ast.Str)]\n                body.append(IRStringArray(var=name, values=strings))\n                self.main_declared.add(name)\n                self.env[name] = DxType('StringArray', [len(strings)])\n                return\n            # Nested: [[0 0] [0 1] ...] \u2192 Tensor[rows cols]\n            if val.elements and isinstance(val.elements[0], ast.VecLiteral):\n                rows = len(val.elements)\n                cols = len(val.elements[0].elements)\n                values = []\n                for inner in val.elements:\n                    for e in inner.elements:\n                        values.append(float(e.value))\n                vtype = IRType('tensor', [rows, cols])\n                body.append(IRVecLiteral(var=name, values=values, var_type=vtype))\n                self.main_declared.add(name)\n                self.env[name] = DxType('Tensor', [rows, cols])\n                self.inline_data[name] = {'rows': rows, 'cols': cols, 'var': name}\n                return\n            # Flat: [0 0] \u2192 Vec[n]\n            values = [e.value for e in val.elements if isinstance(e, ast.Num)]\n            n = len(values)\n            vtype = IRType('vec', [n])\n            body.append(IRVecLiteral(var=name, values=values, var_type=vtype))\n            self.main_declared.add(name)\n            self.env[name] = DxType('Vec', [n])\n            return\n\n        # Type constructor: net : Net(nn'init_normal(1.0))\n        if isinstance(val, ast.Call) and isinstance(val.func, ast.Ident):\n            tname = val.func.name\n            if tname in self.type_defs:\n                n_params = self._total_params(tname)\n                use_heap = n_params > 10000\n                stddev = 0.02\n                if val.args and isinstance(val.args[0], ast.MethodCall):\n                    mc = val.args[0]\n                    if mc.method == 'init_normal' and mc.args:\n                        stddev = mc.args[0].value\n                body.append(IRModelInit(var=name, type_name=tname, stddev=stddev,\n                                        n_params=n_params, heap=use_heap))\n                self.main_declared.add(name)\n                self.model_vars[name] = tname\n                self.env[name] = DxType(tname)\n                if use_heap:\n                    self.heap_models[name] = True\n                return\n\n        # Optimizer: opt : nn'sgd(net'params, lr: X)\n        if isinstance(val, ast.MethodCall) and isinstance(val.obj, ast.Ident):\n            if val.obj.name == 'nn' and val.method in ('sgd', 'adam'):\n                self.opt_method = val.method\n                if 'lr' in val.kwargs and isinstance(val.kwargs['lr'], ast.Num):\n                    self.opt_lr = val.kwargs['lr'].value\n                body.append(IROptDecl(kind=val.method, lr=self.opt_lr))\n                self.main_declared.add(name)\n                return\n\n        # Text data: text : nn'data'text(\"path\")\n        if isinstance(val, ast.MethodCall) and isinstance(val.obj, ast.MemberAccess):\n            if isinstance(val.obj.obj, ast.Ident) and val.obj.obj.name == 'nn' and val.obj.field_name == 'data' and val.method == 'text':\n                path = val.args[0].value if val.args and isinstance(val.args[0], ast.Str) else 'input.txt'\n                body.append(IRTextLoad(var=name, path=path))\n                self.main_declared.add(name)\n                self.env[name] = DxType('TextData')\n                self.text_vars[name] = True\n                return\n\n        # Text encode: prompt : text'encode(\"string\")\n        if isinstance(val, ast.MethodCall) and isinstance(val.obj, ast.Ident):\n            if val.obj.name in self.text_vars and val.method == 'encode':\n                text_var = val.obj.name\n                if val.args and isinstance(val.args[0], ast.Str):\n                    prompt_str = val.args[0].value\n                    max_len = len(prompt_str) + 128\n                    body.append(IRTextEncode(var=name, len_var=f'{name}_len',\n                                             text_var=text_var, prompt_str=prompt_str,\n                                             max_len=max_len))\n                    self.main_declared.add(name)\n                    self.env[name] = DxType('Vec', [max_len])\n                    return\n\n        # Generate: generated : nn'generate(model, prompt, ...)\n        if isinstance(val, ast.MethodCall) and isinstance(val.obj, ast.Ident):\n            if val.obj.name == 'nn' and val.method == 'generate':\n                model_name = val.args[0].name if val.args and isinstance(val.args[0], ast.Ident) else 'model'\n                prompt_name = val.args[1].name if len(val.args) > 1 and isinstance(val.args[1], ast.Ident) else 'prompt'\n                max_len = 500\n                temperature = 0.8\n                if 'max_len' in val.kwargs and isinstance(val.kwargs['max_len'], ast.Num):\n                    max_len = int(val.kwargs['max_len'].value)\n                if 'temperature' in val.kwargs and isinstance(val.kwargs['temperature'], ast.Num):\n                    temperature = val.kwargs['temperature'].value\n                if model_name in self.model_vars:\n                    tname = self.model_vars[model_name]\n                    apply_key = f\"{tname}'apply\"\n                    fundef = self.fun_defs.get(apply_key)\n                    seq_len = 128\n                    if fundef and fundef.params:\n                        pdt = _dt(fundef.params[0][1]) if fundef.params[0][1] else UNKNOWN\n                        if pdt.dims:\n                            seq_len = pdt.dims[0]\n                    ret_dt = _dt(fundef.return_type) if fundef and fundef.return_type else DxType('Tensor', [seq_len, 128])\n                    out_n = _size(ret_dt)\n                    vocab = ret_dt.dims[1] if len(ret_dt.dims) == 2 else 128\n                    cache_var = f'cache_{model_name}'\n                    body.append(IRGenerate(\n                        out_var=name, model_var=model_name, model_type=tname,\n                        prompt_var=prompt_name, max_len=max_len, temperature=temperature,\n                        seq_len=seq_len, vocab_size=vocab, out_n=out_n, cache_var=cache_var))\n                    self.main_declared.add(name)\n                    self.env[name] = DxType('Vec', [max_len])\n                    return\n\n        # Pattern: loss : -log(pred[y]) \u2192 cross_entropy(pred, y)\n        if (isinstance(val, ast.UnaryOp) and val.op == '-'\n                and isinstance(val.operand, ast.Call)\n                and isinstance(val.operand.func, ast.Ident)\n                and val.operand.func.name == 'log'\n                and len(val.operand.args) == 1\n                and isinstance(val.operand.args[0], ast.Index)):\n            idx_node = val.operand.args[0]\n            pred_var = self._main_scalar_expr(idx_node.obj)\n            label_var = self._main_scalar_expr(idx_node.index)\n            pred_dt = self.env.get(pred_var, UNKNOWN)\n            n = _size(pred_dt) if _is_vec_or_tensor(pred_dt) else self._infer_output_dim()\n            loss_reg = self.loss_registry['cross_entropy']\n            ir_type = IRType('float')\n            body.append(IRLoss(out=name, out_type=ir_type, pred=pred_var,\n                               target=label_var, kind='cross_entropy', dims=[n]))\n            if name not in self.main_declared:\n                self.main_declared.add(name)\n                self.env[name] = FLOAT\n            chain_info = {'kind': 'cross_entropy', 'pred': pred_var, 'n': n, 'label': label_var}\n            self.chain[name] = chain_info\n            return\n\n        # Loss function: loss : mse(pred, target) or loss : nn'cross_entropy_seq(logits, y)\n        loss_name = None\n        loss_args = []\n        if isinstance(val, ast.MethodCall) and isinstance(val.obj, ast.Ident):\n            if val.obj.name == 'nn' and val.method in self.loss_registry:\n                loss_name = val.method\n                loss_args = val.args\n        if loss_name is None and isinstance(val, ast.Call) and isinstance(val.func, ast.Ident):\n            if val.func.name in self.loss_registry and len(val.args) == 2:\n                loss_name = val.func.name\n                loss_args = val.args\n        # Pattern: loss : diff . diff  (self-dot = SSE loss)\n        # where diff is tracked as pred - target in chain\n        if loss_name is None and isinstance(val, ast.BinOp) and val.op == '.':\n            if (isinstance(val.left, ast.Ident) and isinstance(val.right, ast.Ident)\n                    and val.left.name == val.right.name):\n                diff_var = val.left.name\n                diff_info = self.chain.get(diff_var, {})\n                if diff_info.get('kind') == 'vec_sub':\n                    loss_name = 'sse'\n                    # Build synthetic args: the pred and target from the subtraction\n                    loss_args = [ast.Ident(name=diff_info['pred']),\n                                 ast.Ident(name=diff_info['target'])]\n        if loss_name:\n            loss_reg = self.loss_registry[loss_name]\n            pred_var = self._main_scalar_expr(loss_args[0])\n            second_var = self._main_scalar_expr(loss_args[1])\n            pred_dt = self.env.get(pred_var, UNKNOWN)\n            n = _size(pred_dt) if _is_vec_or_tensor(pred_dt) else self._infer_output_dim()\n\n            dims = [n]\n            if loss_reg['args'] == 'logits_targets':\n                if pred_dt.name == 'Tensor' and len(pred_dt.dims) == 2:\n                    S, V = pred_dt.dims\n                else:\n                    S, V = 128, 128\n                dims = [S, V]\n\n            ir_type = IRType('float')\n            body.append(IRLoss(out=name, out_type=ir_type, pred=pred_var,\n                               target=second_var, kind=loss_name, dims=dims))\n            if name not in self.main_declared:\n                self.main_declared.add(name)\n                self.env[name] = FLOAT\n\n            # Record chain info for backward pass\n            chain_info = {'kind': loss_name, 'pred': pred_var, 'n': n}\n            if loss_reg['args'] == 'pred_target':\n                chain_info['target'] = second_var\n            elif loss_reg['args'] == 'pred_label':\n                chain_info['label'] = second_var\n            elif loss_reg['args'] == 'logits_targets':\n                chain_info['targets'] = second_var\n                chain_info['S'] = dims[0]\n                chain_info['V'] = dims[1]\n            self.chain[name] = chain_info\n            return\n\n        # Model call: pred : net(x)\n        if isinstance(val, ast.Call) and isinstance(val.func, ast.Ident):\n            vname = val.func.name\n            if vname in self.model_vars:\n                tname = self.model_vars[vname]\n                apply_key = f\"{tname}'apply\"\n                fundef = self.fun_defs.get(apply_key)\n                if fundef and fundef.return_type:\n                    ret_dt = _dt(fundef.return_type)\n                    n = _size(ret_dt)\n                    arg_var = self._main_scalar_expr(val.args[0]) if val.args else '??'\n                    cache_var = f'cache_{vname}'\n                    ir_type = _to_ir_type(ret_dt, self.type_defs)\n                    body.append(IRModelCall(out=name, out_type=ir_type,\n                                           model_var=vname, model_type=tname,\n                                           input_var=arg_var, cache_var=cache_var,\n                                           out_n=n))\n                    if name not in self.main_declared:\n                        self.main_declared.add(name)\n                        self.env[name] = ret_dt\n                    self.chain[name] = {'kind': 'model_apply', 'model': vname,\n                                        'model_type': tname, 'input': arg_var,\n                                        'cache': cache_var}\n                    return\n\n        # Free function call: val : f(2.0)\n        if isinstance(val, ast.Call) and isinstance(val.func, ast.Ident):\n            fname = val.func.name\n            if fname in self.free_fns:\n                arg_c = self._main_scalar_expr(val.args[0]) if val.args else '0'\n                if name not in self.main_declared:\n                    self.main_declared.add(name)\n                    self.env[name] = FLOAT\n                body.append(IRAssign(var=name, expr=f'{fname}({arg_c})',\n                                     var_type=IRType('float'),\n                                     is_decl=True))\n                self.chain[name] = {'kind': 'scalar_fn', 'fn': fname, 'arg': arg_c}\n                return\n\n        # DerivExpr: deriv : d param(expr)\n        if isinstance(val, ast.DerivExpr):\n            # Model params derivative: grads : d model'params(loss)\n            if isinstance(val.param, ast.MemberAccess) and isinstance(val.param.obj, ast.Ident):\n                model_var = val.param.obj.name\n                if model_var in self.model_vars:\n                    loss_var = val.expr.name if isinstance(val.expr, ast.Ident) else '??'\n                    tname = self.model_vars[model_var]\n                    n_params = self._total_params(tname)\n                    loss_info = self.chain.get(loss_var, {})\n                    loss_kind = loss_info.get('kind', '')\n                    pred_var = loss_info.get('pred', '??')\n                    pred_info = self.chain.get(pred_var, {})\n\n                    body.append(IRGradCompute(\n                        grad_var=name, model_var=model_var, model_type=tname,\n                        loss_var=loss_var, loss_kind=loss_kind,\n                        loss_info=dict(loss_info), pred_var=pred_var,\n                        pred_info=dict(pred_info), n_params=n_params,\n                    ))\n                    if name not in self.main_declared:\n                        self.main_declared.add(name)\n                    self.grad_vars[name] = {\n                        'model_var': model_var, 'model_type': tname,\n                        'n_params': n_params,\n                    }\n                    return\n\n            # Scalar derivative: deriv : d param(expr)\n            param_name = val.param.name if isinstance(val.param, ast.Ident) else '??'\n            expr_name = val.expr.name if isinstance(val.expr, ast.Ident) else '??'\n            info = self.chain.get(expr_name, {})\n            if info.get('kind') == 'scalar_fn':\n                fn_name = info['fn']\n                arg_expr = info['arg']\n                body.append(IRDerivExpr(out_var=name, fn_name=fn_name, arg_expr=arg_expr))\n                if name not in self.main_declared:\n                    self.main_declared.add(name)\n                    self.env[name] = FLOAT\n                return\n\n        # Count: n : dataset'zip(labels)'count(lambda)\n        if isinstance(val, ast.MethodCall) and val.method == 'count':\n            inner = val.obj\n            if isinstance(inner, ast.MethodCall) and inner.method == 'zip':\n                x_ds_name = inner.obj.name if isinstance(inner.obj, ast.Ident) else None\n                y_ds_name = inner.args[0].name if inner.args and isinstance(inner.args[0], ast.Ident) else None\n                if x_ds_name and y_ds_name and x_ds_name in self.datasets:\n                    lam = val.args[0] if val.args and isinstance(val.args[0], ast.Lambda) else None\n                    lambda_body = ''\n                    model_call = None\n                    if lam:\n                        import re\n                        bindings = {lam.params[0]: '_cx', lam.params[1]: '_cy'}\n                        cond = self._compile_lambda_body(lam, bindings)\n                        model_match = re.search(r'@@MODELCALL@@(\\w+)@@(\\w+)@@', cond)\n                        if model_match:\n                            model_name = model_match.group(1)\n                            model_arg = model_match.group(2)\n                            if model_name in self.model_vars:\n                                tname = self.model_vars[model_name]\n                                apply_key = f\"{tname}'apply\"\n                                fundef = self.fun_defs.get(apply_key)\n                                ret_n = _size(_dt(fundef.return_type)) if fundef and fundef.return_type else self._infer_output_dim()\n                                model_call = {'model': model_name, 'model_type': tname,\n                                              'model_arg': model_arg, 'ret_n': ret_n}\n                                cond = cond.replace(model_match.group(0), '_eval_pred')\n                        lambda_body = cond\n                    body.append(IRCountOp(out_var=name, x_ds=x_ds_name, y_ds=y_ds_name,\n                                          lambda_body=lambda_body, model_call=model_call))\n                    if name not in self.main_declared:\n                        self.main_declared.add(name)\n                        self.env[name] = INT\n                    return\n\n        # Numeric literal\n        if isinstance(val, ast.Num):\n            c = f'{val.value}f' if not val.is_int else str(int(val.value))\n            dt = FLOAT if not val.is_int else INT\n            self.scalar_vals[name] = val.value\n            if name not in self.main_declared:\n                self.main_declared.add(name)\n                self.env[name] = dt\n            body.append(IRAssign(var=name, expr=c,\n                                 var_type=IRType('float') if dt == FLOAT else IRType('int'),\n                                 is_decl=(name not in self.main_declared)))\n            return\n\n        # Vec subtraction: diff : pred - y  (track in chain for SSE loss pattern)\n        if isinstance(val, ast.BinOp) and val.op == '-':\n            if isinstance(val.left, ast.Ident) and isinstance(val.right, ast.Ident):\n                lvar = val.left.name\n                rvar = val.right.name\n                ldt = self.env.get(lvar, UNKNOWN)\n                if _is_vec_or_tensor(ldt):\n                    self.chain[name] = {'kind': 'vec_sub', 'pred': lvar, 'target': rvar, 'n': _size(ldt)}\n                    self.env[name] = ldt\n\n        # Generic scalar expression\n        c = self._main_scalar_expr(val)\n        dt = self.env.get(name, FLOAT)\n        if name not in self.main_declared:\n            self.main_declared.add(name)\n            self.env[name] = dt\n        body.append(IRAssign(var=name, expr=c,\n                             var_type=IRType('float') if dt == FLOAT else IRType('int'),\n                             is_decl=False))\n\n    def _extract_lr_and_grad(self, value) -> tuple[object, str]:\n        \"\"\"Extract (lr, grad_var) from an expression like lr * grads or 0.001 * grads.\n\n        lr can be a float (literal) or a str (variable name).\n        \"\"\"\n        lr = 0.001\n        grad_var = '??'\n        if isinstance(value, ast.BinOp) and value.op == '*':\n            left, right = value.left, value.right\n            # Num * Ident or Ident * Num\n            if isinstance(left, ast.Num) and isinstance(right, ast.Ident):\n                lr = left.value\n                grad_var = right.name\n            elif isinstance(right, ast.Num) and isinstance(left, ast.Ident):\n                lr = right.value\n                grad_var = left.name\n            # Ident * Ident \u2014 keep variable name for lr\n            elif isinstance(left, ast.Ident) and isinstance(right, ast.Ident):\n                if right.name in self.grad_vars:\n                    lr = left.name\n                    grad_var = right.name\n                elif left.name in self.grad_vars:\n                    lr = right.name\n                    grad_var = left.name\n        elif isinstance(value, ast.Ident):\n            grad_var = value.name\n        return lr, grad_var\n\n    def _lower_compound_assign(self, stmt: ast.CompoundAssign, body: list):\n        \"\"\"Lower model'params -: lr * grads to IRParamUpdate.\"\"\"\n        target = stmt.target\n        # Expect target is model'params (MemberAccess)\n        if not (isinstance(target, ast.MemberAccess) and isinstance(target.obj, ast.Ident)):\n            return\n        model_var = target.obj.name\n        if model_var not in self.model_vars:\n            return\n\n        tname = self.model_vars[model_var]\n        n_params = self._total_params(tname)\n        lr, grad_var = self._extract_lr_and_grad(stmt.value)\n\n        body.append(IRParamUpdate(\n            model_var=model_var, model_type=tname,\n            grad_var=grad_var, n_params=n_params, lr=lr,\n        ))\n\n    def _try_lower_param_update(self, model_var: str, value, body: list) -> bool:\n        \"\"\"Try to lower model'params : model'params - lr * grads to IRParamUpdate.\n\n        Returns True if successfully handled.\n        \"\"\"\n        # Expect: model'params - lr * grads\n        if not (isinstance(value, ast.BinOp) and value.op == '-'):\n            return False\n        # LHS of subtraction should be model'params\n        lhs = value.left\n        if not (isinstance(lhs, ast.MemberAccess) and isinstance(lhs.obj, ast.Ident)\n                and lhs.obj.name == model_var and lhs.field_name == 'params'):\n            return False\n\n        tname = self.model_vars[model_var]\n        n_params = self._total_params(tname)\n        lr, grad_var = self._extract_lr_and_grad(value.right)\n\n        body.append(IRParamUpdate(\n            model_var=model_var, model_type=tname,\n            grad_var=grad_var, n_params=n_params, lr=lr,\n        ))\n        return True\n\n    def _lower_main_tuple_assign(self, stmt: ast.Assignment, body: list):\n        names = stmt.target.names\n        val = stmt.value\n\n        # nn'data'mnist() or nn'data'mnist(split: \"test\")\n        if isinstance(val, ast.MethodCall) and isinstance(val.obj, ast.MemberAccess):\n            if isinstance(val.obj.obj, ast.Ident) and val.obj.obj.name == 'nn' and val.obj.field_name == 'data' and val.method in self.data_providers:\n                split = 'train'\n                if 'split' in val.kwargs:\n                    kv = val.kwargs['split']\n                    if isinstance(kv, ast.Str):\n                        split = kv.value\n                x_name = names[0]\n                y_name = names[1]\n                body.append(IRDataLoad(x_name=x_name, y_name=y_name,\n                                       provider=val.method, split=split))\n                self.main_declared.update([x_name, y_name])\n                self.datasets[x_name] = {\n                    'data_var': f'_data_{x_name}', 'n_var': f'_n_{x_name}',\n                    'kind': 'images', 'dim': self._infer_input_dim()\n                }\n                self.datasets[y_name] = {\n                    'data_var': f'_data_{y_name}', 'n_var': f'_n_{x_name}',\n                    'kind': 'labels'\n                }\n                return\n\n        # text'random_batch(seq_len batch: N)\n        if isinstance(val, ast.MethodCall) and isinstance(val.obj, ast.Ident):\n            if val.obj.name in self.text_vars and val.method == 'random_batch':\n                text_var = val.obj.name\n                seq_len = int(val.args[0].value) if val.args and isinstance(val.args[0], ast.Num) else 128\n                batch_size = 1\n                if 'batch' in val.kwargs and isinstance(val.kwargs['batch'], ast.Num):\n                    batch_size = int(val.kwargs['batch'].value)\n                x_name = names[0]\n                y_name = names[1]\n                body.append(IRTextRandomBatch(x_var=x_name, y_var=y_name,\n                                              text_var=text_var, seq_len=seq_len,\n                                              batch_size=batch_size))\n                if x_name not in self.main_declared:\n                    self.main_declared.update([x_name, y_name])\n                    self.env[x_name] = DxType('Vec', [seq_len])\n                    self.env[y_name] = DxType('Vec', [seq_len])\n                self.chain[x_name] = {'kind': 'text_batch', 'text_var': text_var,\n                                       'seq_len': seq_len, 'batch_size': batch_size,\n                                       'y_name': y_name}\n                return\n\n    def _lower_main_exprstmt(self, stmt: ast.ExprStmt, body: list):\n        expr = stmt.expr\n\n        # print(...)\n        if isinstance(expr, ast.Call) and isinstance(expr.func, ast.Ident) and expr.func.name == 'print':\n            if expr.args and isinstance(expr.args[0], ast.Str):\n                body.append(self._lower_print_str(expr.args[0].value))\n            elif expr.args:\n                arg = expr.args[0]\n                if isinstance(arg, ast.MethodCall) and isinstance(arg.obj, ast.Ident):\n                    if arg.obj.name in self.text_vars and arg.method == 'decode':\n                        tok_arg = arg.args[0] if arg.args else None\n                        if tok_arg and isinstance(tok_arg, ast.Ident):\n                            body.append(IRTextDecode(text_var=arg.obj.name,\n                                                     tokens_var=tok_arg.name))\n            return\n\n        # opt'step(d net'params(loss))\n        if isinstance(expr, ast.MethodCall) and expr.method == 'step':\n            if expr.args and isinstance(expr.args[0], ast.DerivExpr):\n                self._lower_backward_step(expr.args[0], body)\n                return\n\n    def _lower_backward_step(self, deriv: ast.DerivExpr, body: list):\n        loss_var = deriv.expr.name if isinstance(deriv.expr, ast.Ident) else '??'\n        model_var = '??'\n        if isinstance(deriv.param, ast.MemberAccess) and isinstance(deriv.param.obj, ast.Ident):\n            model_var = deriv.param.obj.name\n\n        if model_var not in self.model_vars:\n            return\n\n        tname = self.model_vars[model_var]\n        n_params = self._total_params(tname)\n        loss_info = self.chain.get(loss_var, {})\n        loss_kind = loss_info.get('kind', '')\n        pred_var = loss_info.get('pred', '??')\n        pred_info = self.chain.get(pred_var, {})\n\n        body.append(IRBackwardStep(\n            model_var=model_var, model_type=tname,\n            loss_var=loss_var, loss_kind=loss_kind,\n            loss_info=dict(loss_info), pred_var=pred_var,\n            pred_info=dict(pred_info), grad_var=f'grad_{model_var}',\n            n_params=n_params, opt_kind=self.opt_method,\n            opt_lr=self.opt_lr,\n            opt_config={'beta1': 0.9, 'beta2': 0.999, 'eps': 1e-8} if self.opt_method == 'adam' else {},\n        ))\n\n    def _lower_main_for(self, stmt: ast.ForLoop, body: list):\n        # Tuple destructure: for (x y) in dataset'zip(...)...\n        if isinstance(stmt.var, ast.TupleDestructure):\n            # This is the dataset iteration pattern \u2014 we keep it as an IRFor\n            # with the full AST-level info, emit_c will handle it\n            ir_for = IRFor(var='_tuple_', start=0, end=0, body=[])\n            ir_for._ast_stmt = stmt  # attach original AST for emit_c\n            ir_for._lowerer = self  # attach lowerer for chain/datasets access\n            body.append(ir_for)\n            return\n\n        # Simple range: for var in start..end\n        var = stmt.var.name if isinstance(stmt.var, ast.Ident) else '_i'\n        if isinstance(stmt.iterable, ast.Range):\n            s_node = stmt.iterable.start\n            e_node = stmt.iterable.end\n            if isinstance(s_node, ast.Num):\n                start = int(s_node.value)\n            else:\n                start = self._main_scalar_expr(s_node)\n            if isinstance(e_node, ast.Num):\n                end = int(e_node.value)\n            else:\n                end = self._main_scalar_expr(e_node)\n\n            self.env[var] = INT\n            self.main_declared.add(var)\n\n            ir_for = IRFor(var=var, start=start, end=end, body=[])\n            for s in stmt.body:\n                self._lower_main_stmt(s, ir_for.body)\n            body.append(ir_for)\n\n    def _lower_main_if(self, stmt: ast.IfExpr, body: list):\n        cond = self._main_scalar_expr(stmt.condition)\n        ir_if = IRIf(cond=cond, then_body=[], else_body=[])\n        for s in stmt.body:\n            self._lower_main_stmt(s, ir_if.then_body)\n        for s in stmt.else_body:\n            self._lower_main_stmt(s, ir_if.else_body)\n        body.append(ir_if)\n\n    def _lower_print_str(self, fmt: str) -> IRPrint:\n        \"\"\"Convert dx string interpolation to IRPrint.\"\"\"\n        import re\n        # Match {var}, {var'field}, {var''field}, {var[idx]}, {var[expr]}\n        parts = re.split(r\"\\{(\\w+(?:''?\\w+|\\[[^\\]]+\\])?)\\}\", fmt)\n        format_str = ''\n        args = []\n        for i, part in enumerate(parts):\n            if i % 2 == 0:\n                format_str += part\n            else:\n                # String array index: names[i] or names[i + 1]\n                m_idx = re.match(r'^(\\w+)\\[([^\\]]+)\\]$', part)\n                if m_idx:\n                    var_name = m_idx.group(1)\n                    idx_expr = m_idx.group(2).strip()\n                    dt = self.env.get(var_name, UNKNOWN)\n                    if dt.name == 'StringArray':\n                        format_str += '%s'\n                        args.append((f'{var_name}[{idx_expr}]', '%s'))\n                    elif dt.name == 'Tensor' and len(dt.dims) == 2:\n                        # Tensor row element: prefs[i] prints first element\n                        cols = dt.dims[1]\n                        format_str += '%.4f'\n                        args.append((f'{var_name}[({idx_expr}) * {cols}]', '%.4f'))\n                    else:\n                        format_str += '%.4f'\n                        args.append((f'{var_name}[{idx_expr}]', '%.4f'))\n                    continue\n                # Mention: var''field \u2192 lookup set element name\n                if \"''\" in part:\n                    obj_name, field = part.split(\"''\", 1)\n                    if field == 'name' and obj_name in self.set_element_vars:\n                        set_name = self.set_element_vars[obj_name]\n                        idx_var = self.set_element_vars.get(f'_idx_{obj_name}', '_i')\n                        format_str += '%s'\n                        args.append((f'_{set_name}_names[{idx_var}]', '%s'))\n                    else:\n                        format_str += '%s'\n                        args.append((f'{obj_name}_name', '%s'))\n                    continue\n                if \"'\" in part:\n                    obj_name, field = part.split(\"'\", 1)\n                    if field == 'size' and obj_name in self.datasets:\n                        format_str += '%d'\n                        args.append((self.datasets[obj_name]['n_var'], '%d'))\n                    else:\n                        format_str += '%.4f'\n                        args.append((f'{obj_name}.{field}', '%.4f'))\n                else:\n                    var = part\n                    dt = self.env.get(var, UNKNOWN)\n                    if _is_vec_or_tensor(dt):\n                        format_str += '%.4f'\n                        args.append((f'{var}[0]', '%.4f'))\n                    elif dt == INT or dt == DxType('Int'):\n                        format_str += '%d'\n                        args.append((var, '%d'))\n                    else:\n                        format_str += '%.4f'\n                        args.append((var, '%.4f'))\n        return IRPrint(format_str=format_str, args=args)\n\n    # ------- scalar expression compilation -------\n\n    def _main_vec_ref(self, node) -> tuple[str, DxType]:\n        \"\"\"Return (C pointer expression, type) for a vector-like node.\n\n        Handles plain Vec idents and Tensor row access (tensor[i]).\n        \"\"\"\n        if isinstance(node, ast.Ident):\n            dt = self.env.get(node.name, UNKNOWN)\n            return (node.name, dt)\n        if isinstance(node, ast.Index) and isinstance(node.obj, ast.Ident):\n            obj_dt = self.env.get(node.obj.name, UNKNOWN)\n            if obj_dt.name == 'Tensor' and len(obj_dt.dims) == 2:\n                cols = obj_dt.dims[1]\n                idx = self._main_scalar_expr(node.index)\n                row_ref = f'&{node.obj.name}[({idx}) * {cols}]'\n                return (row_ref, DxType('Vec', [cols]))\n        return (self._main_scalar_expr(node), UNKNOWN)\n\n    def _main_scalar_expr(self, node) -> str:\n        if isinstance(node, ast.UnaryOp) and node.op == '-':\n            inner = self._main_scalar_expr(node.operand)\n            return f'(-{inner})'\n        if isinstance(node, ast.Num):\n            if node.is_int:\n                return str(int(node.value))\n            return f'{node.value}f'\n        if isinstance(node, ast.Ident):\n            return node.name\n        if isinstance(node, ast.BinOp):\n            if node.op == '~':\n                lref, ldt = self._main_vec_ref(node.left)\n                rref, rdt = self._main_vec_ref(node.right)\n                if ldt.name in ('Vec', 'Tensor') and rdt.name in ('Vec', 'Tensor'):\n                    n = ldt.dims[-1] if ldt.dims else 1\n                    return f'CosineSimilarity({lref}, {rref}, {n})'\n            l = self._main_scalar_expr(node.left)\n            r = self._main_scalar_expr(node.right)\n            op = node.op\n            if op == '=':\n                op = '=='\n            if op == '<>':\n                op = '!='\n            return f'({l} {op} {r})'\n        if isinstance(node, ast.Pronoun):\n            if node.name == 'it' and self.it_var:\n                return self.it_var[0]\n            return node.name\n        if isinstance(node, ast.AsExpr):\n            inner = self._main_scalar_expr(node.expr)\n            if node.target_type == 'Float':\n                return f'(float){inner}'\n            if node.target_type == 'Int':\n                return f'(int){inner}'\n            return inner\n        if isinstance(node, ast.MemberAccess):\n            if isinstance(node.obj, ast.Ident) and node.field_name == 'size':\n                ds_name = node.obj.name\n                if ds_name in self.datasets:\n                    return self.datasets[ds_name]['n_var']\n            obj = self._main_scalar_expr(node.obj)\n            return f'{obj}.{node.field_name}'\n        if isinstance(node, ast.MethodCall) and node.method == 'count':\n            inner = node.obj\n            if isinstance(inner, ast.MethodCall) and inner.method == 'zip':\n                pred_var = self._main_scalar_expr(inner.obj)\n                label_var = self._main_scalar_expr(inner.args[0]) if inner.args else '0'\n                lam = node.args[0] if node.args and isinstance(node.args[0], ast.Lambda) else None\n                if lam:\n                    bindings = {lam.params[0]: pred_var, lam.params[1]: label_var}\n                    cond = self._compile_lambda_body(lam, bindings)\n                    return f'({cond} ? 1 : 0)'\n                pred_dt = self.env.get(pred_var, UNKNOWN)\n                n = _size(pred_dt) if _is_vec_or_tensor(pred_dt) else self._infer_output_dim()\n                return f'((Argmax({pred_var}, {n}) == {label_var}) ? 1 : 0)'\n        if isinstance(node, ast.Call) and isinstance(node.func, ast.Ident):\n            fname = node.func.name\n            args = ', '.join(self._main_scalar_expr(a) for a in node.args)\n            if fname in self.model_vars:\n                return f'{fname}_out'\n            return f'{fname}({args})'\n        return '0'\n\n    def _compile_lambda_body(self, lam, bindings: dict[str, str]) -> str:\n        return self._compile_lambda_expr(lam.body, bindings)\n\n    def _compile_lambda_expr(self, node, bindings: dict[str, str]) -> str:\n        if isinstance(node, ast.Ident):\n            if node.name in bindings:\n                return bindings[node.name]\n            return node.name\n        if isinstance(node, ast.Num):\n            return f'{node.value}f' if not node.is_int else str(int(node.value))\n        if isinstance(node, ast.BinOp):\n            l = self._compile_lambda_expr(node.left, bindings)\n            r = self._compile_lambda_expr(node.right, bindings)\n            op = '==' if node.op == '=' else node.op\n            return f'({l} {op} {r})'\n        if isinstance(node, ast.Call) and isinstance(node.func, ast.Ident):\n            fname = node.func.name\n            if fname == 'argmax':\n                arg = self._compile_lambda_expr(node.args[0], bindings)\n                arg_dt = self.env.get(arg, UNKNOWN)\n                n = _size(arg_dt) if _is_vec_or_tensor(arg_dt) else self._infer_output_dim()\n                return f'Argmax({arg}, {n})'\n            if fname in self.model_vars:\n                arg = self._compile_lambda_expr(node.args[0], bindings)\n                return f'@@MODELCALL@@{fname}@@{arg}@@'\n            args = [self._compile_lambda_expr(a, bindings) for a in node.args]\n            return f'{fname}({\", \".join(args)})'\n        return '0'\n\n    # ------- arena helpers -------\n\n    def _body_has_backward(self, body: list) -> bool:\n        for s in body:\n            if isinstance(s, ast.ArenaBlock):\n                if self._body_has_backward(s.body):\n                    return True\n            if isinstance(s, IRArenaBlock):\n                if self._ir_body_has_backward(s.body):\n                    return True\n            if isinstance(s, ast.ExprStmt) and isinstance(s.expr, ast.MethodCall):\n                if s.expr.method == 'step' and s.expr.args:\n                    if isinstance(s.expr.args[0], ast.DerivExpr):\n                        return True\n            if isinstance(s, IRBackwardStep):\n                return True\n        return False\n\n    def _ir_body_has_backward(self, body: list) -> bool:\n        for s in body:\n            if isinstance(s, (IRBackwardStep, IRGradCompute, IRParamUpdate)):\n                return True\n            if isinstance(s, IRArenaBlock):\n                if self._ir_body_has_backward(s.body):\n                    return True\n        return False\n\n    def _find_loss_var_ir(self, body: list) -> str | None:\n        for s in body:\n            if isinstance(s, IRBackwardStep):\n                return s.loss_var\n            if isinstance(s, IRGradCompute):\n                return s.loss_var\n            if isinstance(s, IRArenaBlock):\n                result = self._find_loss_var_ir(s.body)\n                if result:\n                    return result\n        return None\n\n    # ------- top-level: lower entire program -------\n\n    def lower(self) -> IRModule:\n        module = IRModule()\n\n        # Topological sort types\n        type_order = self._topo_sort_types()\n        module.type_order = type_order\n\n        # Struct definitions\n        for name in type_order:\n            fields = self.type_defs[name]\n            module.struct_defs.append(self._lower_struct_def(name, fields))\n            module.grad_struct_defs.append(self._lower_grad_struct_def(name, fields))\n\n        # Free functions (scalar + dual)\n        for key, fundef in self.fun_defs.items():\n            if fundef.type_name:\n                continue\n            self.free_fns.add(fundef.method_name)\n            fn = self._lower_scalar_fn(fundef)\n            module.scalar_fns.append(fn)\n            module.dual_fns.append(fn)  # emit_c will generate dual version\n\n        # Methods (forward/backward) in topological order\n        emitted = set()\n        for tname in type_order:\n            for key, fundef in self.fun_defs.items():\n                if fundef.type_name == tname and key not in emitted:\n                    emitted.add(key)\n                    fn, tape = self._lower_method(fundef)\n                    module.functions.append(fn)\n\n        # Init functions\n        module.init_fns = list(type_order)\n\n        # Main body\n        for s in self.program.statements:\n            if isinstance(s, (ast.TypeDef, ast.FunDef)):\n                continue\n            self._lower_main_stmt(s, module.main_body)\n\n        return module\n\n\ndef lower_to_ir(program: ast.Program) -> IRModule:\n    \"\"\"Lower an AST Program to an IRModule.\"\"\"\n    return IRLower(program).lower()\n",
    "ir_passes.py": "\"\"\"IR optimization passes for the dx compiler.\n\nPasses transform an IRModule to improve the generated C code:\n  - Constant extraction: collect dimensions \u2192 #define\n  - Memory planning: decide stack vs heap for buffers\n  - Redundancy elimination: suppress redundant forward calls in batched training\n\"\"\"\n\nfrom __future__ import annotations\nfrom .ir import *\n\n\ndef extract_constants(module: IRModule) -> IRModule:\n    \"\"\"Scan all IR nodes, collect repeated dimensions, populate module.constants.\n\n    Generates named constants like HIDDEN_DIM, INPUT_DIM, SEQ_LEN, etc.\n    from struct field dimensions and function parameters.\n    \"\"\"\n    dim_uses: dict[int, list[str]] = {}  # value \u2192 list of context hints\n\n    # Collect dimensions from struct fields\n    for sdef in module.struct_defs:\n        for fname, ftype in sdef.fields:\n            for d in ftype.dims:\n                hint = f'{sdef.name}_{fname}'\n                dim_uses.setdefault(d, []).append(hint)\n\n    # Collect dimensions from function tapes\n    for fn in module.functions:\n        if not fn.is_method:\n            continue\n        for entry in fn.body:\n            if isinstance(entry, dict):\n                for key in ('M', 'K', 'P', 'N', 'S', 'D', 'H', 'n'):\n                    if key in entry and isinstance(entry[key], (int, float)):\n                        dim_uses.setdefault(entry[key], []).append(f'{fn.name}_{key}')\n\n    # Name the constants based on common patterns\n    named: dict[int, str] = {}\n    for val, hints in dim_uses.items():\n        if val <= 1:\n            continue\n        # Check if this value appears in multiple contexts (worth naming)\n        if len(hints) < 2 and val < 10:\n            continue\n        name = _infer_constant_name(val, hints)\n        if name:\n            named[val] = name\n\n    # Compute total params for each top-level type\n    type_params = {}\n    for sdef in module.struct_defs:\n        total = _count_params(sdef, {s.name: s for s in module.struct_defs})\n        if total > 100:\n            type_params[sdef.name] = total\n\n    # Add NUM_PARAMS for the largest type\n    if type_params:\n        largest = max(type_params, key=type_params.get)\n        module.constants[f'NUM_PARAMS_{largest.upper()}'] = type_params[largest]\n\n    # Merge named constants\n    for val, name in named.items():\n        if name not in module.constants:\n            module.constants[name] = val\n\n    return module\n\n\ndef _count_params(sdef: IRStructDef, all_structs: dict[str, IRStructDef]) -> int:\n    \"\"\"Count total float parameters in a struct (recursive).\"\"\"\n    total = 0\n    for _, ftype in sdef.fields:\n        if ftype.kind == 'struct' and ftype.struct_name in all_structs:\n            total += _count_params(all_structs[ftype.struct_name], all_structs)\n        else:\n            total += ftype.size\n    return total\n\n\ndef _infer_constant_name(val: int, hints: list[str]) -> str | None:\n    \"\"\"Try to infer a meaningful constant name from context hints.\"\"\"\n    hints_lower = ' '.join(h.lower() for h in hints)\n\n    # Common dimension patterns\n    if 'seq' in hints_lower or 'pos_emb' in hints_lower or 'tok_emb' in hints_lower:\n        if any('_S' in h or '_tokens' in h for h in hints):\n            return 'SEQ_LEN'\n    if 'embed' in hints_lower or '_D' in ' '.join(hints):\n        if val >= 16:\n            return 'EMBED_DIM'\n    if 'head' in hints_lower or '_H' in ' '.join(hints):\n        if val <= 32:\n            return 'NUM_HEADS'\n    if 'ffn' in hints_lower or 'w1' in hints_lower:\n        if val > 64:\n            return 'FFN_DIM'\n    if 'vocab' in hints_lower:\n        return 'VOCAB_SIZE'\n\n    # Fallback: don't name small or uncommon values\n    return None\n\n\ndef plan_memory(module: IRModule) -> IRModule:\n    \"\"\"Decide stack vs heap for each buffer based on size.\n\n    Threshold: 4096 floats (16KB) \u2192 stack, above \u2192 heap.\n    \"\"\"\n    STACK_THRESHOLD = 4096\n\n    for fn in module.functions:\n        if not fn.is_method:\n            continue\n        for entry in fn.body:\n            if isinstance(entry, dict) and 'n' in entry:\n                n = entry['n']\n                entry['_heap'] = n > STACK_THRESHOLD\n\n    return module\n\n\ndef mark_arena_redundant(module: IRModule, lowerer) -> IRModule:\n    \"\"\"In arena blocks with batched training, mark redundant forward/loss vars.\n\n    When batch_size > 1, the backward step re-does forward+loss per batch\n    item, so the arena-level forward call and loss are redundant.\n    \"\"\"\n    for item in module.main_body:\n        _mark_redundant_in(item, lowerer)\n    return module\n\n\ndef _mark_redundant_in(item, lowerer):\n    \"\"\"Recursively mark redundant items in arena blocks.\"\"\"\n    if isinstance(item, IRFor):\n        for child in item.body:\n            _mark_redundant_in(child, lowerer)\n    elif isinstance(item, IRArenaBlock):\n        # Check if this arena contains a backward step\n        has_backward = False\n        backward_step = None\n        for child in item.body:\n            if isinstance(child, IRBackwardStep):\n                has_backward = True\n                backward_step = child\n                break\n        if not has_backward or backward_step is None:\n            return\n\n        # Check if input comes from a batched text source\n        pred_info = backward_step.pred_info\n        if pred_info.get('kind') != 'model_apply':\n            return\n        input_var = pred_info.get('input', '')\n        input_info = lowerer.chain.get(input_var, {})\n        batch_size = input_info.get('batch_size', 1) if input_info.get('kind') == 'text_batch' else 1\n        if batch_size <= 1:\n            return\n\n        # Mark forward call and loss as redundant\n        pred_var = backward_step.pred_var\n        loss_var = backward_step.loss_var\n        for child in item.body:\n            if isinstance(child, IRModelCall) and child.out == pred_var:\n                child._redundant = True\n            if isinstance(child, IRLoss) and child.out == loss_var:\n                child._redundant = True\n\n\ndef run_all_passes(module: IRModule, lowerer=None) -> IRModule:\n    \"\"\"Run all IR passes in order.\"\"\"\n    module = extract_constants(module)\n    module = plan_memory(module)\n    if lowerer:\n        module = mark_arena_redundant(module, lowerer)\n    return module\n",
    "emit_js.py": "\"\"\"IR \u2192 JavaScript emission for the dx compiler.\n\nTakes an IRModule and produces a standalone JS file that runs in Node.js\nor browser. The generated JS follows the same conventions as emit_c.py:\nUpperCamelCase runtime functions, lowerCamelCase variables, named constants.\n\nUsage:\n    from dx.emit_js import emit_js\n    js_code = emit_js(module, lowerer, source_file='xor.dx')\n\"\"\"\n\nfrom __future__ import annotations\nimport re\nfrom .ir import *\nfrom .typechecker import DxType, UNKNOWN, INT, FLOAT\n\n\n# ---------------------------------------------------------------------------\n# JS runtime \u2014 same functions as C runtime, ported to JavaScript\n# ---------------------------------------------------------------------------\n\nJS_RUNTIME = r\"\"\"\n// ---- Runtime ----\n\n// --- RNG (xorshift128) ---\nlet _rx = 123456789, _ry = 362436069, _rz = 521288629, _rw = 88675123;\nfunction RngSeed(s) {\n    s = s | 0;\n    _rx = s;\n    _ry = Math.imul(s, 48271) | 0; if (_ry === 0) _ry = 362436069;\n    _rz = Math.imul(_ry, 48271) | 0; if (_rz === 0) _rz = 521288629;\n    _rw = Math.imul(_rz, 48271) | 0; if (_rw === 0) _rw = 88675123;\n}\nfunction RngNext() {\n    let t = _rx ^ ((_rx << 11) | 0);\n    _rx = _ry; _ry = _rz; _rz = _rw;\n    _rw = (_rw ^ (_rw >>> 19) ^ (t ^ (t >>> 8))) | 0;\n    return _rw;\n}\nfunction RngFloat() {\n    return (RngNext() >>> 0) / 4294967296.0;\n}\nfunction RngNormal() {\n    let u1, u2;\n    do { u1 = RngFloat(); } while (u1 === 0.0);\n    u2 = RngFloat();\n    return Math.sqrt(-2.0 * Math.log(u1)) * Math.cos(6.2831853 * u2);\n}\n\n// --- Matmul: out[M] = W[M,N] @ x[N] ---\nfunction Matmul(W, x, out, M, N) {\n    for (let i = 0; i < M; i++) {\n        let s = 0.0;\n        for (let j = 0; j < N; j++) s += W[i * N + j] * x[j];\n        out[i] = s;\n    }\n}\n\n// --- MatmulTranspose: out[N] = W[M,N]^T @ x[M] ---\nfunction MatmulTranspose(W, x, out, M, N) {\n    out.fill(0);\n    for (let i = 0; i < M; i++)\n        for (let j = 0; j < N; j++)\n            out[j] += W[i * N + j] * x[i];\n}\n\n// --- OuterProductAdd: out[M,N] += a[M] * b[N]^T ---\nfunction OuterProductAdd(a, b, out, M, N) {\n    for (let i = 0; i < M; i++)\n        for (let j = 0; j < N; j++)\n            out[i * N + j] += a[i] * b[j];\n}\n\n// --- Matmul2d: C[M,P] = A[M,K] @ B[K,P] ---\nfunction Matmul2d(A, B, C, M, K, P) {\n    for (let i = 0; i < M; i++)\n        for (let j = 0; j < P; j++) {\n            let s = 0.0;\n            for (let k = 0; k < K; k++) s += A[i*K+k] * B[k*P+j];\n            C[i*P+j] = s;\n        }\n}\n\n// --- Matmul2dTransB: C[M,P] = A[M,K] @ B[P,K]^T ---\nfunction Matmul2dTransB(A, B, C, M, K, P) {\n    for (let i = 0; i < M; i++)\n        for (let j = 0; j < P; j++) {\n            let s = 0.0;\n            for (let k = 0; k < K; k++) s += A[i*K+k] * B[j*K+k];\n            C[i*P+j] = s;\n        }\n}\n\n// --- Matmul2dTransA: C[K,P] = A[M,K]^T @ B[M,P] ---\nfunction Matmul2dTransA(A, B, C, M, K, P) {\n    C.fill(0);\n    for (let i = 0; i < M; i++)\n        for (let j = 0; j < K; j++)\n            for (let p = 0; p < P; p++)\n                C[j*P+p] += A[i*K+j] * B[i*P+p];\n}\n\n// --- VecAdd: out[N] = a[N] + b[N] ---\nfunction VecAdd(a, b, out, N) {\n    for (let i = 0; i < N; i++) out[i] = a[i] + b[i];\n}\n\n// --- VecSub: out[N] = a[N] - b[N] ---\nfunction VecSub(a, b, out, N) {\n    for (let i = 0; i < N; i++) out[i] = a[i] - b[i];\n}\n\n// --- DotProduct: sum(a[i]*b[i]) ---\nfunction DotProduct(a, b, N) {\n    let s = 0.0;\n    for (let i = 0; i < N; i++) s += a[i] * b[i];\n    return s;\n}\n\n// --- CosineSimilarity: dot(a,b) / (||a|| * ||b||) ---\nfunction CosineSimilarity(a, b, N) {\n    let dot = 0.0, na = 0.0, nb = 0.0;\n    for (let i = 0; i < N; i++) { dot += a[i]*b[i]; na += a[i]*a[i]; nb += b[i]*b[i]; }\n    na = Math.sqrt(na); nb = Math.sqrt(nb);\n    return (na > 0 && nb > 0) ? dot / (na * nb) : 0.0;\n}\n\n// --- sigmoid ---\nfunction SigmoidForward(x, out, N) {\n    for (let i = 0; i < N; i++) out[i] = 1.0 / (1.0 + Math.exp(-x[i]));\n}\nfunction SigmoidBackward(sigOut, dOut, dIn, N) {\n    for (let i = 0; i < N; i++) dIn[i] = dOut[i] * sigOut[i] * (1.0 - sigOut[i]);\n}\n\n// --- relu ---\nfunction ReluForward(x, out, N) {\n    for (let i = 0; i < N; i++) out[i] = x[i] > 0.0 ? x[i] : 0.0;\n}\nfunction ReluBackward(x, dOut, dIn, N) {\n    for (let i = 0; i < N; i++) dIn[i] = x[i] > 0.0 ? dOut[i] : 0.0;\n}\n\n// --- BroadcastAdd: out[M,N] = A[M,N] + b[N] ---\nfunction BroadcastAdd(A, b, out, M, N) {\n    for (let i = 0; i < M; i++)\n        for (let j = 0; j < N; j++)\n            out[i*N+j] = A[i*N+j] + b[j];\n}\n\n// --- gelu ---\nfunction GeluForward(x, out, N) {\n    for (let i = 0; i < N; i++) {\n        const xi = x[i];\n        const cdf = 0.5 * (1.0 + Math.tanh(0.7978845608 * (xi + 0.044715 * xi * xi * xi)));\n        out[i] = xi * cdf;\n    }\n}\nfunction GeluBackward(x, dOut, dIn, N) {\n    for (let i = 0; i < N; i++) {\n        const xi = x[i];\n        const t = 0.7978845608 * (xi + 0.044715 * xi * xi * xi);\n        const th = Math.tanh(t);\n        const cdf = 0.5 * (1.0 + th);\n        const pdf = 0.7978845608 * (1.0 + 0.134145 * xi * xi);\n        const sech2 = 1.0 - th * th;\n        dIn[i] = dOut[i] * (cdf + xi * 0.5 * sech2 * pdf);\n    }\n}\n\n// --- softmax ---\nfunction SoftmaxForward(x, out, N) {\n    let mx = x[0]; for (let i = 1; i < N; i++) if (x[i] > mx) mx = x[i];\n    let s = 0.0;\n    for (let i = 0; i < N; i++) { out[i] = Math.exp(x[i] - mx); s += out[i]; }\n    for (let i = 0; i < N; i++) out[i] /= s;\n}\nfunction SoftmaxBackward(out, dOut, dIn, N) {\n    let dot = 0.0;\n    for (let j = 0; j < N; j++) dot += dOut[j] * out[j];\n    for (let i = 0; i < N; i++) dIn[i] = out[i] * (dOut[i] - dot);\n}\n\n// --- log ---\nfunction LogForward(x, out, N) {\n    for (let i = 0; i < N; i++) out[i] = Math.log(x[i] > 1e-7 ? x[i] : 1e-7);\n}\nfunction LogBackward(x, dOut, dIn, N) {\n    for (let i = 0; i < N; i++) { const xi = x[i] > 1e-7 ? x[i] : 1e-7; dIn[i] = dOut[i] / xi; }\n}\n\n// --- negate ---\nfunction NegateForward(x, out, N) {\n    for (let i = 0; i < N; i++) out[i] = -x[i];\n}\nfunction NegateBackward(dOut, dIn, N) {\n    for (let i = 0; i < N; i++) dIn[i] = -dOut[i];\n}\n\n// --- embed ---\nfunction EmbedForward(table, tokens, out, S, D) {\n    const V = (table.length / D) | 0;\n    for (let i = 0; i < S; i++) {\n        const idx = ((tokens[i] | 0) % V + V) % V;\n        out.set(table.subarray(idx*D, idx*D+D), i*D);\n    }\n}\nfunction EmbedBackward(tokens, dOut, dTable, S, D) {\n    const V = (dTable.length / D) | 0;\n    for (let i = 0; i < S; i++) {\n        const idx = ((tokens[i] | 0) % V + V) % V;\n        for (let j = 0; j < D; j++) dTable[idx*D+j] += dOut[i*D+j];\n    }\n}\n\n// --- layer_norm ---\nfunction LayerNormForward(x, gamma, beta, out, mean, rstd, S, D) {\n    for (let i = 0; i < S; i++) {\n        let m = 0.0;\n        for (let j = 0; j < D; j++) m += x[i*D+j];\n        m /= D;\n        let v = 0.0;\n        for (let j = 0; j < D; j++) { const d = x[i*D+j] - m; v += d*d; }\n        v /= D;\n        const rs = 1.0 / Math.sqrt(v + 1e-5);\n        mean[i] = m; rstd[i] = rs;\n        for (let j = 0; j < D; j++)\n            out[i*D+j] = (x[i*D+j] - m) * rs * gamma[j] + beta[j];\n    }\n}\nfunction LayerNormBackward(x, gamma, mean, rstd, dOut, dX, dGamma, dBeta, S, D) {\n    for (let i = 0; i < S; i++) {\n        const m = mean[i], rs = rstd[i];\n        let sumDhat = 0.0, sumDhatXhat = 0.0;\n        for (let j = 0; j < D; j++) {\n            const xhat = (x[i*D+j] - m) * rs;\n            const dhat = dOut[i*D+j] * gamma[j];\n            sumDhat += dhat;\n            sumDhatXhat += dhat * xhat;\n            dGamma[j] += dOut[i*D+j] * xhat;\n            dBeta[j] += dOut[i*D+j];\n        }\n        for (let j = 0; j < D; j++) {\n            const xhat = (x[i*D+j] - m) * rs;\n            const dhat = dOut[i*D+j] * gamma[j];\n            dX[i*D+j] = rs / D * (D * dhat - sumDhat - xhat * sumDhatXhat);\n        }\n    }\n}\n\n// --- causal_mha ---\nfunction CausalMhaForward(Q, K, V, out, attnBuf, S, D, H) {\n    const dh = (D / H) | 0;\n    const scale = 1.0 / Math.sqrt(dh);\n    for (let h = 0; h < H; h++) {\n        for (let i = 0; i < S; i++) {\n            let maxval = -1e9;\n            for (let j = 0; j <= i; j++) {\n                let score = 0.0;\n                for (let d = 0; d < dh; d++)\n                    score += Q[i*D + h*dh + d] * K[j*D + h*dh + d];\n                score *= scale;\n                attnBuf[h*S*S + i*S + j] = score;\n                if (score > maxval) maxval = score;\n            }\n            let sum = 0.0;\n            for (let j = 0; j <= i; j++) {\n                attnBuf[h*S*S + i*S + j] = Math.exp(attnBuf[h*S*S + i*S + j] - maxval);\n                sum += attnBuf[h*S*S + i*S + j];\n            }\n            for (let j = 0; j <= i; j++)\n                attnBuf[h*S*S + i*S + j] /= sum;\n            for (let j = i+1; j < S; j++)\n                attnBuf[h*S*S + i*S + j] = 0.0;\n            for (let d = 0; d < dh; d++) {\n                let val = 0.0;\n                for (let j = 0; j <= i; j++)\n                    val += attnBuf[h*S*S + i*S + j] * V[j*D + h*dh + d];\n                out[i*D + h*dh + d] = val;\n            }\n        }\n    }\n}\nfunction CausalMhaBackward(Q, K, V, attnBuf, dOut, dQ, dK, dV, S, D, H) {\n    const dh = (D / H) | 0;\n    const scale = 1.0 / Math.sqrt(dh);\n    for (let h = 0; h < H; h++) {\n        for (let i = 0; i < S; i++) {\n            const dAttn = new Float32Array(S);\n            for (let j = 0; j <= i; j++)\n                for (let d = 0; d < dh; d++)\n                    dAttn[j] += dOut[i*D + h*dh + d] * V[j*D + h*dh + d];\n            for (let j = 0; j <= i; j++)\n                for (let d = 0; d < dh; d++)\n                    dV[j*D + h*dh + d] += attnBuf[h*S*S + i*S + j] * dOut[i*D + h*dh + d];\n            let dot = 0.0;\n            for (let j = 0; j <= i; j++) dot += attnBuf[h*S*S + i*S + j] * dAttn[j];\n            const dScore = new Float32Array(S);\n            for (let j = 0; j <= i; j++)\n                dScore[j] = attnBuf[h*S*S + i*S + j] * (dAttn[j] - dot) * scale;\n            for (let j = 0; j <= i; j++)\n                for (let d = 0; d < dh; d++) {\n                    dQ[i*D + h*dh + d] += dScore[j] * K[j*D + h*dh + d];\n                    dK[j*D + h*dh + d] += dScore[j] * Q[i*D + h*dh + d];\n                }\n        }\n    }\n}\n\n// --- MSE ---\nfunction MseForward(pred, target, N) {\n    let s = 0.0;\n    for (let i = 0; i < N; i++) { const d = pred[i] - target[i]; s += d * d; }\n    return s / N;\n}\nfunction MseBackward(pred, target, dPred, N) {\n    for (let i = 0; i < N; i++) dPred[i] = 2.0 * (pred[i] - target[i]) / N;\n}\n\n// --- SSE (sum of squared errors, no /N) ---\nfunction SseForward(pred, target, N) {\n    let s = 0.0;\n    for (let i = 0; i < N; i++) { const d = pred[i] - target[i]; s += d * d; }\n    return s;\n}\nfunction SseBackward(pred, target, dPred, N) {\n    for (let i = 0; i < N; i++) dPred[i] = 2.0 * (pred[i] - target[i]);\n}\n\n// --- cross_entropy ---\nfunction CrossEntropyForward(pred, label, N) {\n    let p = pred[label];\n    if (p < 1e-7) p = 1e-7;\n    return -Math.log(p);\n}\nfunction CrossEntropyBackward(pred, label, dPred, N) {\n    dPred.fill(0);\n    let p = pred[label];\n    if (p < 1e-7) p = 1e-7;\n    dPred[label] = -1.0 / p;\n}\n\n// --- cross_entropy_seq ---\nfunction CrossEntropySeqForward(logits, targets, S, V) {\n    let loss = 0.0;\n    for (let i = 0; i < S; i++) {\n        const off = i * V;\n        const label = ((targets[i] | 0) % V + V) % V;\n        let mx = logits[off]; for (let j = 1; j < V; j++) if (logits[off+j] > mx) mx = logits[off+j];\n        let sum = 0.0;\n        for (let j = 0; j < V; j++) sum += Math.exp(logits[off+j] - mx);\n        let p = Math.exp(logits[off+label] - mx) / sum;\n        if (p < 1e-7) p = 1e-7;\n        loss -= Math.log(p);\n    }\n    return loss / S;\n}\nfunction CrossEntropySeqBackward(logits, targets, dLogits, S, V) {\n    for (let i = 0; i < S; i++) {\n        const off = i * V;\n        const label = ((targets[i] | 0) % V + V) % V;\n        let mx = logits[off]; for (let j = 1; j < V; j++) if (logits[off+j] > mx) mx = logits[off+j];\n        let sum = 0.0;\n        for (let j = 0; j < V; j++) sum += Math.exp(logits[off+j] - mx);\n        for (let j = 0; j < V; j++) {\n            const p = Math.exp(logits[off+j] - mx) / sum;\n            dLogits[off + j] = (p - (j === label ? 1.0 : 0.0)) / S;\n        }\n    }\n}\n\n// --- SGD ---\nfunction SgdStep(params, grads, n, lr) {\n    for (let i = 0; i < n; i++) params[i] -= lr * grads[i];\n}\n\n// --- Adam ---\nfunction AdamCreate(n) {\n    return { m: new Float32Array(n), v: new Float32Array(n), n: n, t: 0 };\n}\nfunction AdamStep(params, grads, state, lr, beta1, beta2, eps) {\n    state.t++;\n    const bc1 = 1.0 - Math.pow(beta1, state.t);\n    const bc2 = 1.0 - Math.pow(beta2, state.t);\n    for (let i = 0; i < state.n; i++) {\n        state.m[i] = beta1 * state.m[i] + (1.0 - beta1) * grads[i];\n        state.v[i] = beta2 * state.v[i] + (1.0 - beta2) * grads[i] * grads[i];\n        const mhat = state.m[i] / bc1;\n        const vhat = state.v[i] / bc2;\n        params[i] -= lr * mhat / (Math.sqrt(vhat) + eps);\n    }\n}\n\n// --- Argmax ---\nfunction Argmax(x, N) {\n    let best = 0;\n    for (let i = 1; i < N; i++) if (x[i] > x[best]) best = i;\n    return best;\n}\n\n// --- shuffle (Fisher-Yates) ---\nfunction ShuffleIndices(idx, n) {\n    for (let i = n - 1; i > 0; i--) {\n        const j = ((RngNext() >>> 0) % (i + 1)) | 0;\n        const tmp = idx[i]; idx[i] = idx[j]; idx[j] = tmp;\n    }\n}\n\n// --- TextData provider ---\nfunction TextLoad(path) {\n    const fs = require('fs');\n    const data = fs.readFileSync(path);\n    const seen = new Uint8Array(256);\n    for (let i = 0; i < data.length; i++) seen[data[i]] = 1;\n    const idxToChar = [];\n    const charToIdx = new Int32Array(256).fill(-1);\n    let vocabSize = 0;\n    for (let i = 0; i < 256; i++) {\n        if (seen[i]) { idxToChar[vocabSize] = i; charToIdx[i] = vocabSize++; }\n    }\n    return { data, len: data.length, vocabSize, idxToChar, charToIdx };\n}\nfunction TextRandomBatch(t, x, y, seqLen, batch) {\n    for (let b = 0; b < batch; b++) {\n        const start = ((RngNext() >>> 0) % (t.len - seqLen - 1)) | 0;\n        for (let i = 0; i < seqLen; i++) {\n            x[b * seqLen + i] = t.charToIdx[t.data[start + i]];\n            y[b * seqLen + i] = t.charToIdx[t.data[start + i + 1]];\n        }\n    }\n}\nfunction TextEncode(t, str, out) {\n    for (let i = 0; i < str.length; i++) out[i] = t.charToIdx[str.charCodeAt(i)];\n    return str.length;\n}\nfunction TextDecode(t, tokens, len) {\n    let s = '';\n    for (let i = 0; i < len; i++) s += String.fromCharCode(t.idxToChar[tokens[i] | 0]);\n    return s;\n}\n\n// --- MNIST IDX loader ---\nfunction MnistLoad(imgPath, lblPath) {\n    const fs = require('fs');\n    const imgBuf = fs.readFileSync(imgPath);\n    const lblBuf = fs.readFileSync(lblPath);\n    const n = (imgBuf[4] << 24) | (imgBuf[5] << 16) | (imgBuf[6] << 8) | imgBuf[7];\n    const rows = (imgBuf[8] << 24) | (imgBuf[9] << 16) | (imgBuf[10] << 8) | imgBuf[11];\n    const cols = (imgBuf[12] << 24) | (imgBuf[13] << 16) | (imgBuf[14] << 8) | imgBuf[15];\n    const dim = rows * cols;\n    const images = new Float32Array(n * dim);\n    for (let i = 0; i < n * dim; i++) images[i] = imgBuf[16 + i] / 255.0;\n    const labels = new Uint8Array(n);\n    for (let i = 0; i < n; i++) labels[i] = lblBuf[8 + i];\n    return { images, labels, n };\n}\n\n// --- Dual numbers (forward-mode AD) ---\nfunction DualAdd(a, b) { return { val: a.val+b.val, deriv: a.deriv+b.deriv }; }\nfunction DualSub(a, b) { return { val: a.val-b.val, deriv: a.deriv-b.deriv }; }\nfunction DualMul(a, b) { return { val: a.val*b.val, deriv: a.deriv*b.val+a.val*b.deriv }; }\nfunction DualDiv(a, b) {\n    return { val: a.val/b.val, deriv: (a.deriv*b.val - a.val*b.deriv)/(b.val*b.val) };\n}\nfunction DualConst(c) { return { val: c, deriv: 0.0 }; }\n\"\"\"\n\n\n# ---------------------------------------------------------------------------\n# JS Emitter\n# ---------------------------------------------------------------------------\n\ndef _param_field(cvar: str) -> str | None:\n    if cvar.startswith('me->'):\n        return cvar[4:]\n    return None\n\n\n_RUNTIME_NAMES = {\n    'LayerNormForward', 'LayerNormBackward',\n    'CausalMhaForward', 'CausalMhaBackward',\n    'EmbedForward', 'EmbedBackward',\n    'SigmoidForward', 'SigmoidBackward',\n    'ReluForward', 'ReluBackward',\n    'GeluForward', 'GeluBackward',\n    'SoftmaxForward', 'SoftmaxBackward',\n    'LogForward', 'LogBackward',\n    'NegateForward', 'NegateBackward',\n    'MseForward', 'MseBackward',\n    'CrossEntropyForward', 'CrossEntropyBackward',\n    'CrossEntropySeqForward', 'CrossEntropySeqBackward',\n}\n\n\nclass JSEmitter:\n    \"\"\"Emit JavaScript code from an IRModule.\"\"\"\n\n    def __init__(self, module: IRModule, lowerer=None, source_file: str = ''):\n        self.module = module\n        self.lowerer = lowerer\n        self.source_file = source_file\n        self.out: list[str] = []\n        self.main_declared: set[str] = set()\n        # Build reverse constant lookup: value \u2192 name\n        self.const_rev: dict[int, str] = {}\n        for name, val in module.constants.items():\n            if not name.startswith('NUM_PARAMS_'):\n                self.const_rev[val] = name\n\n    def _fn_name(self, tname: str, suffix: str) -> str:\n        name = f'{tname}{suffix}'\n        if name in _RUNTIME_NAMES:\n            return f'{tname}Apply{suffix}'\n        return name\n\n    def _cache_name(self, tname: str) -> str:\n        return self._fn_name(tname, 'Cache')\n\n    def _dim_name(self, val) -> str:\n        return self.const_rev.get(val, str(val))\n\n    def _size_expr(self, irtype: IRType) -> str:\n        if not irtype.dims:\n            return str(irtype.size)\n        parts = [self._dim_name(d) for d in irtype.dims]\n        return ' * '.join(parts)\n\n    def _dims_args(self, e: dict, keys: list[str]) -> str:\n        return ', '.join(self._dim_name(e[k]) for k in keys)\n\n    def _o(self, indent: int, line: str):\n        self.out.append('    ' * indent + line)\n\n    def _c_to_js_expr(self, expr: str) -> str:\n        \"\"\"Translate a C expression embedded in IR to JavaScript.\"\"\"\n        # Replace C-style casts\n        expr = re.sub(r'\\(float\\)\\s*', '', expr)\n        expr = re.sub(r'\\(int\\)\\s*', '(', expr)  # (int)x -> (x  \u2014 not perfect but rare\n        # Replace C float suffixes\n        expr = re.sub(r'(\\d+\\.\\d+)f\\b', r'\\1', expr)\n        expr = re.sub(r'(\\d+)\\.0f\\b', r'\\1.0', expr)\n        # Replace C math functions\n        expr = expr.replace('expf(', 'Math.exp(')\n        expr = expr.replace('sqrtf(', 'Math.sqrt(')\n        expr = expr.replace('logf(', 'Math.log(')\n        expr = expr.replace('tanhf(', 'Math.tanh(')\n        expr = expr.replace('cosf(', 'Math.cos(')\n        expr = expr.replace('fabsf(', 'Math.abs(')\n        expr = expr.replace('fmaxf(', 'Math.max(')\n        expr = expr.replace('powf(', 'Math.pow(')\n        # atof \u2192 parseFloat\n        expr = expr.replace('atof(', 'parseFloat(')\n        return expr\n\n    # ------- struct factories (replaces C typedefs) -------\n\n    def _compute_struct_layout(self, sdef) -> list[tuple[str, int, int, str | None]]:\n        \"\"\"Compute flat buffer layout for a struct.\n        Returns list of (field_name, offset, size, sub_struct_name_or_None).\n        \"\"\"\n        layout = []\n        offset = 0\n        for fn, ftype in sdef.fields:\n            if ftype.kind == 'struct':\n                sub_size = self.lowerer._total_params(ftype.struct_name)\n                layout.append((fn, offset, sub_size, ftype.struct_name))\n                offset += sub_size\n            else:\n                sz = ftype.size\n                layout.append((fn, offset, sz, None))\n                offset += sz\n        return layout\n\n    def _emit_struct_factory(self, sdef: IRStructDef):\n        \"\"\"Emit createType() factory function.\"\"\"\n        layout = self._compute_struct_layout(sdef)\n        total = sum(sz for _, _, sz, _ in layout)\n\n        self._o(0, f'function create{sdef.name}() {{')\n        self._o(1, f'const _buf = new Float32Array({total});')\n\n        has_sub = any(sub is not None for _, _, _, sub in layout)\n\n        parts = ['_buf']\n        for fn, off, sz, sub in layout:\n            if sub:\n                parts.append(f'{fn}: create{sub}View(_buf, {off})')\n            else:\n                parts.append(f'{fn}: _buf.subarray({off}, {off + sz})')\n        self._o(1, 'return { ' + ', '.join(parts) + ' };')\n        self._o(0, '}')\n        self._o(0, '')\n\n        # If struct has sub-struct fields, emit view creator for the sub-struct\n        for fn, off, sz, sub in layout:\n            if sub:\n                self._emit_nested_view(sub)\n\n    def _emit_nested_view(self, sub_type: str):\n        \"\"\"Emit createTypeView(buf, baseOff) for nested struct access.\"\"\"\n        view_fn = f'create{sub_type}View'\n        # Check if already emitted\n        view_sig = f'function {view_fn}('\n        for line in self.out:\n            if view_sig in line:\n                return\n\n        sub_sdef = None\n        for sd in self.module.struct_defs:\n            if sd.name == sub_type:\n                sub_sdef = sd\n                break\n        if not sub_sdef:\n            return\n\n        layout = self._compute_struct_layout(sub_sdef)\n\n        self._o(0, f'function {view_fn}(buf, baseOff) {{')\n        parts = ['_buf: buf']\n        for fn, off, sz, sub in layout:\n            if sub:\n                parts.append(f'{fn}: create{sub}View(buf, baseOff + {off})')\n            else:\n                parts.append(f'{fn}: buf.subarray(baseOff + {off}, baseOff + {off + sz})')\n        self._o(1, 'return { ' + ', '.join(parts) + ' };')\n        self._o(0, '}')\n        self._o(0, '')\n\n        # Recurse for deeper nesting\n        for fn, off, sz, sub in layout:\n            if sub:\n                self._emit_nested_view(sub)\n\n    def _emit_grad_factory(self, gdef: IRGradStructDef):\n        \"\"\"Emit createTypeGrad() factory with flat _buf + subarray views.\"\"\"\n        # Compute layout from grad fields\n        layout = []\n        offset = 0\n        for fn, ftype in gdef.fields:\n            if ftype.kind == 'struct':\n                sub_size = self.lowerer._total_params(ftype.struct_name.replace('Grad', ''))\n                layout.append((fn, offset, sub_size, ftype.struct_name))\n                offset += sub_size\n            else:\n                sz = ftype.size\n                layout.append((fn, offset, sz, None))\n                offset += sz\n        total = offset\n\n        self._o(0, f'function create{gdef.name}() {{')\n        self._o(1, f'const _buf = new Float32Array({total});')\n\n        parts = ['_buf']\n        for fn, off, sz, sub in layout:\n            if sub:\n                parts.append(f'{fn}: create{sub}View(_buf, {off})')\n            else:\n                parts.append(f'{fn}: _buf.subarray({off}, {off + sz})')\n        self._o(1, 'return { ' + ', '.join(parts) + ' };')\n        self._o(0, '}')\n        self._o(0, '')\n\n        # Emit view creators for nested grad structs\n        for fn, off, sz, sub in layout:\n            if sub:\n                self._emit_nested_grad_view(sub)\n\n    def _emit_nested_grad_view(self, grad_type: str):\n        \"\"\"Emit view creator for nested grad struct.\"\"\"\n        view_fn = f'create{grad_type}View'\n        view_sig = f'function {view_fn}('\n        for line in self.out:\n            if view_sig in line:\n                return\n\n        # Find the grad struct def\n        gdef = None\n        for gd in self.module.grad_struct_defs:\n            if gd.name == grad_type:\n                gdef = gd\n                break\n        if not gdef:\n            return\n\n        layout = []\n        offset = 0\n        for fn, ftype in gdef.fields:\n            if ftype.kind == 'struct':\n                sub_size = self.lowerer._total_params(ftype.struct_name.replace('Grad', ''))\n                layout.append((fn, offset, sub_size, ftype.struct_name))\n                offset += sub_size\n            else:\n                sz = ftype.size\n                layout.append((fn, offset, sz, None))\n                offset += sz\n\n        self._o(0, f'function {view_fn}(buf, baseOff) {{')\n        parts = ['_buf: buf']\n        for fn, off, sz, sub in layout:\n            if sub:\n                parts.append(f'{fn}: create{sub}View(buf, baseOff + {off})')\n            else:\n                parts.append(f'{fn}: buf.subarray(baseOff + {off}, baseOff + {off + sz})')\n        self._o(1, 'return { ' + ', '.join(parts) + ' };')\n        self._o(0, '}')\n        self._o(0, '')\n\n        for fn, off, sz, sub in layout:\n            if sub:\n                self._emit_nested_grad_view(sub)\n\n    # ------- cache factory -------\n\n    def _tape_size_expr(self, e: dict) -> str:\n        op = e['op']\n        if op in ('matmul_2d', 'matmul_2d_transB'):\n            return f'{self._dim_name(e[\"M\"])} * {self._dim_name(e[\"P\"])}'\n        if op == 'matmul':\n            return self._dim_name(e['M'])\n        if op in ('vec_add', 'vec_sub', 'broadcast_add'):\n            if 'M' in e and 'N' in e:\n                return f'{self._dim_name(e[\"M\"])} * {self._dim_name(e[\"N\"])}'\n            return self._dim_name(e['N'])\n        if op in ('dot_product', 'cosine_similarity'):\n            return '1'\n        if op in ('sigmoid', 'relu', 'softmax', 'gelu', 'log', 'negate'):\n            return self._dim_name(e['N'])\n        if op == 'index':\n            return '1'\n        if op in ('embed', 'layer_norm'):\n            return f'{self._dim_name(e[\"S\"])} * {self._dim_name(e[\"D\"])}'\n        if op == 'causal_mha':\n            return f'{self._dim_name(e[\"S\"])} * {self._dim_name(e[\"D\"])}'\n        if op == 'sub_apply':\n            return self._dim_name(e['n'])\n        return str(e['n'])\n\n    def _tape_size_val(self, e: dict) -> int:\n        \"\"\"Compute numeric size of tape entry.\"\"\"\n        op = e['op']\n        if op in ('matmul_2d', 'matmul_2d_transB'):\n            return e['M'] * e['P']\n        if op == 'matmul':\n            return e['M']\n        if op in ('vec_add', 'vec_sub', 'broadcast_add'):\n            if 'M' in e and 'N' in e:\n                return e['M'] * e['N']\n            return e['N']\n        if op in ('dot_product', 'cosine_similarity'):\n            return 1\n        if op in ('sigmoid', 'relu', 'softmax', 'gelu', 'log', 'negate'):\n            return e['N']\n        if op == 'index':\n            return 1\n        if op in ('embed', 'layer_norm'):\n            return e['S'] * e['D']\n        if op == 'causal_mha':\n            return e['S'] * e['D']\n        if op == 'sub_apply':\n            return e['n']\n        return e.get('n', 0)\n\n    def _emit_cache_factory(self, fn: IRFunction):\n        \"\"\"Emit createTypeCache() factory for caching forward pass intermediates.\"\"\"\n        tname = fn.type_name\n        tape = fn.body\n        cache_name = self._cache_name(tname)\n\n        self._o(0, f'function create{cache_name}() {{')\n        self._o(1, 'return {')\n        for e in tape:\n            sz = self._tape_size_val(e)\n            self._o(2, f'{e[\"out\"]}: new Float32Array({sz}),')\n            if e['op'] == 'layer_norm':\n                S = e['S']\n                self._o(2, f'{e[\"out\"]}_mean: new Float32Array({S}),')\n                self._o(2, f'{e[\"out\"]}_rstd: new Float32Array({S}),')\n            elif e['op'] == 'causal_mha':\n                H, S = e['H'], e['S']\n                self._o(2, f'{e[\"out\"]}_attn: new Float32Array({H * S * S}),')\n            elif e['op'] == 'sub_apply':\n                st = e['sub_type']\n                sf = e['sub_field']\n                self._o(2, f'_sc_{sf}: create{self._cache_name(st)}(),')\n        self._o(1, '};')\n        self._o(0, '}')\n        self._o(0, '')\n\n    # ------- forward function -------\n\n    def _emit_forward_fn(self, fn: IRFunction):\n        tname = fn.type_name\n        tape = fn.body\n        tape_outs = {e['out'] for e in tape}\n\n        def _ref(v: str) -> str:\n            if v in tape_outs:\n                return f'cache.{v}'\n            # me-> references become me.\n            if v.startswith('me->'):\n                return 'me.' + v[4:]\n            return v\n\n        # Build function signature\n        fundef = self.lowerer.fun_defs.get(f\"{tname}'{fn.method_name}\")\n        params_js = ['me']\n        if fundef:\n            for pname, ptype in fundef.params:\n                params_js.append(pname)\n        params_js.append('output')\n        params_js.append('cache')\n        sig = ', '.join(params_js)\n        self._o(0, f'function {self._fn_name(tname, \"Forward\")}({sig}) {{')\n\n        for e in tape:\n            o = e['out']\n            if e['op'] == 'matmul':\n                dims = self._dims_args(e, ['M', 'N'])\n                self._o(1, f'Matmul({_ref(e[\"W\"])}, {_ref(e[\"x\"])}, cache.{o}, {dims});')\n            elif e['op'] == 'matmul_2d':\n                dims = self._dims_args(e, ['M', 'K', 'P'])\n                self._o(1, f'Matmul2d({_ref(e[\"A\"])}, {_ref(e[\"B\"])}, cache.{o}, {dims});')\n            elif e['op'] == 'matmul_2d_transB':\n                dims = self._dims_args(e, ['M', 'K', 'P'])\n                self._o(1, f'Matmul2dTransB({_ref(e[\"A\"])}, {_ref(e[\"B\"])}, cache.{o}, {dims});')\n            elif e['op'] == 'vec_add':\n                self._o(1, f'VecAdd({_ref(e[\"a\"])}, {_ref(e[\"b\"])}, cache.{o}, {self._dim_name(e[\"N\"])});')\n            elif e['op'] == 'vec_sub':\n                self._o(1, f'VecSub({_ref(e[\"a\"])}, {_ref(e[\"b\"])}, cache.{o}, {self._dim_name(e[\"N\"])});')\n            elif e['op'] == 'dot_product':\n                self._o(1, f'cache.{o} = DotProduct({_ref(e[\"a\"])}, {_ref(e[\"b\"])}, {self._dim_name(e[\"N\"])});')\n            elif e['op'] == 'cosine_similarity':\n                self._o(1, f'cache.{o} = CosineSimilarity({_ref(e[\"a\"])}, {_ref(e[\"b\"])}, {self._dim_name(e[\"N\"])});')\n            elif e['op'] == 'broadcast_add':\n                dims = self._dims_args(e, ['M', 'N'])\n                self._o(1, f'BroadcastAdd({_ref(e[\"A\"])}, {_ref(e[\"b\"])}, cache.{o}, {dims});')\n            elif e['op'] in ('sigmoid', 'relu', 'softmax', 'gelu', 'log'):\n                cfn = e['op'].capitalize() + 'Forward'\n                self._o(1, f'{cfn}({_ref(e[\"x\"])}, cache.{o}, {self._dim_name(e[\"N\"])});')\n            elif e['op'] == 'negate':\n                self._o(1, f'NegateForward({_ref(e[\"x\"])}, cache.{o}, {self._dim_name(e[\"N\"])});')\n            elif e['op'] == 'index':\n                self._o(1, f'cache.{o}[0] = {_ref(e[\"x\"])}[{_ref(e[\"idx\"])} | 0];')\n            elif e['op'] == 'embed':\n                dims = self._dims_args(e, ['S', 'D'])\n                self._o(1, f'EmbedForward({_ref(e[\"table\"])}, {_ref(e[\"tokens\"])}, cache.{o}, {dims});')\n            elif e['op'] == 'layer_norm':\n                dims = self._dims_args(e, ['S', 'D'])\n                self._o(1, f'LayerNormForward({_ref(e[\"x\"])}, {_ref(e[\"gamma\"])}, {_ref(e[\"beta\"])}, cache.{o}, cache.{o}_mean, cache.{o}_rstd, {dims});')\n            elif e['op'] == 'causal_mha':\n                dims = self._dims_args(e, ['S', 'D', 'H'])\n                self._o(1, f'CausalMhaForward({_ref(e[\"Q\"])}, {_ref(e[\"K\"])}, {_ref(e[\"V\"])}, cache.{o}, cache.{o}_attn, {dims});')\n            elif e['op'] == 'sub_apply':\n                sf = e['sub_field']\n                st = e['sub_type']\n                self._o(1, f'{self._fn_name(st, \"Forward\")}(me.{sf}, {_ref(e[\"input\"])}, cache.{o}, cache._sc_{sf});')\n\n        last = tape[-1]\n        self._o(1, f'output.set(cache.{last[\"out\"]});')\n        self._o(0, '}')\n        self._o(0, '')\n\n    # ------- backward function -------\n\n    def _count_multi_use_inputs(self, tape: list[dict]) -> set[str]:\n        counts: dict[str, int] = {}\n        for e in tape:\n            inputs: set[str] = set()\n            op = e['op']\n            if op == 'vec_add':\n                for k in ('a', 'b'):\n                    if not _param_field(e[k]): inputs.add(e[k])\n            elif op in ('matmul_2d', 'matmul_2d_transB'):\n                if not _param_field(e['A']): inputs.add(e['A'])\n            elif op == 'sub_apply':\n                if not _param_field(e['input']): inputs.add(e['input'])\n            elif op == 'broadcast_add':\n                if not _param_field(e['A']): inputs.add(e['A'])\n            elif op == 'layer_norm':\n                if not _param_field(e['x']): inputs.add(e['x'])\n            elif op in ('gelu', 'sigmoid', 'relu', 'log', 'negate'):\n                if not _param_field(e['x']): inputs.add(e['x'])\n            elif op == 'index':\n                if not _param_field(e['x']): inputs.add(e['x'])\n            elif op == 'causal_mha':\n                for k in ('Q', 'K', 'V'):\n                    if not _param_field(e[k]): inputs.add(e[k])\n            elif op == 'matmul':\n                if not _param_field(e['x']): inputs.add(e['x'])\n            elif op == 'embed':\n                if not _param_field(e['table']): inputs.add(e['table'])\n            for v in inputs:\n                counts[v] = counts.get(v, 0) + 1\n        return {v for v, c in counts.items() if c > 1}\n\n    def _emit_backward_fn(self, fn: IRFunction):\n        tname = fn.type_name\n        tape = fn.body\n\n        fundef = self.lowerer.fun_defs.get(f\"{tname}'{fn.method_name}\")\n        param_names = {p[0] for p in fundef.params} if fundef else set()\n        params_js = ['me']\n        if fundef:\n            for pname, ptype in fundef.params:\n                params_js.append(pname)\n        params_js.extend(['c', 'd_output', 'g', 'd_input'])\n        sig = ', '.join(params_js)\n        self._o(0, f'function {self._fn_name(tname, \"Backward\")}({sig}) {{')\n\n        multi_use = self._count_multi_use_inputs(tape)\n\n        # Declare d_input for first parameter\n        if fundef and fundef.params:\n            from .typechecker import DxType as DT\n            fp = fundef.params[0]\n            fp_dt = DT(fp[1].name, list(fp[1].dims)) if fp[1] else DT('?')\n            fp_n = fp_dt.dims[0] if fp_dt.name == 'Vec' and fp_dt.dims else (fp_dt.dims[0] * fp_dt.dims[1] if fp_dt.name == 'Tensor' and len(fp_dt.dims) == 2 else 1)\n            self._o(1, f'const d_{fp[0]} = new Float32Array({fp_n});')\n\n        last = tape[-1]\n        self._o(1, f'const d_{last[\"out\"]} = new Float32Array({last[\"n\"]});')\n        self._o(1, f'd_{last[\"out\"]}.set(d_output);')\n\n        declared_grads = set()\n        if fundef and fundef.params:\n            declared_grads.add(f'd_{fundef.params[0][0]}')\n        declared_grads.add(f'd_{last[\"out\"]}')\n        tape_outs = {e['out'] for e in tape}\n\n        def _declare_grad(var_name: str, size: int):\n            gname = f'd_{var_name}'\n            if gname not in declared_grads:\n                declared_grads.add(gname)\n                self._o(1, f'const {gname} = new Float32Array({size});')\n\n        def _c_ref(v: str) -> str:\n            \"\"\"Cache reference for backward pass.\"\"\"\n            if v in tape_outs:\n                return f'c.{v}'\n            if v.startswith('me->'):\n                return 'me.' + v[4:]\n            return v\n\n        for i in range(len(tape) - 1, -1, -1):\n            e = tape[i]\n            d_out = f'd_{e[\"out\"]}'\n\n            if e['op'] == 'sigmoid':\n                inp = e['x']; n = e['N']\n                if not _param_field(inp): _declare_grad(inp, n)\n                if inp in multi_use:\n                    self._o(1, f'{{ const _tmp = new Float32Array({n}); SigmoidBackward(c.{e[\"out\"]}, {d_out}, _tmp, {n});')\n                    self._o(1, f'for (let _i=0; _i<{n}; _i++) d_{inp}[_i] += _tmp[_i]; }}')\n                else:\n                    self._o(1, f'SigmoidBackward(c.{e[\"out\"]}, {d_out}, d_{inp}, {n});')\n\n            elif e['op'] == 'relu':\n                inp = e['x']; n = e['N']\n                if not _param_field(inp): _declare_grad(inp, n)\n                if inp in multi_use:\n                    self._o(1, f'{{ const _tmp = new Float32Array({n}); ReluBackward(c.{inp}, {d_out}, _tmp, {n});')\n                    self._o(1, f'for (let _i=0; _i<{n}; _i++) d_{inp}[_i] += _tmp[_i]; }}')\n                else:\n                    self._o(1, f'ReluBackward(c.{inp}, {d_out}, d_{inp}, {n});')\n\n            elif e['op'] == 'softmax':\n                inp = e['x']; n = e['N']\n                if not _param_field(inp): _declare_grad(inp, n)\n                self._o(1, f'SoftmaxBackward(c.{e[\"out\"]}, {d_out}, d_{inp}, {n});')\n\n            elif e['op'] == 'log':\n                inp = e['x']; n = e['N']\n                inp_ref = f'c.{inp}' if inp in tape_outs else _c_ref(inp)\n                if not _param_field(inp): _declare_grad(inp, n)\n                if inp in multi_use:\n                    self._o(1, f'{{ const _tmp = new Float32Array({n}); LogBackward({inp_ref}, {d_out}, _tmp, {n});')\n                    self._o(1, f'for (let _i=0; _i<{n}; _i++) d_{inp}[_i] += _tmp[_i]; }}')\n                else:\n                    self._o(1, f'LogBackward({inp_ref}, {d_out}, d_{inp}, {n});')\n\n            elif e['op'] == 'negate':\n                inp = e['x']; n = e['N']\n                if not _param_field(inp): _declare_grad(inp, n)\n                if inp in multi_use:\n                    self._o(1, f'for (let _i=0; _i<{n}; _i++) d_{inp}[_i] += (-{d_out}[_i]);')\n                else:\n                    self._o(1, f'NegateBackward({d_out}, d_{inp}, {n});')\n\n            elif e['op'] == 'index':\n                inp = e['x']\n                idx_var = e['idx']\n                N = e['N']\n                if not _param_field(inp):\n                    _declare_grad(inp, N)\n                    self._o(1, f'd_{inp}.fill(0);')\n                    self._o(1, f'd_{inp}[{_c_ref(idx_var)} | 0] += {d_out}[0];')\n\n            elif e['op'] == 'vec_add':\n                a, b, n = e['a'], e['b'], e['N']\n                pf_a = _param_field(a); pf_b = _param_field(b)\n                if pf_a:\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) g.d_{pf_a}[_i] += {d_out}[_i];')\n                else:\n                    _declare_grad(a, n)\n                    if a in multi_use:\n                        self._o(1, f'for (let _i = 0; _i < {n}; _i++) d_{a}[_i] += {d_out}[_i];')\n                    else:\n                        self._o(1, f'd_{a}.set({d_out});')\n                if pf_b:\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) g.d_{pf_b}[_i] += {d_out}[_i];')\n                else:\n                    _declare_grad(b, n)\n                    if b in multi_use:\n                        self._o(1, f'for (let _i = 0; _i < {n}; _i++) d_{b}[_i] += {d_out}[_i];')\n                    else:\n                        self._o(1, f'd_{b}.set({d_out});')\n\n            elif e['op'] == 'vec_sub':\n                a, b, n = e['a'], e['b'], e['N']\n                pf_a = _param_field(a); pf_b = _param_field(b)\n                if pf_a:\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) g.d_{pf_a}[_i] += {d_out}[_i];')\n                else:\n                    _declare_grad(a, n)\n                    if a in multi_use:\n                        self._o(1, f'for (let _i = 0; _i < {n}; _i++) d_{a}[_i] += {d_out}[_i];')\n                    else:\n                        self._o(1, f'd_{a}.set({d_out});')\n                if pf_b:\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) g.d_{pf_b}[_i] -= {d_out}[_i];')\n                else:\n                    _declare_grad(b, n)\n                    if b in multi_use:\n                        self._o(1, f'for (let _i = 0; _i < {n}; _i++) d_{b}[_i] -= {d_out}[_i];')\n                    else:\n                        self._o(1, f'for (let _i = 0; _i < {n}; _i++) d_{b}[_i] = -{d_out}[_i];')\n\n            elif e['op'] == 'dot_product':\n                a, b, n = e['a'], e['b'], e['N']\n                a_ref = f'c.{a}' if a in tape_outs else _c_ref(a)\n                b_ref = f'c.{b}' if b in tape_outs else _c_ref(b)\n                pf_a = _param_field(a); pf_b = _param_field(b)\n                if pf_a:\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) g.d_{pf_a}[_i] += {d_out} * {b_ref}[_i];')\n                else:\n                    _declare_grad(a, n)\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) d_{a}[_i] += {d_out} * {b_ref}[_i];')\n                if pf_b:\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) g.d_{pf_b}[_i] += {d_out} * {a_ref}[_i];')\n                else:\n                    _declare_grad(b, n)\n                    self._o(1, f'for (let _i = 0; _i < {n}; _i++) d_{b}[_i] += {d_out} * {a_ref}[_i];')\n\n            elif e['op'] == 'cosine_similarity':\n                a, b, n = e['a'], e['b'], e['N']\n                a_ref = f'c.{a}' if a in tape_outs else _c_ref(a)\n                b_ref = f'c.{b}' if b in tape_outs else _c_ref(b)\n                pf_a = _param_field(a); pf_b = _param_field(b)\n                # d/da_i = (b_i/(||a||*||b||)) - (a.b/(||a||^3*||b||))*a_i\n                # = (1/(||a||*||b||)) * (b_i - cos*a_i)  where cos = c.{o}\n                self._o(1, '{')\n                self._o(2, f'let _na2=0, _nb2=0; for(let _i=0;_i<{n};_i++){{ _na2+={a_ref}[_i]*{a_ref}[_i]; _nb2+={b_ref}[_i]*{b_ref}[_i]; }}')\n                self._o(2, f'let _na=Math.sqrt(_na2), _nb=Math.sqrt(_nb2), _cos=c.{o};')\n                self._o(2, f'let _inv = (_na>0&&_nb>0) ? {d_out}/(_na*_nb) : 0;')\n                if pf_a:\n                    self._o(2, f'for(let _i=0;_i<{n};_i++) g.d_{pf_a}[_i] += _inv*({b_ref}[_i] - _cos*{a_ref}[_i]*_nb/_na);')\n                else:\n                    _declare_grad(a, n)\n                    self._o(2, f'for(let _i=0;_i<{n};_i++) d_{a}[_i] += _inv*({b_ref}[_i] - _cos*{a_ref}[_i]*_nb/_na);')\n                if pf_b:\n                    self._o(2, f'for(let _i=0;_i<{n};_i++) g.d_{pf_b}[_i] += _inv*({a_ref}[_i] - _cos*{b_ref}[_i]*_na/_nb);')\n                else:\n                    _declare_grad(b, n)\n                    self._o(2, f'for(let _i=0;_i<{n};_i++) d_{b}[_i] += _inv*({a_ref}[_i] - _cos*{b_ref}[_i]*_na/_nb);')\n                self._o(1, '}')\n\n            elif e['op'] == 'matmul':\n                W, x, M, N = e['W'], e['x'], e['M'], e['N']\n                pf_W = _param_field(W); pf_x = _param_field(x)\n                if pf_W:\n                    x_ref = f'c.{x}' if x in tape_outs else x\n                    self._o(1, f'OuterProductAdd({d_out}, {x_ref}, g.d_{pf_W}, {M}, {N});')\n                if not pf_x:\n                    _declare_grad(x, N)\n                    self._o(1, f'MatmulTranspose({_c_ref(W)}, {d_out}, d_{x}, {M}, {N});')\n\n            elif e['op'] == 'matmul_2d':\n                A, B, M, K, P = e['A'], e['B'], e['M'], e['K'], e['P']\n                pf_A = _param_field(A); pf_B = _param_field(B)\n                A_ref = f'c.{A}' if A in tape_outs else _c_ref(A)\n                B_ref = f'c.{B}' if B in tape_outs else _c_ref(B)\n                if pf_A:\n                    self._o(1, f'{{ const _tmp = new Float32Array({M*K});')\n                    self._o(1, f'Matmul2dTransB({d_out}, {B_ref}, _tmp, {M}, {P}, {K});')\n                    self._o(1, f'for (let _i=0; _i<{M*K}; _i++) g.d_{pf_A}[_i] += _tmp[_i]; }}')\n                elif A in multi_use:\n                    _declare_grad(A, M * K)\n                    self._o(1, f'{{ const _tmp = new Float32Array({M*K});')\n                    self._o(1, f'Matmul2dTransB({d_out}, {B_ref}, _tmp, {M}, {P}, {K});')\n                    self._o(1, f'for (let _i=0; _i<{M*K}; _i++) d_{A}[_i] += _tmp[_i]; }}')\n                else:\n                    _declare_grad(A, M * K)\n                    self._o(1, f'Matmul2dTransB({d_out}, {B_ref}, d_{A}, {M}, {P}, {K});')\n                if pf_B:\n                    self._o(1, f'{{ const _tmp = new Float32Array({K*P});')\n                    self._o(1, f'Matmul2dTransA({A_ref}, {d_out}, _tmp, {M}, {K}, {P});')\n                    self._o(1, f'for (let _i=0; _i<{K*P}; _i++) g.d_{pf_B}[_i] += _tmp[_i]; }}')\n                elif B in multi_use:\n                    _declare_grad(B, K * P)\n                    self._o(1, f'{{ const _tmp = new Float32Array({K*P});')\n                    self._o(1, f'Matmul2dTransA({A_ref}, {d_out}, _tmp, {M}, {K}, {P});')\n                    self._o(1, f'for (let _i=0; _i<{K*P}; _i++) d_{B}[_i] += _tmp[_i]; }}')\n                else:\n                    _declare_grad(B, K * P)\n                    self._o(1, f'Matmul2dTransA({A_ref}, {d_out}, d_{B}, {M}, {K}, {P});')\n\n            elif e['op'] == 'matmul_2d_transB':\n                A, B, M, K, P = e['A'], e['B'], e['M'], e['K'], e['P']\n                pf_A = _param_field(A); pf_B = _param_field(B)\n                A_ref = f'c.{A}' if A in tape_outs else _c_ref(A)\n                B_ref = f'c.{B}' if B in tape_outs else _c_ref(B)\n                if pf_A:\n                    self._o(1, f'{{ const _tmp = new Float32Array({M*K});')\n                    self._o(1, f'Matmul2d({d_out}, {B_ref}, _tmp, {M}, {P}, {K});')\n                    self._o(1, f'for (let _i=0; _i<{M*K}; _i++) g.d_{pf_A}[_i] += _tmp[_i]; }}')\n                elif A in multi_use:\n                    _declare_grad(A, M * K)\n                    self._o(1, f'{{ const _tmp = new Float32Array({M*K});')\n                    self._o(1, f'Matmul2d({d_out}, {B_ref}, _tmp, {M}, {P}, {K});')\n                    self._o(1, f'for (let _i=0; _i<{M*K}; _i++) d_{A}[_i] += _tmp[_i]; }}')\n                else:\n                    _declare_grad(A, M * K)\n                    self._o(1, f'Matmul2d({d_out}, {B_ref}, d_{A}, {M}, {P}, {K});')\n                if pf_B:\n                    self._o(1, f'{{ const _tmp = new Float32Array({P*K});')\n                    self._o(1, f'Matmul2dTransA({d_out}, {A_ref}, _tmp, {M}, {P}, {K});')\n                    self._o(1, f'for (let _i=0; _i<{P*K}; _i++) g.d_{pf_B}[_i] += _tmp[_i]; }}')\n                elif B in multi_use:\n                    _declare_grad(B, P * K)\n                    self._o(1, f'{{ const _tmp = new Float32Array({P*K});')\n                    self._o(1, f'Matmul2dTransA({d_out}, {A_ref}, _tmp, {M}, {P}, {K});')\n                    self._o(1, f'for (let _i=0; _i<{P*K}; _i++) d_{B}[_i] += _tmp[_i]; }}')\n                else:\n                    _declare_grad(B, P * K)\n                    self._o(1, f'Matmul2dTransA({d_out}, {A_ref}, d_{B}, {M}, {P}, {K});')\n\n            elif e['op'] == 'broadcast_add':\n                A, b, M, N = e['A'], e['b'], e['M'], e['N']\n                pf_A = _param_field(A); pf_b = _param_field(b)\n                if pf_A:\n                    self._o(1, f'for (let _i=0; _i<{M*N}; _i++) g.d_{pf_A}[_i] += {d_out}[_i];')\n                else:\n                    _declare_grad(A, M * N)\n                    if A in multi_use:\n                        self._o(1, f'for (let _i=0; _i<{M*N}; _i++) d_{A}[_i] += {d_out}[_i];')\n                    else:\n                        self._o(1, f'd_{A}.set({d_out});')\n                if pf_b:\n                    self._o(1, f'for (let _j=0; _j<{N}; _j++)')\n                    self._o(2, f'for (let _i=0; _i<{M}; _i++) g.d_{pf_b}[_j] += {d_out}[_i*{N}+_j];')\n                else:\n                    _declare_grad(b, N)\n                    self._o(1, f'd_{b}.fill(0);')\n                    self._o(1, f'for (let _j=0; _j<{N}; _j++)')\n                    self._o(2, f'for (let _i=0; _i<{M}; _i++) d_{b}[_j] += {d_out}[_i*{N}+_j];')\n\n            elif e['op'] == 'gelu':\n                inp = e['x']; n = e['N']\n                inp_ref = f'c.{inp}' if inp in tape_outs else _c_ref(inp)\n                if not _param_field(inp): _declare_grad(inp, n)\n                if inp in multi_use:\n                    self._o(1, f'{{ const _tmp = new Float32Array({n}); GeluBackward({inp_ref}, {d_out}, _tmp, {n});')\n                    self._o(1, f'for (let _i=0; _i<{n}; _i++) d_{inp}[_i] += _tmp[_i]; }}')\n                else:\n                    self._o(1, f'GeluBackward({inp_ref}, {d_out}, d_{inp}, {n});')\n\n            elif e['op'] == 'embed':\n                table = e['table']; tokens = e['tokens']\n                S, D = e['S'], e['D']\n                pf_table = _param_field(table)\n                tok_ref = f'c.{tokens}' if tokens in tape_outs else tokens\n                if pf_table:\n                    self._o(1, f'EmbedBackward({tok_ref}, {d_out}, g.d_{pf_table}, {S}, {D});')\n\n            elif e['op'] == 'layer_norm':\n                x_var, gamma_var, beta_var = e['x'], e['gamma'], e['beta']\n                S, D = e['S'], e['D']\n                x_ref = f'c.{x_var}' if x_var in tape_outs else _c_ref(x_var)\n                pf_gamma = _param_field(gamma_var); pf_beta = _param_field(beta_var)\n                _declare_grad(x_var, S * D)\n                gamma_target = f'g.d_{pf_gamma}' if pf_gamma else f'd_{gamma_var}'\n                beta_target = f'g.d_{pf_beta}' if pf_beta else f'd_{beta_var}'\n                gamma_ref = _c_ref(gamma_var)\n                if x_var in multi_use:\n                    self._o(1, f'{{ const _tmp_dx = new Float32Array({S*D});')\n                    self._o(1, f'LayerNormBackward({x_ref}, {gamma_ref}, c.{e[\"out\"]}_mean, c.{e[\"out\"]}_rstd, {d_out}, _tmp_dx, {gamma_target}, {beta_target}, {S}, {D});')\n                    self._o(1, f'for (let _i=0; _i<{S*D}; _i++) d_{x_var}[_i] += _tmp_dx[_i]; }}')\n                else:\n                    self._o(1, f'LayerNormBackward({x_ref}, {gamma_ref}, c.{e[\"out\"]}_mean, c.{e[\"out\"]}_rstd, {d_out}, d_{x_var}, {gamma_target}, {beta_target}, {S}, {D});')\n\n            elif e['op'] == 'causal_mha':\n                Q, K_v, V_v = e['Q'], e['K'], e['V']\n                S, D, H = e['S'], e['D'], e['H']\n                Q_ref = f'c.{Q}' if Q in tape_outs else _c_ref(Q)\n                K_ref = f'c.{K_v}' if K_v in tape_outs else _c_ref(K_v)\n                V_ref = f'c.{V_v}' if V_v in tape_outs else _c_ref(V_v)\n                for var in [Q, K_v, V_v]:\n                    if not _param_field(var):\n                        _declare_grad(var, S * D)\n                        self._o(1, f'd_{var}.fill(0);')\n                self._o(1, f'CausalMhaBackward({Q_ref}, {K_ref}, {V_ref}, c.{e[\"out\"]}_attn, {d_out}, d_{Q}, d_{K_v}, d_{V_v}, {S}, {D}, {H});')\n\n            elif e['op'] == 'sub_apply':\n                sf = e['sub_field']; st = e['sub_type']\n                inp = e['input']; inp_n = e['input_n']\n                inp_ref = f'c.{inp}' if inp in tape_outs else _c_ref(inp)\n                _declare_grad(inp, inp_n)\n                if inp in multi_use:\n                    self._o(1, f'{{ const _tmp_di = new Float32Array({inp_n});')\n                    self._o(1, f'{self._fn_name(st, \"Backward\")}(me.{sf}, {inp_ref}, c._sc_{sf}, {d_out}, g.d_{sf}, _tmp_di);')\n                    self._o(1, f'for (let _i=0; _i<{inp_n}; _i++) d_{inp}[_i] += _tmp_di[_i]; }}')\n                else:\n                    self._o(1, f'{self._fn_name(st, \"Backward\")}(me.{sf}, {inp_ref}, c._sc_{sf}, {d_out}, g.d_{sf}, d_{inp});')\n\n        # Copy first param gradient to d_input\n        if fundef and fundef.params:\n            from .typechecker import DxType as DT\n            first_dt = DT(fundef.params[0][1].name, list(fundef.params[0][1].dims)) if fundef.params[0][1] else DT('?')\n            first_n = first_dt.dims[0] if first_dt.name == 'Vec' and first_dt.dims else (first_dt.dims[0] * first_dt.dims[1] if first_dt.name == 'Tensor' and len(first_dt.dims) == 2 else 1)\n            self._o(1, f'if (d_input) d_input.set(d_{fundef.params[0][0]});')\n\n        self._o(0, '}')\n        self._o(0, '')\n\n    # ------- init function -------\n\n    def _emit_init_fn(self, type_name: str):\n        n = self.lowerer._total_params(type_name)\n        init_name = self._fn_name(type_name, 'Init')\n        self._o(0, f'function {init_name}(obj, stddev) {{')\n        self._o(1, f'const data = obj._buf;')\n        self._o(1, f'for (let i = 0; i < {n}; i++) data[i] = RngNormal() * stddev;')\n        self._o(0, '}')\n        self._o(0, '')\n\n    # ------- scalar functions -------\n\n    def _scalar_expr(self, node) -> str:\n        from . import ast_nodes as ast\n        if isinstance(node, ast.Num):\n            return str(int(node.value)) if node.is_int else str(node.value)\n        if isinstance(node, ast.Ident):\n            return node.name\n        if isinstance(node, ast.BinOp):\n            l = self._scalar_expr(node.left)\n            r = self._scalar_expr(node.right)\n            return f'({l} {node.op} {r})'\n        return '0'\n\n    def _dual_expr(self, node) -> str:\n        from . import ast_nodes as ast\n        if isinstance(node, ast.Num):\n            return f'DualConst({int(node.value)}.0)' if node.is_int else f'DualConst({node.value})'\n        if isinstance(node, ast.Ident):\n            return node.name\n        if isinstance(node, ast.BinOp):\n            l = self._dual_expr(node.left)\n            r = self._dual_expr(node.right)\n            op_map = {'+': 'DualAdd', '-': 'DualSub', '*': 'DualMul', '/': 'DualDiv'}\n            fn = op_map.get(node.op)\n            if fn:\n                return f'{fn}({l}, {r})'\n            return f'({l} {node.op} {r})'\n        return 'DualConst(0.0)'\n\n    def _emit_scalar_fn(self, fn: IRFunction):\n        from . import ast_nodes as ast\n        fname = fn.name\n        params_js = [p[0] for p in fn.params]\n        sig = ', '.join(params_js)\n        self._o(0, f'function {fname}({sig}) {{')\n\n        declared = set(p[0] for p in fn.params)\n        body = fn.body\n        for i, stmt in enumerate(body):\n            is_last = (i == len(body) - 1)\n            if isinstance(stmt, ast.Assignment) and isinstance(stmt.target, ast.Ident):\n                vname = stmt.target.name\n                expr_js = self._scalar_expr(stmt.value)\n                if vname not in declared:\n                    self._o(1, f'let {vname} = {expr_js};')\n                    declared.add(vname)\n                else:\n                    self._o(1, f'{vname} = {expr_js};')\n            elif isinstance(stmt, ast.ForLoop):\n                var = stmt.var.name if isinstance(stmt.var, ast.Ident) else '_i'\n                start = int(stmt.iterable.start.value) if isinstance(stmt.iterable, ast.Range) and isinstance(stmt.iterable.start, ast.Num) else 0\n                end = int(stmt.iterable.end.value) if isinstance(stmt.iterable, ast.Range) and isinstance(stmt.iterable.end, ast.Num) else 0\n                self._o(1, f'for (let {var} = {start}; {var} < {end}; {var}++) {{')\n                for s in stmt.body:\n                    if isinstance(s, ast.Assignment) and isinstance(s.target, ast.Ident):\n                        vn = s.target.name\n                        ec = self._scalar_expr(s.value)\n                        if vn not in declared:\n                            self._o(2, f'let {vn} = {ec};')\n                            declared.add(vn)\n                        else:\n                            self._o(2, f'{vn} = {ec};')\n                self._o(1, '}')\n            elif isinstance(stmt, ast.ExprStmt) and is_last:\n                expr_js = self._scalar_expr(stmt.expr)\n                self._o(1, f'return {expr_js};')\n\n        self._o(0, '}')\n        self._o(0, '')\n\n    def _emit_dual_fn(self, fn: IRFunction):\n        from . import ast_nodes as ast\n        fname = fn.name\n        params_js = [p[0] for p in fn.params]\n        sig = ', '.join(params_js)\n        self._o(0, f'function {fname}_dual({sig}) {{')\n\n        declared = set(p[0] for p in fn.params)\n        body = fn.body\n        for i, stmt in enumerate(body):\n            is_last = (i == len(body) - 1)\n            if isinstance(stmt, ast.Assignment) and isinstance(stmt.target, ast.Ident):\n                vname = stmt.target.name\n                expr_js = self._dual_expr(stmt.value)\n                if vname not in declared:\n                    self._o(1, f'let {vname} = {expr_js};')\n                    declared.add(vname)\n                else:\n                    self._o(1, f'{vname} = {expr_js};')\n            elif isinstance(stmt, ast.ForLoop):\n                var = stmt.var.name if isinstance(stmt.var, ast.Ident) else '_i'\n                start = int(stmt.iterable.start.value) if isinstance(stmt.iterable, ast.Range) and isinstance(stmt.iterable.start, ast.Num) else 0\n                end = int(stmt.iterable.end.value) if isinstance(stmt.iterable, ast.Range) and isinstance(stmt.iterable.end, ast.Num) else 0\n                self._o(1, f'for (let {var} = {start}; {var} < {end}; {var}++) {{')\n                for s in stmt.body:\n                    if isinstance(s, ast.Assignment) and isinstance(s.target, ast.Ident):\n                        vn = s.target.name\n                        ec = self._dual_expr(s.value)\n                        if vn not in declared:\n                            self._o(2, f'let {vn} = {ec};')\n                            declared.add(vn)\n                        else:\n                            self._o(2, f'{vn} = {ec};')\n                self._o(1, '}')\n            elif isinstance(stmt, ast.ExprStmt) and is_last:\n                expr_js = self._dual_expr(stmt.expr)\n                self._o(1, f'return {expr_js};')\n\n        self._o(0, '}')\n        self._o(0, '')\n\n    # ------- main body emission -------\n\n    def _collect_decls(self, items: list):\n        \"\"\"Pre-scan all items (recursively) to collect variable declarations at function scope.\"\"\"\n        for item in items:\n            if isinstance(item, IRVecLiteral):\n                if item.var not in self.main_declared:\n                    elems = ', '.join(str(v) for v in item.values)\n                    self.main_decls.append(f'    const {item.var} = new Float32Array([{elems}]);')\n                    self.main_declared.add(item.var)\n            elif isinstance(item, IRStringArray):\n                if item.var not in self.main_declared:\n                    elems = ', '.join(f'\"{v}\"' for v in item.values)\n                    self.main_decls.append(f'    const {item.var} = [{elems}];')\n                    self.main_declared.add(item.var)\n            elif isinstance(item, IRSetDecl):\n                if item.var not in self.main_declared:\n                    ptrs = ', '.join(item.elements)\n                    names = ', '.join(f'\"{e}\"' for e in item.elements)\n                    self.main_decls.append(f'    const _{item.var}_data = [{ptrs}];')\n                    self.main_decls.append(f'    const _{item.var}_names = [{names}];')\n                    self.main_declared.add(item.var)\n            elif isinstance(item, IRModelInit):\n                if item.var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.var} = create{item.type_name}();')\n                    self.main_declared.add(item.var)\n            elif isinstance(item, IRTextLoad):\n                if item.var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.var} = TextLoad(\"{item.path}\");')\n                    self.main_declared.add(item.var)\n            elif isinstance(item, IRTextEncode):\n                if item.var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.var} = new Float32Array({item.max_len});')\n                    self.main_declared.add(item.var)\n                if item.len_var not in self.main_declared:\n                    self.main_decls.append(f'    let {item.len_var} = 0;')\n                    self.main_declared.add(item.len_var)\n            elif isinstance(item, IRTextRandomBatch):\n                total = item.seq_len * item.batch_size\n                if item.x_var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.x_var} = new Float32Array({total});')\n                    self.main_decls.append(f'    const {item.y_var} = new Float32Array({total});')\n                    self.main_declared.update([item.x_var, item.y_var])\n            elif isinstance(item, IRModelCall):\n                n = item.out_n\n                if item.out not in self.main_declared:\n                    self.main_decls.append(f'    const {item.out} = new Float32Array({n});')\n                    self.main_declared.add(item.out)\n                cache_name = self._cache_name(item.model_type)\n                if item.cache_var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.cache_var} = create{cache_name}();')\n                    self.main_declared.add(item.cache_var)\n            elif isinstance(item, IRLoss):\n                if item.out not in self.main_declared:\n                    self.main_decls.append(f'    let {item.out} = 0.0;')\n                    self.main_declared.add(item.out)\n            elif isinstance(item, IRBackwardStep):\n                d_pred_var = f'd_{item.pred_var}'\n                pred_n = item.loss_info.get('n', 1)\n                if d_pred_var not in self.main_declared:\n                    self.main_decls.append(f'    const {d_pred_var} = new Float32Array({pred_n});')\n                    self.main_declared.add(d_pred_var)\n                if item.grad_var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.grad_var} = create{item.model_type}Grad();')\n                    self.main_declared.add(item.grad_var)\n                # Check for batched backward \u2014 may need bg_var\n                input_info = self.lowerer.chain.get(item.pred_info.get('input', ''), {})\n                batch_size = input_info.get('batch_size', 1) if input_info.get('kind') == 'text_batch' else 1\n                if batch_size > 1:\n                    bg_var = f'_bg_{item.model_var}'\n                    if bg_var not in self.main_declared:\n                        self.main_decls.append(f'    const {bg_var} = create{item.model_type}Grad();')\n                        self.main_declared.add(bg_var)\n                if item.opt_kind == 'adam':\n                    state_var = f'adam_state_{item.model_var}'\n                    if state_var not in self.main_declared:\n                        self.main_decls.append(f'    const {state_var} = AdamCreate({item.n_params});')\n                        self.main_declared.add(state_var)\n            elif isinstance(item, IRGradCompute):\n                d_pred_var = f'd_{item.pred_var}'\n                pred_n = item.pred_info.get('n', item.loss_info.get('n', 1))\n                if d_pred_var not in self.main_declared:\n                    self.main_decls.append(f'    const {d_pred_var} = new Float32Array({pred_n});')\n                    self.main_declared.add(d_pred_var)\n                if item.grad_var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.grad_var} = create{item.model_type}Grad();')\n                    self.main_declared.add(item.grad_var)\n            elif isinstance(item, IRParamUpdate):\n                pass  # no new declarations needed\n            elif isinstance(item, IRAssign):\n                if item.var not in self.main_declared:\n                    self.main_decls.append(f'    let {item.var} = 0;')\n                    self.main_declared.add(item.var)\n            elif isinstance(item, IRDerivExpr):\n                if item.out_var not in self.main_declared:\n                    self.main_decls.append(f'    let {item.out_var} = 0.0;')\n                    self.main_declared.add(item.out_var)\n            elif isinstance(item, IRGenerate):\n                if item.out_var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.out_var} = new Float32Array({item.max_len});')\n                    self.main_decls.append(f'    let {item.out_var}_len = 0;')\n                    self.main_declared.add(item.out_var)\n                logits_buf = '_gen_logits'\n                if logits_buf not in self.main_declared:\n                    self.main_decls.append(f'    const {logits_buf} = new Float32Array({item.out_n});')\n                    self.main_declared.add(logits_buf)\n                cache_name = self._cache_name(item.model_type)\n                if item.cache_var not in self.main_declared:\n                    self.main_decls.append(f'    const {item.cache_var} = create{cache_name}();')\n                    self.main_declared.add(item.cache_var)\n            elif isinstance(item, IRDataLoad):\n                data_x = f'_data_{item.x_name}'\n                data_y = f'_data_{item.y_name}'\n                n_var = f'_n_{item.x_name}'\n                if data_x not in self.main_declared:\n                    provider = self.lowerer.data_providers[item.provider]\n                    paths = provider[item.split]\n                    self.main_decls.append(f'    const {{ images: {data_x}, labels: {data_y}, n: {n_var} }} = MnistLoad(\"{paths[\"images\"]}\", \"{paths[\"labels\"]}\");')\n                    self.main_declared.update([data_x, data_y, n_var, item.x_name, item.y_name])\n            elif isinstance(item, IRCountOp):\n                if item.out_var not in self.main_declared:\n                    self.main_decls.append(f'    let {item.out_var} = 0;')\n                    self.main_declared.add(item.out_var)\n            # Recurse into child bodies\n            elif isinstance(item, IRFor):\n                if hasattr(item, '_ast_stmt') and not item.body:\n                    # Pre-lower tuple for body so we can collect its declarations\n                    self._pre_lower_tuple_for(item)\n                self._collect_decls(item.body)\n            elif isinstance(item, IRIf):\n                self._collect_decls(item.then_body)\n                self._collect_decls(item.else_body)\n            elif isinstance(item, IRArenaBlock):\n                self._collect_decls(item.body)\n\n    def _emit_main(self):\n        self._o(0, 'function main() {')\n        self._o(1, 'RngSeed(Date.now() & 0x7fffffff);')\n        self._o(1, 'const _target_loss = (typeof process !== \"undefined\" && process.argv[2]) ? parseFloat(process.argv[2]) : 0.0;')\n        self._o(0, '')\n\n        self.main_declared = set()\n        self.main_decls = []\n\n        # Two-pass: collect all declarations first, then emit body\n        self._collect_decls(self.module.main_body)\n\n        # Emit declarations\n        for d in self.main_decls:\n            self.out.append(d)\n        if self.main_decls:\n            self._o(0, '')\n\n        # Emit body\n        for item in self.module.main_body:\n            self._emit_main_item(item, 1)\n\n        self._o(0, '}')\n        self._o(0, 'main();')\n\n    def _emit_main_item(self, item, ind: int):\n        if isinstance(item, IRVecLiteral):\n            pass  # declaration handled by _collect_decls\n\n        elif isinstance(item, IRStringArray):\n            pass  # declaration handled by _collect_decls\n\n        elif isinstance(item, IRSetDecl):\n            pass  # declaration handled by _collect_decls\n\n        elif isinstance(item, IRModelInit):\n            self._o(ind, f'{self._fn_name(item.type_name, \"Init\")}({item.var}, {item.stddev});')\n            n_params = item.n_params\n            self._o(ind, f'console.log(`Training {item.type_name} (${{({n_params})}} params)`);')\n\n        elif isinstance(item, IROptDecl):\n            self._o(ind, f'// opt : nn\\'{item.kind}(... lr: {item.lr})')\n\n        elif isinstance(item, IRTextLoad):\n            pass  # declaration handled by _collect_decls\n\n        elif isinstance(item, IRTextEncode):\n            self._o(ind, f'{item.len_var} = TextEncode({item.text_var}, \"{item.prompt_str}\", {item.var});')\n\n        elif isinstance(item, IRTextRandomBatch):\n            self._o(ind, f'TextRandomBatch({item.text_var}, {item.x_var}, {item.y_var}, {item.seq_len}, {item.batch_size});')\n\n        elif isinstance(item, IRModelCall):\n            if getattr(item, '_redundant', False):\n                return\n            self._o(ind, f'{self._fn_name(item.model_type, \"Forward\")}({item.model_var}, {item.input_var}, {item.out}, {item.cache_var});')\n\n        elif isinstance(item, IRLoss):\n            if getattr(item, '_redundant', False):\n                return\n            loss_reg = self.lowerer.loss_registry[item.kind]\n            if loss_reg['args'] == 'pred_target':\n                self._o(ind, f'{item.out} = {loss_reg[\"forward\"]}({item.pred}, {item.target}, {item.dims[0]});')\n            elif loss_reg['args'] == 'pred_label':\n                self._o(ind, f'{item.out} = {loss_reg[\"forward\"]}({item.pred}, {item.target}, {item.dims[0]});')\n            elif loss_reg['args'] == 'logits_targets':\n                S, V = item.dims\n                self._o(ind, f'{item.out} = {loss_reg[\"forward\"]}({item.pred}, {item.target}, {S}, {V});')\n\n        elif isinstance(item, IRBackwardStep):\n            self._emit_backward_step(item, ind)\n\n        elif isinstance(item, IRGradCompute):\n            self._emit_grad_compute(item, ind)\n\n        elif isinstance(item, IRParamUpdate):\n            self._emit_param_update(item, ind)\n\n        elif isinstance(item, IRFor):\n            if hasattr(item, '_ast_stmt'):\n                self._emit_tuple_for(item, ind)\n                return\n            self._o(ind, f'for (let {item.var} = {item.start}; {item.var} < {item.end}; {item.var}++) {{')\n            has_backward = self.lowerer._ir_body_has_backward(item.body)\n            for child in item.body:\n                self._emit_main_item(child, ind + 1)\n            if has_backward:\n                loss_var = self.lowerer._find_loss_var_ir(item.body)\n                if loss_var:\n                    self._o(ind + 1, f'if (_target_loss > 0.0 && {loss_var} <= _target_loss) {{')\n                    self._o(ind + 2, f'console.log(`Target loss ${{_target_loss.toFixed(4)}} reached at step ${{{item.var}}}`);')\n                    self._o(ind + 2, f'break;')\n                    self._o(ind + 1, f'}}')\n            self._o(ind, '}')\n\n        elif isinstance(item, IRIf):\n            cond_js = self._c_to_js_expr(item.cond)\n            # Replace C-style '=' equality with '==='\n            cond_js = re.sub(r'(?<!=)=(?!=)', '===', cond_js)\n            self._o(ind, f'if ({cond_js}) {{')\n            for child in item.then_body:\n                self._emit_main_item(child, ind + 1)\n            self._o(ind, '}')\n            if item.else_body:\n                self._o(ind, 'else {')\n                for child in item.else_body:\n                    self._emit_main_item(child, ind + 1)\n                self._o(ind, '}')\n\n        elif isinstance(item, IRPrint):\n            self._emit_print(item, ind)\n\n        elif isinstance(item, IRArenaBlock):\n            self._o(ind, '{ // arena')\n            for child in item.body:\n                self._emit_main_item(child, ind + 1)\n            self._o(ind, '}')\n\n        elif isinstance(item, IRAssign):\n            expr_js = self._c_to_js_expr(item.expr)\n            self._o(ind, f'{item.var} = {expr_js};')\n\n        elif isinstance(item, IRDerivExpr):\n            arg_js = self._c_to_js_expr(item.arg_expr)\n            self._o(ind, f'{{ const _d_in = {{ val: {arg_js}, deriv: 1.0 }};')\n            self._o(ind, f'const _d_out = {item.fn_name}_dual(_d_in);')\n            self._o(ind, f'{item.out_var} = _d_out.deriv; }}')\n\n        elif isinstance(item, IRGenerate):\n            self._emit_generate(item, ind)\n\n        elif isinstance(item, IRTextDecode):\n            self._o(ind, f'console.log(TextDecode({item.text_var}, {item.tokens_var}, {item.tokens_var}_len));')\n\n        elif isinstance(item, IRDataLoad):\n            self._emit_data_load(item, ind)\n\n        elif isinstance(item, IRCountOp):\n            self._emit_count_op(item, ind)\n\n    def _emit_print(self, item: IRPrint, ind: int):\n        \"\"\"Convert C printf format to JS console.log with template literal.\"\"\"\n        fmt = item.format_str\n        args = list(item.args)\n\n        if not args:\n            self._o(ind, f'console.log(\"{fmt}\");')\n            return\n\n        # Build template literal\n        result = ''\n        arg_idx = 0\n        i = 0\n        while i < len(fmt):\n            if fmt[i] == '%' and i + 1 < len(fmt) and arg_idx < len(args):\n                spec_start = i\n                i += 1\n                # Skip flags\n                while i < len(fmt) and fmt[i] in '-+ 0#':\n                    i += 1\n                # Skip width\n                while i < len(fmt) and fmt[i].isdigit():\n                    i += 1\n                # Skip precision\n                if i < len(fmt) and fmt[i] == '.':\n                    i += 1\n                    prec = ''\n                    while i < len(fmt) and fmt[i].isdigit():\n                        prec += fmt[i]\n                        i += 1\n                else:\n                    prec = ''\n                # Type\n                if i < len(fmt):\n                    ftype = fmt[i]\n                    i += 1\n                else:\n                    ftype = 's'\n\n                var_name = args[arg_idx][0]\n                if ftype == 'd':\n                    result += '${' + var_name + '}'\n                elif ftype in ('f', 'e', 'g'):\n                    if prec:\n                        result += '${' + var_name + '.toFixed(' + prec + ')}'\n                    else:\n                        result += '${' + var_name + '.toFixed(4)}'\n                elif ftype == 's':\n                    result += '${' + var_name + '}'\n                else:\n                    result += '${' + var_name + '}'\n                arg_idx += 1\n            else:\n                result += fmt[i]\n                i += 1\n\n        self._o(ind, f'console.log(`{result}`);')\n\n    def _emit_backward_step(self, item: IRBackwardStep, ind: int):\n        tname = item.model_type\n        n_params = item.n_params\n        model_var = item.model_var\n        loss_var = item.loss_var\n        loss_info = item.loss_info\n        loss_kind = item.loss_kind\n        pred_var = item.pred_var\n        pred_info = item.pred_info\n        grad_var = item.grad_var\n\n        if loss_kind not in self.lowerer.loss_registry:\n            self._o(ind, f'// ERROR: cannot differentiate {loss_var} (kind={loss_kind})')\n            return\n        if pred_info.get('kind') != 'model_apply':\n            self._o(ind, f'// ERROR: {pred_var} not from model apply')\n            return\n\n        loss_reg = self.lowerer.loss_registry[loss_kind]\n        input_var = pred_info['input']\n        cache_var = pred_info['cache']\n        d_pred_var = f'd_{pred_var}'\n        pred_n = loss_info.get('n', 1)\n\n        input_info = self.lowerer.chain.get(input_var, {})\n        batch_size = input_info.get('batch_size', 1) if input_info.get('kind') == 'text_batch' else 1\n        seq_len = input_info.get('seq_len', 128) if input_info.get('kind') == 'text_batch' else 0\n        y_batch_var = input_info.get('y_name') if input_info.get('kind') == 'text_batch' else None\n\n        if batch_size > 1 and loss_reg['args'] == 'logits_targets':\n            S = loss_info.get('S', seq_len)\n            V = loss_info.get('V', 128)\n            targets_var = loss_info.get('targets', y_batch_var or 'y')\n\n            bg_var = f'_bg_{model_var}'\n            self._o(ind, f'{loss_var} = 0.0;')\n            self._o(ind, f'{grad_var}._buf.fill(0);')\n            self._o(ind, f'for (let _b = 0; _b < {batch_size}; _b++) {{')\n            self._o(ind + 1, f'const _bx = {input_var}.subarray(_b * {seq_len}, (_b + 1) * {seq_len});')\n            self._o(ind + 1, f'const _by = {targets_var}.subarray(_b * {seq_len}, (_b + 1) * {seq_len});')\n            self._o(ind + 1, f'{self._fn_name(tname, \"Forward\")}({model_var}, _bx, {pred_var}, {cache_var});')\n            self._o(ind + 1, f'{loss_var} += {loss_reg[\"forward\"]}({pred_var}, _by, {S}, {V});')\n            self._o(ind + 1, f'{loss_reg[\"backward\"]}({pred_var}, _by, {d_pred_var}, {S}, {V});')\n            self._o(ind + 1, f'{bg_var}._buf.fill(0);')\n            self._o(ind + 1, f'{self._fn_name(tname, \"Backward\")}({model_var}, _bx, {cache_var}, {d_pred_var}, {bg_var}, null);')\n            self._o(ind + 1, f'for (let _i = 0; _i < {n_params}; _i++) {grad_var}._buf[_i] += {bg_var}._buf[_i];')\n            self._o(ind, '}')\n            self._o(ind, f'{loss_var} /= {batch_size}.0;')\n            self._o(ind, f'for (let _i = 0; _i < {n_params}; _i++) {grad_var}._buf[_i] /= {batch_size}.0;')\n        else:\n            if loss_reg['args'] == 'pred_target':\n                target_var = loss_info['target']\n                self._o(ind, f'{loss_reg[\"backward\"]}({pred_var}, {target_var}, {d_pred_var}, {pred_n});')\n            elif loss_reg['args'] == 'pred_label':\n                label_var = loss_info['label']\n                self._o(ind, f'{loss_reg[\"backward\"]}({pred_var}, {label_var}, {d_pred_var}, {pred_n});')\n            elif loss_reg['args'] == 'logits_targets':\n                S = loss_info.get('S', 128)\n                V = loss_info.get('V', 128)\n                targets_var = loss_info.get('targets', 'y')\n                self._o(ind, f'{loss_reg[\"backward\"]}({pred_var}, {targets_var}, {d_pred_var}, {S}, {V});')\n\n            self._o(ind, f'{grad_var}._buf.fill(0);')\n            self._o(ind, f'{self._fn_name(tname, \"Backward\")}({model_var}, {input_var}, {cache_var}, {d_pred_var}, {grad_var}, null);')\n\n        # Optimizer step\n        if item.opt_kind == 'adam':\n            state_var = f'adam_state_{model_var}'\n            self._o(ind, f'AdamStep({model_var}._buf, {grad_var}._buf, {state_var}, {item.opt_lr}, 0.9, 0.999, 1e-8);')\n        else:\n            self._o(ind, f'SgdStep({model_var}._buf, {grad_var}._buf, {n_params}, {item.opt_lr});')\n\n    def _emit_grad_compute(self, item: IRGradCompute, ind: int):\n        \"\"\"Emit gradient computation (backward pass) without optimizer step.\"\"\"\n        tname = item.model_type\n        model_var = item.model_var\n        loss_var = item.loss_var\n        loss_info = item.loss_info\n        loss_kind = item.loss_kind\n        pred_var = item.pred_var\n        pred_info = item.pred_info\n        grad_var = item.grad_var\n\n        if loss_kind not in self.lowerer.loss_registry:\n            self._o(ind, f'// ERROR: cannot differentiate {loss_var} (kind={loss_kind})')\n            return\n        if pred_info.get('kind') != 'model_apply':\n            self._o(ind, f'// ERROR: {pred_var} not from model apply')\n            return\n\n        loss_reg = self.lowerer.loss_registry[loss_kind]\n        input_var = pred_info['input']\n        cache_var = pred_info['cache']\n        d_pred_var = f'd_{pred_var}'\n        pred_n = loss_info.get('n', 1)\n\n        # Loss backward\n        if loss_reg['args'] == 'pred_target':\n            target_var = loss_info['target']\n            self._o(ind, f'{loss_reg[\"backward\"]}({pred_var}, {target_var}, {d_pred_var}, {pred_n});')\n        elif loss_reg['args'] == 'pred_label':\n            label_var = loss_info['label']\n            self._o(ind, f'{loss_reg[\"backward\"]}({pred_var}, {label_var}, {d_pred_var}, {pred_n});')\n        elif loss_reg['args'] == 'logits_targets':\n            S = loss_info.get('S', 128)\n            V = loss_info.get('V', 128)\n            targets_var = loss_info.get('targets', 'y')\n            self._o(ind, f'{loss_reg[\"backward\"]}({pred_var}, {targets_var}, {d_pred_var}, {S}, {V});')\n\n        # Model backward\n        self._o(ind, f'{grad_var}._buf.fill(0);')\n        self._o(ind, f'{self._fn_name(tname, \"Backward\")}({model_var}, {input_var}, {cache_var}, {d_pred_var}, {grad_var}, null);')\n\n    def _emit_param_update(self, item: IRParamUpdate, ind: int):\n        \"\"\"Emit parameter update: model'params -= lr * grads.\"\"\"\n        lr_expr = item.lr if isinstance(item.lr, str) else item.lr\n        self._o(ind, f'SgdStep({item.model_var}._buf, {item.grad_var}._buf, {item.n_params}, {lr_expr});')\n\n    def _emit_generate(self, item: IRGenerate, ind: int):\n        name = item.out_var\n        model_name = item.model_var\n        tname = item.model_type\n        prompt_name = item.prompt_var\n        max_len = item.max_len\n        temperature = item.temperature\n        seq_len = item.seq_len\n        vocab = item.vocab_size\n        out_n = item.out_n\n        cache_var = item.cache_var\n\n        logits_buf = '_gen_logits'\n\n        self._o(ind, f'{name}.set({prompt_name}.subarray(0, {prompt_name}_len));')\n        self._o(ind, f'{name}_len = {prompt_name}_len;')\n        self._o(ind, f'while ({name}_len < {max_len}) {{')\n        self._o(ind + 1, f'const _ctx = new Float32Array({seq_len});')\n        self._o(ind + 1, f'const _cstart = {name}_len > {seq_len} ? {name}_len - {seq_len} : 0;')\n        self._o(ind + 1, f'const _clen = {name}_len - _cstart;')\n        self._o(ind + 1, f'for (let _i = 0; _i < _clen; _i++) _ctx[_i] = {name}[_cstart + _i];')\n        self._o(ind + 1, f'{self._fn_name(tname, \"Forward\")}({model_name}, _ctx, {logits_buf}, {cache_var});')\n        self._o(ind + 1, f'const _lp = {logits_buf}.subarray((_clen - 1) * {vocab}, _clen * {vocab});')\n        self._o(ind + 1, f'for (let _i = 0; _i < {vocab}; _i++) _lp[_i] /= {temperature};')\n        self._o(ind + 1, f'let _mx = _lp[0]; for (let _i = 1; _i < {vocab}; _i++) if (_lp[_i] > _mx) _mx = _lp[_i];')\n        self._o(ind + 1, f'let _sm = 0.0; for (let _i = 0; _i < {vocab}; _i++) {{ _lp[_i] = Math.exp(_lp[_i] - _mx); _sm += _lp[_i]; }}')\n        self._o(ind + 1, f'for (let _i = 0; _i < {vocab}; _i++) _lp[_i] /= _sm;')\n        self._o(ind + 1, f'const _r = RngFloat();')\n        self._o(ind + 1, f'let _cum = 0.0, _tok = 0;')\n        self._o(ind + 1, f'for (let _i = 0; _i < {vocab}; _i++) {{ _cum += _lp[_i]; if (_cum >= _r) {{ _tok = _i; break; }} }}')\n        self._o(ind + 1, f'{name}[{name}_len++] = _tok;')\n        self._o(ind, '}')\n\n    def _emit_data_load(self, item: IRDataLoad, ind: int):\n        \"\"\"Emit MNIST data loading for Node.js \u2014 declaration handled by _collect_decls.\"\"\"\n        pass  # MnistLoad call emitted in _collect_decls\n\n    def _emit_count_op(self, item: IRCountOp, ind: int):\n        \"\"\"Emit count operation over dataset.\"\"\"\n        x_ds = self.lowerer.datasets.get(item.x_ds, {})\n        y_ds = self.lowerer.datasets.get(item.y_ds, {})\n        n_var = x_ds.get('n_var', '_n')\n        dim = x_ds.get('dim', 784)\n        data_x = x_ds.get('data_var', '_data_x')\n        data_y = y_ds.get('data_var', '_data_y')\n        if item.out_var not in self.main_declared:\n            self._o(ind, f'let {item.out_var} = 0;')\n            self.main_declared.add(item.out_var)\n        self._o(ind, f'{item.out_var} = 0;')\n        self._o(ind, f'for (let _ci = 0; _ci < {n_var}; _ci++) {{')\n        self._o(ind + 1, f'const _cx = {data_x}.subarray(_ci * {dim}, (_ci + 1) * {dim});')\n        self._o(ind + 1, f'const _cy = {data_y}[_ci] | 0;')\n        cond = self._c_to_js_expr(item.lambda_body)\n        cond = re.sub(r'(?<!=)=(?!=)', '===', cond)\n        if item.model_call:\n            mc = item.model_call\n            tname = mc['model_type']\n            model_name = mc['model']\n            model_arg = mc['model_arg']\n            ret_n = mc['ret_n']\n            pred_buf = '_eval_pred'\n            cache_buf = '_eval_cache'\n            cache_name = self._cache_name(tname)\n            if pred_buf not in self.main_declared:\n                self._o(ind + 1, f'const {pred_buf} = new Float32Array({ret_n});')\n                self._o(ind + 1, f'const {cache_buf} = create{cache_name}();')\n                self.main_declared.update([pred_buf, cache_buf])\n            self._o(ind + 1, f'{self._fn_name(tname, \"Forward\")}({model_name}, {model_arg}, {pred_buf}, {cache_buf});')\n        self._o(ind + 1, f'if ({cond}) {item.out_var}++;')\n        self._o(ind, '}')\n\n    def _parse_tuple_for_info(self, item: IRFor):\n        \"\"\"Parse tuple for info: returns dict with 'kind' and relevant fields, or None.\"\"\"\n        from . import ast_nodes as ast\n        stmt = item._ast_stmt\n\n        chain_ops = []\n        node = stmt.iterable\n        while isinstance(node, ast.MethodCall):\n            chain_ops.append({'op': node.method, 'args': list(node.args)})\n            node = node.obj\n        if isinstance(node, ast.Ident):\n            chain_ops.append({'op': 'base', 'name': node.name})\n        chain_ops.reverse()\n\n        if not chain_ops or chain_ops[0].get('op') != 'base':\n            return None\n\n        x_ds_name = chain_ops[0]['name']\n        y_ds_name = None\n        do_shuffle = False\n        choose_k = None\n        for step in chain_ops[1:]:\n            if step['op'] == 'zip' and step['args']:\n                y_ds_name = step['args'][0].name if isinstance(step['args'][0], ast.Ident) else None\n            elif step['op'] == 'shuffle':\n                do_shuffle = True\n            elif step['op'] == 'choose' and step['args']:\n                choose_k = int(step['args'][0].value) if isinstance(step['args'][0], ast.Num) else None\n\n        if choose_k == 2 and x_ds_name in item._lowerer.sets:\n            return {'kind': 'choose', 'set_name': x_ds_name, 'choose_k': choose_k}\n\n        if not y_ds_name:\n            return None\n        return {'kind': 'zip', 'x_ds_name': x_ds_name, 'y_ds_name': y_ds_name, 'do_shuffle': do_shuffle}\n\n    def _pre_lower_tuple_for(self, item: IRFor):\n        \"\"\"Pre-lower the tuple for body so _collect_decls can see the IR nodes.\"\"\"\n        info = self._parse_tuple_for_info(item)\n        if not info:\n            return\n        lowerer = item._lowerer\n        stmt = item._ast_stmt\n\n        # choose(2) path\n        if info['kind'] == 'choose':\n            set_name = info['set_name']\n            set_info = lowerer.sets[set_name]\n            dim = set_info['dim']\n            var1 = stmt.var.names[0]\n            var2 = stmt.var.names[1]\n            lowerer.env[var1] = DxType('Vec', [dim])\n            lowerer.env[var2] = DxType('Vec', [dim])\n            lowerer.main_declared.update([var1, var2])\n            lowerer.set_element_vars[var1] = set_name\n            lowerer.set_element_vars[var2] = set_name\n            lowerer.set_element_vars[f'_idx_{var1}'] = '_i'\n            lowerer.set_element_vars[f'_idx_{var2}'] = '_j'\n            for s in stmt.body:\n                lowerer._lower_main_stmt(s, item.body)\n            for key in [var1, var2, f'_idx_{var1}', f'_idx_{var2}']:\n                lowerer.set_element_vars.pop(key, None)\n            item._pre_lowered = True\n            return\n\n        x_ds_name = info['x_ds_name']\n        y_ds_name = info['y_ds_name']\n\n        # Inline data path\n        x_inl = lowerer.inline_data.get(x_ds_name)\n        y_inl = lowerer.inline_data.get(y_ds_name)\n        if x_inl and y_inl:\n            x_var = stmt.var.names[0]\n            y_var = stmt.var.names[1]\n            lowerer.env[x_var] = DxType('Vec', [x_inl['cols']])\n            lowerer.env[y_var] = DxType('Vec', [y_inl['cols']])\n            lowerer.main_declared.update([x_var, y_var])\n            for s in stmt.body:\n                lowerer._lower_main_stmt(s, item.body)\n            item._pre_lowered = True\n            return\n\n        # Dataset path\n        x_ds = lowerer.datasets.get(x_ds_name)\n        y_ds = lowerer.datasets.get(y_ds_name)\n        if not (x_ds and y_ds):\n            return\n\n        dim = x_ds['dim']\n        x_var = stmt.var.names[0]\n        y_var = stmt.var.names[1]\n        lowerer.env[x_var] = DxType('Vec', [dim])\n        lowerer.env[y_var] = INT\n        lowerer.main_declared.update([x_var, y_var])\n\n        for s in stmt.body:\n            lowerer._lower_main_stmt(s, item.body)\n        # Mark as pre-lowered so _emit_tuple_for doesn't re-lower\n        item._pre_lowered = True\n\n    def _emit_tuple_for(self, item: IRFor, ind: int):\n        \"\"\"Emit for loop with tuple destructuring over dataset, inline data, or set choose.\"\"\"\n        info = self._parse_tuple_for_info(item)\n        if not info:\n            return\n        lowerer = item._lowerer\n        stmt = item._ast_stmt\n\n        # --- choose(2): for (a b) in set'choose(2) \u2192 nested i,j loops ---\n        if info['kind'] == 'choose':\n            set_name = info['set_name']\n            set_info = lowerer.sets[set_name]\n            n = len(set_info['elements'])\n            dim = set_info['dim']\n            var1 = stmt.var.names[0]\n            var2 = stmt.var.names[1]\n\n            self._o(ind, f'for (let _i = 0; _i < {n}; _i++) {{')\n            self._o(ind + 1, f'for (let _j = _i + 1; _j < {n}; _j++) {{')\n            self._o(ind + 2, f'const {var1} = _{set_name}_data[_i];')\n            self._o(ind + 2, f'const {var2} = _{set_name}_data[_j];')\n\n            if not getattr(item, '_pre_lowered', False):\n                lowerer.env[var1] = DxType('Vec', [dim])\n                lowerer.env[var2] = DxType('Vec', [dim])\n                lowerer.main_declared.update([var1, var2])\n                lowerer.set_element_vars[var1] = set_name\n                lowerer.set_element_vars[var2] = set_name\n                lowerer.set_element_vars[f'_idx_{var1}'] = '_i'\n                lowerer.set_element_vars[f'_idx_{var2}'] = '_j'\n                for s in stmt.body:\n                    lowerer._lower_main_stmt(s, item.body)\n                for key in [var1, var2, f'_idx_{var1}', f'_idx_{var2}']:\n                    lowerer.set_element_vars.pop(key, None)\n\n            for child in item.body:\n                self._emit_main_item(child, ind + 2)\n\n            self._o(ind + 1, '}')\n            self._o(ind, '}')\n            return\n\n        x_ds_name = info['x_ds_name']\n        y_ds_name = info['y_ds_name']\n        do_shuffle = info['do_shuffle']\n\n        # --- Inline data: for (x y) in X'zip(Y) where X, Y are Tensor ---\n        x_inl = lowerer.inline_data.get(x_ds_name)\n        y_inl = lowerer.inline_data.get(y_ds_name)\n        if x_inl and y_inl:\n            rows = x_inl['rows']\n            x_cols = x_inl['cols']\n            y_cols = y_inl['cols']\n            x_var = stmt.var.names[0]\n            y_var = stmt.var.names[1]\n\n            self._o(ind, f'for (let _i = 0; _i < {rows}; _i++) {{')\n            self._o(ind + 1, f'const {x_var} = {x_ds_name}.subarray(_i * {x_cols}, (_i + 1) * {x_cols});')\n            self._o(ind + 1, f'const {y_var} = {y_ds_name}.subarray(_i * {y_cols}, (_i + 1) * {y_cols});')\n\n            if not getattr(item, '_pre_lowered', False):\n                lowerer.env[x_var] = DxType('Vec', [x_cols])\n                lowerer.env[y_var] = DxType('Vec', [y_cols])\n                lowerer.main_declared.update([x_var, y_var])\n                for s in stmt.body:\n                    lowerer._lower_main_stmt(s, item.body)\n\n            for child in item.body:\n                self._emit_main_item(child, ind + 1)\n\n            self._o(ind, '}')\n            return\n\n        # --- Dataset: for (x y) in train_x'zip(train_y)'shuffle()... ---\n        x_ds = lowerer.datasets.get(x_ds_name)\n        y_ds = lowerer.datasets.get(y_ds_name)\n        if not (x_ds and y_ds):\n            return\n\n        n_var = x_ds['n_var']\n        dim = x_ds['dim']\n        data_x = x_ds['data_var']\n        data_y = y_ds['data_var']\n        idx_var = f'_idx_{x_ds_name}'\n\n        if idx_var not in self.main_declared:\n            self._o(ind, f'const {idx_var} = new Int32Array({n_var});')\n            self.main_declared.add(idx_var)\n\n        self._o(ind, f'for (let _i = 0; _i < {n_var}; _i++) {idx_var}[_i] = _i;')\n        if do_shuffle:\n            self._o(ind, f'ShuffleIndices({idx_var}, {n_var});')\n\n        self._o(ind, f'for (let _s = 0; _s < {n_var}; _s++) {{')\n        self._o(ind + 1, f'const _didx = {idx_var}[_s];')\n\n        x_var = stmt.var.names[0]\n        y_var = stmt.var.names[1]\n        self._o(ind + 1, f'const {x_var} = {data_x}.subarray(_didx * {dim}, (_didx + 1) * {dim});')\n        self._o(ind + 1, f'const {y_var} = {data_y}[_didx] | 0;')\n\n        if not getattr(item, '_pre_lowered', False):\n            lowerer.env[x_var] = DxType('Vec', [dim])\n            lowerer.env[y_var] = INT\n            lowerer.main_declared.update([x_var, y_var])\n            for s in stmt.body:\n                lowerer._lower_main_stmt(s, item.body)\n\n        for child in item.body:\n            self._emit_main_item(child, ind + 1)\n\n        self._o(ind, '}')\n        return\n\n    # ------- top-level: generate entire JS file -------\n\n    def _emit_header(self):\n        source = self.source_file or 'unknown.dx'\n        types = ', '.join(self.module.type_order) if self.module.type_order else 'none'\n        total_params = 0\n        for name, val in self.module.constants.items():\n            if name.startswith('NUM_PARAMS_'):\n                total_params = val\n        self._o(0, '// Generated by dx compiler (JS backend)')\n        self._o(0, f'// Source: {source}')\n        if total_params:\n            self._o(0, f'// Total parameters: {total_params:,}')\n        self._o(0, '\"use strict\";')\n        self._o(0, '')\n\n    def _emit_constants(self):\n        constants = self.module.constants\n        if not constants:\n            return\n        self._o(0, '// ---- Dimensions ----')\n        for name, val in constants.items():\n            self._o(0, f'const {name} = {val};')\n        self._o(0, '')\n\n    def emit(self) -> str:\n        self.out = []\n\n        self._emit_header()\n        self.out.append(JS_RUNTIME)\n        self._emit_constants()\n\n        # Struct factories\n        self._o(0, '// ---- Structs ----')\n        for sdef in self.module.struct_defs:\n            self._emit_struct_factory(sdef)\n        for gdef in self.module.grad_struct_defs:\n            self._emit_grad_factory(gdef)\n\n        # Free functions (scalar + dual)\n        if self.module.scalar_fns or self.module.dual_fns:\n            self._o(0, '// ---- Free functions ----')\n            for fn in self.module.scalar_fns:\n                self._emit_scalar_fn(fn)\n            for fn in self.module.dual_fns:\n                self._emit_dual_fn(fn)\n\n        # Methods: cache factory, forward fn, backward fn\n        self._o(0, '// ---- Forward/backward functions ----')\n        for fn in self.module.functions:\n            if fn.is_method:\n                self._emit_cache_factory(fn)\n                self._emit_forward_fn(fn)\n                self._emit_backward_fn(fn)\n\n        # Init functions\n        self._o(0, '// ---- Init functions ----')\n        for tname in self.module.init_fns:\n            self._emit_init_fn(tname)\n\n        # Main\n        self._o(0, '// ---- Main ----')\n        self._emit_main()\n\n        return '\\n'.join(self.out)\n\n\n# ---------------------------------------------------------------------------\n# Entry point\n# ---------------------------------------------------------------------------\n\ndef emit_js(module: IRModule, lowerer=None, source_file: str = '') -> str:\n    \"\"\"Generate JavaScript code from an IRModule.\"\"\"\n    return JSEmitter(module, lowerer, source_file=source_file).emit()\n"
};

// ================================================================
// PYODIDE SETUP
// ================================================================
let pyodide = null;

async function initPyodide() {
    pyodide = await loadPyodide();
    // Write dx package to virtual filesystem
    pyodide.FS.mkdir('/dx');
    for (const [name, code] of Object.entries(DX_MODULES))
        pyodide.FS.writeFile('/dx/' + name, code);
    pyodide.runPython('import sys; sys.path.insert(0, "/")');
}

// ================================================================
// COMPILE DX → JS via Pyodide
// ================================================================
function compileDx(source) {
    pyodide.globals.set('__dx_source', source);
    return pyodide.runPython(`
from dx.parser import parse
from dx.ir_lower import IRLower
from dx.ir_passes import run_all_passes
from dx.emit_js import emit_js

program = parse(__dx_source)
lowerer = IRLower(program)
module = lowerer.lower()
module = run_all_passes(module, lowerer)
emit_js(module, lowerer)
`);
}

// ================================================================
// TRAINING DATA (universal for XOR)
// ================================================================
const TRAIN = [
    { x: new Float32Array([0, 0]), y: new Float32Array([0]) },
    { x: new Float32Array([0, 1]), y: new Float32Array([1]) },
    { x: new Float32Array([1, 0]), y: new Float32Array([1]) },
    { x: new Float32Array([1, 1]), y: new Float32Array([0]) },
];
const DATA_POINTS = [[0,0,0],[0,1,1],[1,0,1],[1,1,0]];

// ================================================================
// DEMO STATE
// ================================================================
let net, grad, cache, predCache, pred, dPred;
let epoch, losses, running, consoleLines;
let N_PARAMS, LR;

function initState() {
    net = createNet(); grad = createNetGrad(); cache = createNetCache();
    predCache = createNetCache();
    pred = new Float32Array(1); dPred = new Float32Array(1);
    epoch = 0; losses = []; consoleLines = [];
    RngSeed(42);
    NetInit(net, 1.0);
}

function trainOneEpoch() {
    let totalLoss = 0;
    for (const { x, y } of TRAIN) {
        NetForward(net, x, pred, cache);
        const loss = SseForward(pred, y, 1);
        SseBackward(pred, y, dPred, 1);
        grad._buf.fill(0);
        NetBackward(net, x, cache, dPred, grad, null);
        SgdStep(net._buf, grad._buf, N_PARAMS, LR);
        totalLoss += loss;
    }
    return totalLoss;
}

function predict(a, b) {
    const inp = new Float32Array([a, b]);
    const out = new Float32Array(1);
    NetForward(net, inp, out, predCache);
    return out[0];
}

// ================================================================
// VISUALIZATION (same as xor.html)
// ================================================================
const GRID = 80;
const offCv = document.createElement('canvas');
offCv.width = GRID; offCv.height = GRID;
const offCtx = offCv.getContext('2d');

function setupCanvas(canvas) {
    const dpr = window.devicePixelRatio || 1;
    const rect = canvas.getBoundingClientRect();
    canvas.width = Math.round(rect.width * dpr);
    canvas.height = Math.round(rect.height * dpr);
    const ctx = canvas.getContext('2d');
    ctx.scale(dpr, dpr);
    return { ctx, w: rect.width, h: rect.height };
}

function drawBoundary() {
    const cvEl = document.getElementById('cv-boundary');
    const { ctx, w, h } = setupCanvas(cvEl);
    const imgData = offCtx.createImageData(GRID, GRID);
    const inp = new Float32Array(2);
    const out = new Float32Array(1);
    const c = createNetCache();
    const vals = new Float32Array(GRID * GRID);

    for (let gy = 0; gy < GRID; gy++) {
        for (let gx = 0; gx < GRID; gx++) {
            inp[0] = gx / (GRID - 1);
            inp[1] = 1 - gy / (GRID - 1);
            NetForward(net, inp, out, c);
            vals[gy * GRID + gx] = out[0];
            const v = Math.max(0, Math.min(1, out[0]));
            const idx = (gy * GRID + gx) * 4;
            imgData.data[idx]     = Math.round(59  + v * 180);
            imgData.data[idx + 1] = Math.round(130 - v * 62);
            imgData.data[idx + 2] = Math.round(246 - v * 178);
            imgData.data[idx + 3] = 255;
        }
    }
    offCtx.putImageData(imgData, 0, 0);
    const margin = 24;
    ctx.fillStyle = '#0d1117';
    ctx.fillRect(0, 0, w, h);
    ctx.imageSmoothingEnabled = true;
    ctx.imageSmoothingQuality = 'high';
    ctx.drawImage(offCv, margin, margin, w - 2 * margin, h - 2 * margin);

    const toX = x1 => margin + x1 * (w - 2 * margin);
    const toY = x2 => (h - margin) - x2 * (h - 2 * margin);

    // Draw actual decision boundary (output = 0.5 contour via marching squares)
    ctx.strokeStyle = 'rgba(255, 255, 255, 0.7)';
    ctx.lineWidth = 1.5;
    ctx.setLineDash([6, 4]);
    ctx.beginPath();
    for (let gy = 0; gy < GRID - 1; gy++) {
        for (let gx = 0; gx < GRID - 1; gx++) {
            const v00 = vals[gy * GRID + gx];
            const v10 = vals[gy * GRID + gx + 1];
            const v01 = vals[(gy+1) * GRID + gx];
            const v11 = vals[(gy+1) * GRID + gx + 1];
            const pts = [];
            if ((v00 - 0.5) * (v10 - 0.5) < 0) {
                const t = (0.5 - v00) / (v10 - v00);
                pts.push([(gx + t) / (GRID-1), 1 - gy / (GRID-1)]);
            }
            if ((v10 - 0.5) * (v11 - 0.5) < 0) {
                const t = (0.5 - v10) / (v11 - v10);
                pts.push([(gx+1) / (GRID-1), 1 - (gy + t) / (GRID-1)]);
            }
            if ((v01 - 0.5) * (v11 - 0.5) < 0) {
                const t = (0.5 - v01) / (v11 - v01);
                pts.push([(gx + t) / (GRID-1), 1 - (gy+1) / (GRID-1)]);
            }
            if ((v00 - 0.5) * (v01 - 0.5) < 0) {
                const t = (0.5 - v00) / (v01 - v00);
                pts.push([gx / (GRID-1), 1 - (gy + t) / (GRID-1)]);
            }
            if (pts.length >= 2) {
                ctx.moveTo(toX(pts[0][0]), toY(pts[0][1]));
                ctx.lineTo(toX(pts[1][0]), toY(pts[1][1]));
            }
        }
    }
    ctx.stroke();
    ctx.setLineDash([]);

    for (const [a, b, label] of DATA_POINTS) {
        const px = toX(a);
        const py = toY(b);
        ctx.beginPath();
        ctx.arc(px, py, 11, 0, Math.PI * 2);
        ctx.fillStyle = 'rgba(0,0,0,0.4)';
        ctx.fill();
        ctx.beginPath();
        ctx.arc(px, py, 9, 0, Math.PI * 2);
        ctx.fillStyle = label ? '#ef4444' : '#3b82f6';
        ctx.fill();
        ctx.strokeStyle = '#fff';
        ctx.lineWidth = 2.5;
        ctx.stroke();
        ctx.fillStyle = '#fff';
        ctx.font = 'bold 10px -apple-system, sans-serif';
        ctx.textAlign = 'center';
        ctx.textBaseline = 'middle';
        ctx.fillText(label.toString(), px, py);
    }

    ctx.fillStyle = '#8b949e';
    ctx.font = '10px -apple-system, sans-serif';
    ctx.textAlign = 'center';
    ctx.fillText('x\u2081', w / 2, h - 4);
    ctx.save();
    ctx.translate(10, h / 2);
    ctx.rotate(-Math.PI / 2);
    ctx.fillText('x\u2082', 0, 0);
    ctx.restore();
}

// ================================================================
// 3D OUTPUT SURFACE
// ================================================================
const GRID3D = 30;
let az3d = 2.3, el3d = 0.55;
let drag3d = false, mx3d = 0, my3d = 0;

function draw3D() {
    const cvEl = document.getElementById('cv-3d');
    if (!cvEl || typeof createNetCache !== 'function' || !net) return;
    const { ctx, w, h } = setupCanvas(cvEl);
    ctx.clearRect(0, 0, w, h);

    const inp = new Float32Array(2);
    const out = new Float32Array(1);
    const c3 = createNetCache();
    const G = GRID3D;

    const cosA = Math.cos(az3d), sinA = Math.sin(az3d);
    const cosE = Math.cos(el3d), sinE = Math.sin(el3d);
    const scale = Math.min(w, h) * 0.7;

    function proj(x, y, z) {
        const rx = x * cosA - y * sinA;
        const ry = x * sinA + y * cosA;
        const ry2 = ry * cosE - z * sinE;
        const rz = ry * sinE + z * cosE;
        return [w / 2 + rx * scale, h / 2 - ry2 * scale, rz];
    }

    // Build grid values
    const vals = new Float32Array((G + 1) * (G + 1));
    for (let j = 0; j <= G; j++) {
        for (let i = 0; i <= G; i++) {
            inp[0] = i / G; inp[1] = j / G;
            NetForward(net, inp, out, c3);
            vals[j * (G + 1) + i] = out[0];
        }
    }

    // Build surface quads
    const quads = [];
    for (let j = 0; j < G; j++) {
        for (let i = 0; i < G; i++) {
            const idx = [j*(G+1)+i, j*(G+1)+i+1, (j+1)*(G+1)+i+1, (j+1)*(G+1)+i];
            const corners = idx.map(k => {
                const gi = k % (G + 1), gj = Math.floor(k / (G + 1));
                return proj(gi / G - 0.5, gj / G - 0.5, vals[k] * 0.8 - 0.4);
            });
            const avgZ = (corners[0][2] + corners[1][2] + corners[2][2] + corners[3][2]) / 4;
            const avgV = (vals[idx[0]] + vals[idx[1]] + vals[idx[2]] + vals[idx[3]]) / 4;
            quads.push({ corners, avgZ, avgV, type: 's' });
        }
    }

    // Decision plane quads at z = 0.5 (rendered as grid)
    const PG = 6;
    for (let j = 0; j < PG; j++) {
        for (let i = 0; i < PG; i++) {
            const x0 = i / PG - 0.5, x1 = (i + 1) / PG - 0.5;
            const y0 = j / PG - 0.5, y1 = (j + 1) / PG - 0.5;
            const zp = 0.5 * 0.8 - 0.4;
            const corners = [[x0,y0,zp],[x1,y0,zp],[x1,y1,zp],[x0,y1,zp]].map(([x,y,z]) => proj(x,y,z));
            const avgZ = (corners[0][2] + corners[1][2] + corners[2][2] + corners[3][2]) / 4;
            quads.push({ corners, avgZ, avgV: 0.5, type: 'p' });
        }
    }

    // Sort by depth (painter's algorithm)
    quads.sort((a, b) => a.avgZ - b.avgZ);

    // Draw quads
    for (const q of quads) {
        ctx.beginPath();
        ctx.moveTo(q.corners[0][0], q.corners[0][1]);
        for (let k = 1; k < 4; k++) ctx.lineTo(q.corners[k][0], q.corners[k][1]);
        ctx.closePath();
        if (q.type === 'p') {
            ctx.fillStyle = 'rgba(255, 255, 255, 0.05)';
            ctx.fill();
            ctx.strokeStyle = 'rgba(255, 255, 255, 0.15)';
            ctx.lineWidth = 0.5;
            ctx.stroke();
        } else {
            const r = Math.round(59 + q.avgV * 180);
            const g = Math.round(130 - q.avgV * 62);
            const b = Math.round(246 - q.avgV * 178);
            ctx.fillStyle = `rgb(${r},${g},${b})`;
            ctx.fill();
            ctx.strokeStyle = 'rgba(0,0,0,0.12)';
            ctx.lineWidth = 0.3;
            ctx.stroke();
        }
    }

    // Axes
    const o = proj(-0.5, -0.5, -0.4);
    const ax = proj(0.55, -0.5, -0.4);
    const ay = proj(-0.5, 0.55, -0.4);
    const az = proj(-0.5, -0.5, 0.5);
    ctx.strokeStyle = 'rgba(255,255,255,0.2)';
    ctx.lineWidth = 1;
    ctx.beginPath(); ctx.moveTo(o[0], o[1]); ctx.lineTo(ax[0], ax[1]); ctx.stroke();
    ctx.beginPath(); ctx.moveTo(o[0], o[1]); ctx.lineTo(ay[0], ay[1]); ctx.stroke();
    ctx.beginPath(); ctx.moveTo(o[0], o[1]); ctx.lineTo(az[0], az[1]); ctx.stroke();
    ctx.fillStyle = '#8b949e';
    ctx.font = '11px -apple-system, sans-serif';
    ctx.fillText('x\u2081', ax[0] + 4, ax[1] + 2);
    ctx.fillText('x\u2082', ay[0] + 4, ay[1] + 2);
    ctx.fillText('out', az[0] + 4, az[1] - 2);

    // Data points (on the surface)
    for (const [a, b, label] of DATA_POINTS) {
        inp[0] = a; inp[1] = b;
        NetForward(net, inp, out, c3);
        const p = proj(a - 0.5, b - 0.5, out[0] * 0.8 - 0.4);
        ctx.beginPath();
        ctx.arc(p[0], p[1], 7, 0, Math.PI * 2);
        ctx.fillStyle = label ? '#ef4444' : '#3b82f6';
        ctx.fill();
        ctx.strokeStyle = '#fff';
        ctx.lineWidth = 2;
        ctx.stroke();
        ctx.fillStyle = '#fff';
        ctx.font = 'bold 9px -apple-system, sans-serif';
        ctx.textAlign = 'center';
        ctx.textBaseline = 'middle';
        ctx.fillText(label.toString(), p[0], p[1]);
    }
}

function drawLossCurve() {
    const cvEl = document.getElementById('cv-loss');
    const { ctx, w, h } = setupCanvas(cvEl);
    const pad = { top: 16, right: 16, bottom: 28, left: 44 };
    const pw = w - pad.left - pad.right;
    const ph = h - pad.top - pad.bottom;

    ctx.clearRect(0, 0, w, h);

    if (losses.length < 2) {
        ctx.fillStyle = '#8b949e';
        ctx.font = '12px -apple-system, sans-serif';
        ctx.textAlign = 'center';
        ctx.fillText('Waiting for training...', w / 2, h / 2);
        return;
    }

    const maxLoss = Math.max(...losses) * 1.05;

    ctx.strokeStyle = '#21262d';
    ctx.lineWidth = 1;
    for (let i = 0; i <= 4; i++) {
        const y = pad.top + (ph * i / 4);
        ctx.beginPath(); ctx.moveTo(pad.left, y); ctx.lineTo(pad.left + pw, y); ctx.stroke();
    }

    ctx.beginPath();
    for (let i = 0; i < losses.length; i++) {
        const x = pad.left + (i / (losses.length - 1)) * pw;
        const y = pad.top + (1 - losses[i] / maxLoss) * ph;
        if (i === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y);
    }
    ctx.strokeStyle = '#22c55e';
    ctx.lineWidth = 2;
    ctx.stroke();

    const lastX = pad.left + pw;
    ctx.lineTo(lastX, pad.top + ph);
    ctx.lineTo(pad.left, pad.top + ph);
    ctx.closePath();
    const grd = ctx.createLinearGradient(0, pad.top, 0, pad.top + ph);
    grd.addColorStop(0, 'rgba(34, 197, 94, 0.2)');
    grd.addColorStop(1, 'rgba(34, 197, 94, 0.0)');
    ctx.fillStyle = grd;
    ctx.fill();

    ctx.fillStyle = '#8b949e';
    ctx.font = '10px -apple-system, sans-serif';
    ctx.textAlign = 'right';
    for (let i = 0; i <= 4; i++) {
        const val = maxLoss * (1 - i / 4);
        ctx.fillText(val.toFixed(2), pad.left - 6, pad.top + ph * i / 4 + 3);
    }
    ctx.textAlign = 'center';
    ctx.fillText('epoch', w / 2, h - 4);
    ctx.textAlign = 'left';
    ctx.fillText('0', pad.left, h - 10);
    ctx.textAlign = 'right';
    ctx.fillText(epoch.toString(), pad.left + pw, h - 10);
}

function updatePredictions() {
    const ids = ['p00', 'p01', 'p10', 'p11'];
    for (let i = 0; i < 4; i++) {
        const [a, b, expected] = DATA_POINTS[i];
        const val = predict(a, b);
        const el = document.getElementById(ids[i]);
        const err = Math.abs(val - expected);
        el.textContent = val.toFixed(4);
        el.style.color = err < 0.1 ? '#22c55e' : err < 0.3 ? '#eab308' : '#8b949e';
    }
}

function updateStats(loss) {
    document.getElementById('stat-epoch').textContent = epoch;
    if (loss !== undefined) {
        document.getElementById('stat-loss').textContent = loss.toFixed(4);
    }
}

function logLine(text) {
    consoleLines.push(text);
    if (consoleLines.length > 50) consoleLines.shift();
    document.getElementById('console-out').textContent = consoleLines.join('\n');
    const el = document.getElementById('console-out');
    el.scrollTop = el.scrollHeight;
}

// ================================================================
// ANIMATION LOOP
// ================================================================
let MAX_EPOCH = 20000;
let LOG_INTERVAL = 4000;

function parseSourceParams(source) {
    const mEpoch = source.match(/for\s+\w+\s+in\s+\d+\.\.(\d+)/);
    if (mEpoch) MAX_EPOCH = parseInt(mEpoch[1]);
    const mLog = source.match(/if\s+\w+\s+%\s+(\d+)/);
    if (mLog) LOG_INTERVAL = parseInt(mLog[1]);
}

function animate() {
    if (!running) return;
    const speed = parseInt(document.getElementById('speed').value);
    let lastLoss = 0;

    for (let i = 0; i < speed && epoch < MAX_EPOCH; i++) {
        lastLoss = trainOneEpoch();
        if (epoch % 10 === 0) losses.push(lastLoss);
        if (epoch % LOG_INTERVAL === 0) {
            logLine(`epoch ${epoch}  loss ${lastLoss.toFixed(4)}`);
        }
        epoch++;
    }

    drawBoundary();
    draw3D();
    drawLossCurve();
    updatePredictions();
    updateStats(lastLoss);

    if (!running || epoch >= MAX_EPOCH) {
        running = false;
        document.getElementById('btn-train').textContent = 'Train';
        document.getElementById('btn-train').disabled = false;
        document.getElementById('btn-stop').disabled = true;
        document.getElementById('stat-status').textContent = epoch >= MAX_EPOCH ? 'Done' : 'Stopped';
        document.getElementById('stat-status').style.color = epoch >= MAX_EPOCH ? '#22c55e' : '#f97316';
        logLine(`\nPredictions:`);
        for (const [a, b, exp] of DATA_POINTS) {
            logLine(`  ${a} xor ${b} = ${predict(a, b).toFixed(4)}  (expected ${exp})`);
        }
    } else {
        document.getElementById('stat-status').textContent = 'Training';
        document.getElementById('stat-status').style.color = '#eab308';
        requestAnimationFrame(animate);
    }
}

// ================================================================
// COMPILE + START
// ================================================================
function compileAndStart() {
    const source = document.getElementById('source-editor').value;
    parseSourceParams(source);

    logLine('Compiling dx \u2192 JS via Pyodide...');
    const t0 = performance.now();

    let jsCode;
    try {
        jsCode = compileDx(source);
    } catch (e) {
        logLine('Compilation error: ' + e.message);
        document.getElementById('stat-status').textContent = 'Error';
        document.getElementById('stat-status').style.color = '#f85149';
        document.getElementById('btn-train').textContent = 'Train';
        document.getElementById('btn-train').disabled = false;
        return;
    }

    const dt = (performance.now() - t0).toFixed(0);
    logLine(`Compiled in ${dt}ms`);

    // Show generated JS in collapsible panel
    document.getElementById('js-output').textContent = jsCode;

    // Strip main() and "use strict", convert let/const to var so
    // re-compilation works (re-declaring let/const is a SyntaxError)
    const lib = jsCode
        .replace(/"use strict";\s*\n?/, '')
        .replace(/\nmain\(\);\s*$/, '')
        .replace(/^let /gm, 'var ')
        .replace(/^const /gm, 'var ');

    try {
        (0, eval)(lib);
    } catch (e) {
        logLine('JS eval error: ' + e.message);
        document.getElementById('stat-status').textContent = 'Error';
        document.getElementById('stat-status').style.color = '#f85149';
        document.getElementById('btn-train').textContent = 'Train';
        document.getElementById('btn-train').disabled = false;
        return;
    }

    // Extract N_PARAMS from the model
    const tmpNet = createNet();
    N_PARAMS = tmpNet._buf.length;
    document.getElementById('stat-params').textContent = N_PARAMS;

    // Extract LR from dx source (not from JS — JS may use a variable)
    const src = document.getElementById('source-editor').value;
    const lrMatch = src.match(/\blr\s*:\s*([\d.]+(?:e[+-]?\d+)?)/i);
    LR = lrMatch ? parseFloat(lrMatch[1]) : 0.5;

    logLine(`Model: ${N_PARAMS} params, lr=${LR}`);
    logLine('Training Net (' + N_PARAMS + ' params)\n');

    initState();
    running = true;
    document.getElementById('btn-train').textContent = 'Training...';
    document.getElementById('btn-train').disabled = true;
    document.getElementById('btn-stop').disabled = false;
    document.getElementById('stat-status').textContent = 'Training';
    document.getElementById('stat-status').style.color = '#eab308';
    requestAnimationFrame(animate);
}

// ================================================================
// INIT
// ================================================================
async function init() {
    // Render source code with highlighting
    const ta = document.getElementById('source-editor');
    ta.value = DX_SOURCE;
    syncHighlight();
    ta.addEventListener('input', syncHighlight);
    ta.addEventListener('scroll', syncScroll);
    // Handle Tab key for indentation
    ta.addEventListener('keydown', (e) => {
        if (e.key === 'Tab') {
            e.preventDefault();
            const start = ta.selectionStart;
            const end = ta.selectionEnd;
            ta.value = ta.value.substring(0, start) + '    ' + ta.value.substring(end);
            ta.selectionStart = ta.selectionEnd = start + 4;
            syncHighlight();
        }
    });

    // JS toggle
    document.getElementById('js-toggle').addEventListener('click', () => {
        const el = document.getElementById('js-output');
        const toggle = document.getElementById('js-toggle');
        if (el.style.display === 'none' || !el.style.display) {
            el.style.display = 'block';
            toggle.innerHTML = '&#9660; Hide generated JS';
        } else {
            el.style.display = 'none';
            toggle.innerHTML = '&#9654; Show generated JS';
        }
    });

    // Load Pyodide
    try {
        await initPyodide();
    } catch (e) {
        document.getElementById('btn-train').textContent = 'Error loading Pyodide';
        document.getElementById('stat-status').textContent = 'Error';
        document.getElementById('stat-status').style.color = '#f85149';
        console.error('Pyodide init failed:', e);
        return;
    }

    document.getElementById('btn-train').textContent = 'Train';
    document.getElementById('btn-train').disabled = false;
    document.getElementById('stat-status').textContent = 'Ready';
    document.getElementById('stat-status').style.color = '#8b949e';

    // Train button
    document.getElementById('btn-train').addEventListener('click', () => {
        if (running) return;
        // Reset
        document.getElementById('stat-loss').textContent = '\u2014';
        document.getElementById('stat-epoch').textContent = '0';
        document.getElementById('stat-status').textContent = 'Compiling...';
        document.getElementById('stat-status').style.color = '#d2a8ff';
        document.getElementById('console-out').textContent = '';
        consoleLines = [];
        const ids = ['p00', 'p01', 'p10', 'p11'];
        ids.forEach(id => {
            const el = document.getElementById(id);
            el.textContent = '\u2014';
            el.style.color = '#8b949e';
        });
        // Use setTimeout to let the UI update before blocking on compile
        setTimeout(compileAndStart, 20);
    });

    // Stop button
    document.getElementById('btn-stop').addEventListener('click', () => {
        running = false;
        document.getElementById('btn-train').textContent = 'Train';
        document.getElementById('btn-train').disabled = false;
        document.getElementById('btn-stop').disabled = true;
        document.getElementById('stat-status').textContent = 'Stopped';
        document.getElementById('stat-status').style.color = '#f97316';
    });

    // 3D mouse drag rotation
    const cv3d = document.getElementById('cv-3d');
    cv3d.addEventListener('mousedown', (e) => {
        drag3d = true; mx3d = e.clientX; my3d = e.clientY;
        cv3d.style.cursor = 'grabbing';
    });
    window.addEventListener('mousemove', (e) => {
        if (!drag3d) return;
        az3d += (e.clientX - mx3d) * 0.01;
        el3d = Math.max(-1.2, Math.min(1.2, el3d + (e.clientY - my3d) * 0.01));
        mx3d = e.clientX; my3d = e.clientY;
        if (!running) draw3D();
    });
    window.addEventListener('mouseup', () => {
        drag3d = false;
        cv3d.style.cursor = 'grab';
    });

    // Handle resize
    window.addEventListener('resize', () => {
        if (typeof createNetCache === 'function' && net) {
            drawBoundary();
            draw3D();
            drawLossCurve();
        }
    });
}

document.addEventListener('DOMContentLoaded', init);
</script>
<script src="https://cdn.jsdelivr.net/pyodide/v0.27.0/full/pyodide.js"></script>

</body>
</html>